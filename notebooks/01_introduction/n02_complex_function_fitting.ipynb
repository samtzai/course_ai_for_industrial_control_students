{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b675463",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Complex Non-Linear Function Fitting Tutorial\n",
    "\n",
    "## **Master Non-Linear Model Training with Interactive PyTorch Implementation**\n",
    "\n",
    "### **Main Goal:**\n",
    "Train a neural network to learn the relationship **`y = 3*sin(x) + 2*cos(10x) - 0.5`** from noisy synthetic data, while understanding every step of the machine learning pipeline through interactive visualizations.\n",
    "\n",
    "### **Key Learning Objectives:**\n",
    "- **Model Complex Functions**: Handle multi-frequency sinusoidal components\n",
    "- **Neural Network Design**: Build deep networks for non-linear pattern recognition\n",
    "- **Fourier Analysis**: Understand frequency domain characteristics\n",
    "- **Advanced Training**: Regularization, batch normalization, and optimization\n",
    "- **Overfitting Analysis**: Recognize and prevent overfitting in complex models\n",
    "\n",
    "### **Industrial Relevance:**\n",
    "Apply to **vibration analysis**, **signal processing**, **time series forecasting**, and **control system identification** in industrial applications.\n",
    "\n",
    "### **Interactive Features:**\n",
    "ðŸŽ® Real-time architecture tuning | ðŸ“Š Fourier spectrum analysis | âš¡ Advanced training monitoring | ðŸ”¬ Regularization experimentation\n",
    "\n",
    "**Target Function**: `y = 3*sin(x) + 2*cos(10x) - 0.5`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a472d",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll use PyTorch, NumPy, Matplotlib, and Scikit-learn for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e1e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "%matplotlib widget\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f3b80f",
   "metadata": {},
   "source": [
    "## 2. Generate Complex Synthetic Dataset\n",
    "\n",
    "We'll create a dataset following the relationship `y = 3*sin(x) + 2*cos(10x) - 0.5` with added Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9460a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_complex_dataset(n_samples=200, noise_std=0.5, x_range=(-4, 4)):\n",
    "    x = np.random.uniform(x_range[0], x_range[1], n_samples)\n",
    "    y_true = 3 * np.sin(x) + 2 * np.cos(10 * x) - 0.5\n",
    "    noise = np.random.normal(0, noise_std, n_samples)\n",
    "    y_noisy = y_true + noise\n",
    "    return x, y_noisy, y_true\n",
    "\n",
    "n_train, n_test = 160, 40\n",
    "noise_level = 0.5\n",
    "x_train, y_train, y_train_true = generate_complex_dataset(n_train, noise_level)\n",
    "x_test, y_test, y_test_true = generate_complex_dataset(n_test, noise_level)\n",
    "\n",
    "x_train_tensor = torch.FloatTensor(x_train).unsqueeze(1).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)\n",
    "x_test_tensor = torch.FloatTensor(x_test).unsqueeze(1).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1).to(device)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(x_train, y_train, alpha=0.7, label='Noisy train', color='blue')\n",
    "ax.scatter(x_test, y_test, alpha=0.7, label='Noisy test', color='green')\n",
    "x_plot = np.linspace(-4, 4, 500)\n",
    "y_plot = 3 * np.sin(x_plot) + 2 * np.cos(10 * x_plot) - 0.5\n",
    "ax.plot(x_plot, y_plot, 'r-', label='True function', linewidth=2)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Complex Function Dataset')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c382250",
   "metadata": {},
   "source": [
    "## 3. Define Neural Network Model Architecture\n",
    "\n",
    "A simple linear model cannot fit this function. We'll use a multi-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNet(nn.Module):\n",
    "    def __init__(self, hidden_size=64, num_layers=3):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(1, hidden_size), nn.ReLU()]\n",
    "        for _ in range(num_layers-1):\n",
    "            layers += [nn.Linear(hidden_size, hidden_size), nn.ReLU()]\n",
    "        layers += [nn.Linear(hidden_size, 1)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ComplexNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcec78f",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "\n",
    "Let's train the model to fit the complex function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb91c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_test, y_test, lr=0.01, epochs=1000):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    train_losses, test_losses = [], []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x_train)\n",
    "        loss = criterion(pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_pred = model(x_test)\n",
    "            test_loss = criterion(test_pred, y_test)\n",
    "        train_losses.append(loss.item())\n",
    "        test_losses.append(test_loss.item())\n",
    "        if (epoch+1) % 200 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {loss.item():.4f} | Test Loss: {test_loss.item():.4f}\")\n",
    "    return train_losses, test_losses\n",
    "\n",
    "model = ComplexNet().to(device)\n",
    "train_losses, test_losses = train_model(model, x_train_tensor, y_train_tensor, x_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7fc6c1",
   "metadata": {},
   "source": [
    "## 5. Visualize Model Predictions\n",
    "\n",
    "Let's see how well the trained model fits the data and the true function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1433e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x_plot = np.linspace(-4, 4, 500)\n",
    "x_plot_tensor = torch.FloatTensor(x_plot).unsqueeze(1).to(device)\n",
    "with torch.no_grad():\n",
    "    y_pred_plot = model(x_plot_tensor).cpu().numpy().squeeze()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(x_train, y_train, alpha=0.5, label='Train data', color='blue')\n",
    "plt.scatter(x_test, y_test, alpha=0.5, label='Test data', color='green')\n",
    "plt.plot(x_plot, 3 * np.sin(x_plot) + 2 * np.cos(10 * x_plot) - 0.5, 'r-', label='True function', linewidth=2)\n",
    "plt.plot(x_plot, y_pred_plot, 'orange', label='Model prediction', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Model Fit to Complex Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bc1aa4",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Evaluate the model using MSE, MAE, and RÂ² metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c241cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_pred = model(x_train_tensor).cpu().numpy().squeeze()\n",
    "    test_pred = model(x_test_tensor).cpu().numpy().squeeze()\n",
    "train_mse = mean_squared_error(y_train, train_pred)\n",
    "test_mse = mean_squared_error(y_test, test_pred)\n",
    "train_mae = mean_absolute_error(y_train, train_pred)\n",
    "test_mae = mean_absolute_error(y_test, test_pred)\n",
    "train_r2 = r2_score(y_train, train_pred)\n",
    "test_r2 = r2_score(y_test, test_pred)\n",
    "print(f\"Train MSE: {train_mse:.4f} | Test MSE: {test_mse:.4f}\")\n",
    "print(f\"Train MAE: {train_mae:.4f} | Test MAE: {test_mae:.4f}\")\n",
    "print(f\"Train RÂ²: {train_r2:.4f} | Test RÂ²: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc163bc0",
   "metadata": {},
   "source": [
    "## 6.1 Regression Plot: Predicted vs True Values\n",
    "\n",
    "Visualize the relationship between predicted and true values with a regression plot to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f351f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression plot showing predicted vs true values\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3))\n",
    "\n",
    "# Plot 1: Training data regression plot\n",
    "min_val = min(y_train.min(), train_pred.min())\n",
    "max_val = max(y_train.max(), train_pred.max())\n",
    "ax1.scatter(y_train, train_pred, alpha=0.6, color='blue', s=50, edgecolors='darkblue', linewidth=0.5)\n",
    "ax1.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "ax1.set_xlabel('True Values')\n",
    "ax1.set_ylabel('Predicted Values')\n",
    "ax1.set_title(f'Training Set: Predicted vs True Values\\nMSE: {train_mse:.4f} | MAE: {train_mae:.4f} | RÂ²: {train_r2:.4f}')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Add correlation coefficient for training\n",
    "train_corr = np.corrcoef(y_train, train_pred)[0, 1]\n",
    "ax1.text(0.05, 0.95, f'Correlation: {train_corr:.4f}', transform=ax1.transAxes, \n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),\n",
    "         verticalalignment='top', fontsize=10)\n",
    "\n",
    "# Plot 2: Test data regression plot\n",
    "min_val_test = min(y_test.min(), test_pred.min())\n",
    "max_val_test = max(y_test.max(), test_pred.max())\n",
    "ax2.scatter(y_test, test_pred, alpha=0.6, color='green', s=50, edgecolors='darkgreen', linewidth=0.5)\n",
    "ax2.plot([min_val_test, max_val_test], [min_val_test, max_val_test], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "ax2.set_xlabel('True Values')\n",
    "ax2.set_ylabel('Predicted Values')\n",
    "ax2.set_title(f'Test Set: Predicted vs True Values\\nMSE: {test_mse:.4f} | MAE: {test_mae:.4f} | RÂ²: {test_r2:.4f}')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "# Add correlation coefficient for test\n",
    "test_corr = np.corrcoef(y_test, test_pred)[0, 1]\n",
    "ax2.text(0.05, 0.95, f'Correlation: {test_corr:.4f}', transform=ax2.transAxes, \n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8),\n",
    "         verticalalignment='top', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comprehensive metrics summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š COMPREHENSIVE MODEL EVALUATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<20} {'Train':<12} {'Test':<12} {'Difference':<12}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'MSE':<20} {train_mse:<12.4f} {test_mse:<12.4f} {abs(test_mse-train_mse):<12.4f}\")\n",
    "print(f\"{'MAE':<20} {train_mae:<12.4f} {test_mae:<12.4f} {abs(test_mae-train_mae):<12.4f}\")\n",
    "print(f\"{'RÂ² Score':<20} {train_r2:<12.4f} {test_r2:<12.4f} {abs(test_r2-train_r2):<12.4f}\")\n",
    "print(f\"{'Correlation':<20} {train_corr:<12.4f} {test_corr:<12.4f} {abs(test_corr-train_corr):<12.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Performance assessment\n",
    "if abs(test_mse - train_mse) < 0.1:\n",
    "    print(\"âœ… Good generalization: Similar performance on train and test sets\")\n",
    "elif test_mse > train_mse * 1.5:\n",
    "    print(\"âš ï¸ Possible overfitting: Test performance significantly worse than training\")\n",
    "else:\n",
    "    print(\"âœ… Reasonable generalization: Test performance slightly worse than training\")\n",
    "\n",
    "if test_r2 > 0.9:\n",
    "    print(\"ðŸŽ¯ Excellent model performance: RÂ² > 0.9\")\n",
    "elif test_r2 > 0.7:\n",
    "    print(\"ðŸ‘ Good model performance: RÂ² > 0.7\")\n",
    "elif test_r2 > 0.5:\n",
    "    print(\"ðŸ“Š Fair model performance: RÂ² > 0.5\")\n",
    "else:\n",
    "    print(\"ðŸ“‰ Poor model performance: RÂ² < 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c5f36",
   "metadata": {},
   "source": [
    "## 7. Interactive Hyperparameter Tuning (Optional)\n",
    "\n",
    "Try different network sizes and training epochs interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c5f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_training(hidden_size=64, num_layers=3, lr=0.01, epochs=1000):\n",
    "    model = ComplexNet(hidden_size=hidden_size, num_layers=num_layers).to(device)\n",
    "    train_losses, test_losses = train_model(model, x_train_tensor, y_train_tensor, x_test_tensor, y_test_tensor, lr=lr, epochs=epochs)\n",
    "    model.eval()\n",
    "    x_plot = np.linspace(-4, 4, 500)\n",
    "    x_plot_tensor = torch.FloatTensor(x_plot).unsqueeze(1).to(device)\n",
    "    with torch.no_grad():\n",
    "        y_pred_plot = model(x_plot_tensor).cpu().numpy().squeeze()\n",
    "    \n",
    "    # Calculate metrics for display\n",
    "    with torch.no_grad():\n",
    "        train_pred = model(x_train_tensor).cpu().numpy().squeeze()\n",
    "        test_pred = model(x_test_tensor).cpu().numpy().squeeze()\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train, train_pred)\n",
    "    test_mse = mean_squared_error(y_test, test_pred)\n",
    "    train_r2 = r2_score(y_train, train_pred)\n",
    "    test_r2 = r2_score(y_test, test_pred)\n",
    "    \n",
    "    # Create subplots for function fit and learning curves\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Function fit with metrics\n",
    "    ax1.plot(x_plot, 3 * np.sin(x_plot) + 2 * np.cos(10 * x_plot) - 0.5, 'r-', label='True function', linewidth=2)\n",
    "    ax1.plot(x_plot, y_pred_plot, 'orange', label='Model prediction', linewidth=2)\n",
    "    ax1.scatter(x_train, y_train, alpha=0.4, s=20, color='blue', label='Train data')\n",
    "    ax1.scatter(x_test, y_test, alpha=0.4, s=20, color='green', label='Test data')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.set_title(f'Interactive Model Fit\\nTrain RÂ²: {train_r2:.3f}, MSE: {train_mse:.3f}\\nTest RÂ²: {test_r2:.3f}, MSE: {test_mse:.3f}')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Learning curves with final metrics\n",
    "    ax2.plot(train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "    ax2.plot(test_losses, label='Test Loss', color='red', linewidth=2)\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Loss (MSE)')\n",
    "    ax2.set_title(f'Learning Curves\\nFinal Train MSE: {train_mse:.3f}\\nFinal Test MSE: {test_mse:.3f}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')  # Log scale for better visualization\n",
    "    \n",
    "    # Add text box with all metrics\n",
    "    metrics_text = f'Train: RÂ²={train_r2:.3f}, MSE={train_mse:.3f}\\nTest: RÂ²={test_r2:.3f}, MSE={test_mse:.3f}'\n",
    "    ax2.text(0.02, 0.98, metrics_text, transform=ax2.transAxes, \n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
    "             verticalalignment='top', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "interact(interactive_training, hidden_size=(16, 256, 16), num_layers=(2, 5), lr=(0.001, 0.05, 0.001), epochs=(500, 2000, 100));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e5eca9",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "- Neural networks can fit highly non-linear, multi-frequency functions.\n",
    "- Model depth and width are crucial for capturing complex patterns.\n",
    "- Overfitting can occur if the model is too large or trained too long.\n",
    "- Interactive tuning helps find the best architecture for your data.\n",
    "\n",
    "---\n",
    "\n",
    "**Try modifying the function or adding more noise to further challenge your model!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
