{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb12d0d",
   "metadata": {},
   "source": [
    "# üéØ Practical Work Objective Summary\n",
    "\n",
    "## **Master Linear Model Training with Interactive PyTorch Implementation**\n",
    "\n",
    "### **Main Goal:**\n",
    "Train a PyTorch linear regression model to learn the relationship **`y = 2x + 1`** from noisy synthetic data, while understanding every step of the machine learning pipeline through interactive visualizations.\n",
    "\n",
    "### **Key Learning Objectives:**\n",
    "- **Build & Train Models**: Implement linear regression using PyTorch's `nn.Module`\n",
    "- **Interactive Exploration**: Use real-time sliders to understand parameter impact on predictions\n",
    "- **Complete ML Pipeline**: Master data generation ‚Üí model design ‚Üí training ‚Üí evaluation\n",
    "- **Visual Understanding**: Explore 3D loss landscapes and optimization trajectories\n",
    "- **Architecture Comparison**: Compare Linear vs. Polynomial vs. Neural Network approaches\n",
    "\n",
    "### **Industrial Relevance:**\n",
    "Apply learned concepts to real-world scenarios like **process control**, **quality prediction**, and **predictive maintenance** in industrial systems.\n",
    "\n",
    "### **Interactive Features:**\n",
    "üéÆ Real-time parameter adjustment | üìä 3D loss visualization | ‚ö° Live training monitoring | üî¨ Architecture experimentation\n",
    "\n",
    "**Duration**: 3-4 hours | **Assessment**: Comprehensive questionnaire | **Outcome**: Solid foundation for industrial AI applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792da56",
   "metadata": {},
   "source": [
    "# Linear Model Training Tutorial with PyTorch\n",
    "\n",
    "## Interactive Learning: From Theory to Practice\n",
    "\n",
    "Welcome to this comprehensive tutorial on training linear models using PyTorch! In this notebook, you'll learn:\n",
    "\n",
    "- üéØ How to create and train a linear model to predict noisy data\n",
    "- üìä Interactive visualization of loss functions under different weight conditions  \n",
    "- üîÑ The complete training pipeline: forward pass, loss calculation, backpropagation\n",
    "- üìà Model evaluation and performance metrics\n",
    "- üèóÔ∏è Experimenting with different model architectures\n",
    "\n",
    "**Learning Objective**: Train a model to predict values following the relationship `y = 2x + 1` with added noise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b671e979",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Let's start by importing all the libraries we'll need for this tutorial:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb7a3c1",
   "metadata": {},
   "source": [
    "### üéÆ Interactive Mode Enabled\n",
    "\n",
    "This notebook now features **full interactive widgets** for enhanced learning experience! You can:\n",
    "\n",
    "- **Use sliders** to adjust parameters and see real-time changes\n",
    "- **Interactive plots** that update automatically as you modify values\n",
    "- **Real-time exploration** of different model configurations\n",
    "- **Dynamic visualizations** that respond to your inputs instantly\n",
    "\n",
    "The interactive widgets make learning more engaging and intuitive, allowing you to experiment with different parameters and immediately see their effects on model behavior!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2997db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Interactive widgets for better user experience\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Set style and random seeds for reproducibility\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configure matplotlib for interactive plots\n",
    "%matplotlib widget\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "print(\"üéÆ Interactive widgets enabled!\")\n",
    "print(\"   You can now use sliders and interactive controls to explore parameters in real-time.\")\n",
    "print(\"   The notebook has been upgraded from WSL-compatible static mode to full interactive mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784f757",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Dataset\n",
    "\n",
    "We'll create a dataset following the relationship `y = 2x + 1` with added Gaussian noise to simulate real-world data conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n_samples=100, noise_std=0.5, x_range=(-5, 5)):\n",
    "    \"\"\"\n",
    "    Generate synthetic dataset following y = 2x + 1 + noise\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples: Number of data points\n",
    "    - noise_std: Standard deviation of Gaussian noise\n",
    "    - x_range: Range of x values (min, max)\n",
    "    \"\"\"\n",
    "    # Generate x values uniformly distributed\n",
    "    x = np.random.uniform(x_range[0], x_range[1], n_samples)\n",
    "    \n",
    "    # True relationship: y = 2x + 1\n",
    "    y_true = 2 * x + 1\n",
    "    \n",
    "    # Add Gaussian noise\n",
    "    noise = np.random.normal(0, noise_std, n_samples)\n",
    "    y_noisy = y_true + noise\n",
    "    \n",
    "    return x, y_noisy, y_true\n",
    "\n",
    "# Generate training and test datasets\n",
    "n_train, n_test = 80, 20\n",
    "noise_level = 0.8\n",
    "\n",
    "x_train, y_train, y_train_true = generate_dataset(n_train, noise_level)\n",
    "x_test, y_test, y_test_true = generate_dataset(n_test, noise_level)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train_tensor = torch.FloatTensor(x_train).unsqueeze(1).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)\n",
    "x_test_tensor = torch.FloatTensor(x_test).unsqueeze(1).to(device)\n",
    "y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1).to(device)\n",
    "\n",
    "# Visualize the dataset\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "# Training data\n",
    "ax1.scatter(x_train, y_train, alpha=0.7, label='Noisy data', color='blue')\n",
    "ax1.plot(x_train, y_train_true, 'r--', label='True: y = 2x + 1', linewidth=2)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('Training Dataset')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Test data\n",
    "ax2.scatter(x_test, y_test, alpha=0.7, label='Noisy data', color='green')\n",
    "ax2.plot(x_test, y_test_true, 'r--', label='True: y = 2x + 1', linewidth=2)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Test Dataset')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Test samples: {len(x_test)}\")\n",
    "print(f\"Noise standard deviation: {noise_level}\")\n",
    "print(f\"Data range: x ‚àà [{x_train.min():.2f}, {x_train.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927d0d5",
   "metadata": {},
   "source": [
    "## 3. Define Linear Model Architecture\n",
    "\n",
    "Let's implement a simple linear regression model using PyTorch's `nn.Module`. Our goal is to learn the parameters `weight` and `bias` such that `y = weight * x + bias` approximates our target function `y = 2x + 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38bdd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Linear Regression Model: y = weight * x + bias\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1, output_dim=1):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset parameters to random values\"\"\"\n",
    "        nn.init.normal_(self.linear.weight, mean=0.0, std=1.0)\n",
    "        nn.init.normal_(self.linear.bias, mean=0.0, std=1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        \"\"\"Get current weight and bias values\"\"\"\n",
    "        weight = self.linear.weight.item()\n",
    "        bias = self.linear.bias.item()\n",
    "        return weight, bias\n",
    "    \n",
    "    def set_parameters(self, weight, bias):\n",
    "        \"\"\"Manually set weight and bias values\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.linear.weight.data.fill_(weight)\n",
    "            self.linear.bias.data.fill_(bias)\n",
    "\n",
    "# Create model instance\n",
    "model = LinearModel().to(device)\n",
    "\n",
    "# Display initial parameters\n",
    "weight, bias = model.get_parameters()\n",
    "print(f\"Initial model parameters:\")\n",
    "print(f\"Weight: {weight:.4f}\")\n",
    "print(f\"Bias: {bias:.4f}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.tensor([[1.0], [2.0], [3.0]]).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"\\nTest forward pass:\")\n",
    "print(f\"Input: {test_input.squeeze().tolist()}\")\n",
    "print(f\"Output: {test_output.squeeze().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f841c692",
   "metadata": {},
   "source": [
    "## 4. Interactive Weight Visualization\n",
    "\n",
    "Now let's create an interactive visualization to see how changing the model parameters affects the predictions. This will help you understand the relationship between model parameters and the resulting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1259efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_interactive(weight=1.0, bias=0.0):\n",
    "    \"\"\"\n",
    "    Interactive function to visualize predictions with real-time parameter adjustment\n",
    "    \"\"\"\n",
    "    # Set model parameters\n",
    "    model.set_parameters(weight, bias)\n",
    "    \n",
    "    # Generate predictions for plotting\n",
    "    x_plot = np.linspace(-6, 6, 100)\n",
    "    x_plot_tensor = torch.FloatTensor(x_plot).unsqueeze(1).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred_plot = model(x_plot_tensor).cpu().numpy().squeeze()\n",
    "    \n",
    "    # Calculate loss on training data\n",
    "    with torch.no_grad():\n",
    "        y_pred_train = model(x_train_tensor)\n",
    "        loss = nn.MSELoss()(y_pred_train, y_train_tensor).item()\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    # Plot training data\n",
    "    plt.scatter(x_train, y_train, alpha=0.7, label='Training data', color='blue', s=50)\n",
    "    \n",
    "    # Plot true function\n",
    "    y_true_plot = 2 * x_plot + 1\n",
    "    plt.plot(x_plot, y_true_plot, 'r--', label='True: y = 2x + 1', linewidth=3)\n",
    "    \n",
    "    # Plot model prediction\n",
    "    plt.plot(x_plot, y_pred_plot, 'g-', label=f'Model: y = {weight:.2f}x + {bias:.2f}', linewidth=3)\n",
    "    \n",
    "    plt.xlabel('x', fontsize=12)\n",
    "    plt.ylabel('y', fontsize=12)\n",
    "    plt.title(f'Interactive Parameter Exploration\\nMSE Loss: {loss:.4f}', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(-6, 6)\n",
    "    plt.ylim(-15, 15)\n",
    "    \n",
    "    # Add performance text box\n",
    "    distance_to_optimal = np.sqrt((weight - 2.0)**2 + (bias - 1.0)**2)\n",
    "    textstr = f'Weight: {weight:.2f}\\nBias: {bias:.2f}\\nMSE Loss: {loss:.4f}\\n\\nTarget: w=2.0, b=1.0\\nDistance: {distance_to_optimal:.3f}'\n",
    "    props = dict(boxstyle='round', facecolor='lightblue', alpha=0.8)\n",
    "    plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
    "             verticalalignment='top', bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create interactive widget for parameter exploration\n",
    "print(\"üéØ Interactive Parameter Exploration\")\n",
    "print(\"Use the sliders below to adjust weight and bias values and see real-time updates!\")\n",
    "\n",
    "# Create interactive sliders\n",
    "weight_slider = widgets.FloatSlider(\n",
    "    value=0.0,\n",
    "    min=-3.0,\n",
    "    max=5.0,\n",
    "    step=0.1,\n",
    "    description='Weight:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "bias_slider = widgets.FloatSlider(\n",
    "    value=0.0,\n",
    "    min=-3.0,\n",
    "    max=5.0,\n",
    "    step=0.1,\n",
    "    description='Bias:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Create the interactive widget\n",
    "interactive_plot = interactive(plot_predictions_interactive, \n",
    "                              weight=weight_slider, \n",
    "                              bias=bias_slider)\n",
    "\n",
    "# Display the widget\n",
    "display(interactive_plot)\n",
    "\n",
    "# Add some preset buttons for quick exploration\n",
    "def set_random():\n",
    "    weight_slider.value = np.random.uniform(-2, 4)\n",
    "    bias_slider.value = np.random.uniform(-2, 4)\n",
    "\n",
    "def set_optimal():\n",
    "    weight_slider.value = 2.0\n",
    "    bias_slider.value = 1.0\n",
    "\n",
    "def set_zeros():\n",
    "    weight_slider.value = 0.0\n",
    "    bias_slider.value = 0.0\n",
    "\n",
    "# Create preset buttons\n",
    "random_button = widgets.Button(description=\"Random Parameters\", button_style='info')\n",
    "optimal_button = widgets.Button(description=\"Optimal (2, 1)\", button_style='success')\n",
    "zeros_button = widgets.Button(description=\"Reset to Zero\", button_style='warning')\n",
    "\n",
    "random_button.on_click(lambda b: set_random())\n",
    "optimal_button.on_click(lambda b: set_optimal())\n",
    "zeros_button.on_click(lambda b: set_zeros())\n",
    "\n",
    "# Display preset buttons\n",
    "print(\"\\nüîß Quick preset buttons:\")\n",
    "button_box = widgets.HBox([zeros_button, random_button, optimal_button])\n",
    "display(button_box)\n",
    "\n",
    "print(\"\\nüí° Tips:\")\n",
    "print(\"‚Ä¢ Move the sliders to see how different parameters affect the model prediction\")\n",
    "print(\"‚Ä¢ Watch how the loss changes as you get closer to the optimal values (weight=2, bias=1)\")\n",
    "print(\"‚Ä¢ Try the preset buttons for quick parameter changes\")\n",
    "print(\"‚Ä¢ The green line shows your current model, red dashed line shows the true function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45492e03",
   "metadata": {},
   "source": [
    "## 5. Loss Function Implementation and Visualization\n",
    "\n",
    "Understanding loss functions is crucial for training neural networks. Let's explore different loss functions and visualize how the loss changes with different parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41417a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(weight, bias, loss_type='mse'):\n",
    "    \"\"\"\n",
    "    Calculate loss for given weight and bias values\n",
    "    \"\"\"\n",
    "    model.set_parameters(weight, bias)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(x_train_tensor)\n",
    "        \n",
    "        if loss_type == 'mse':\n",
    "            loss = nn.MSELoss()(predictions, y_train_tensor).item()\n",
    "        elif loss_type == 'mae':\n",
    "            loss = nn.L1Loss()(predictions, y_train_tensor).item()\n",
    "        elif loss_type == 'huber':\n",
    "            loss = nn.SmoothL1Loss()(predictions, y_train_tensor).item()\n",
    "        \n",
    "    return loss\n",
    "\n",
    "# Create 3D loss landscape visualization\n",
    "def plot_loss_landscape():\n",
    "    \"\"\"\n",
    "    Create a 3D visualization of the loss landscape\n",
    "    \"\"\"\n",
    "    # Create grid of weight and bias values\n",
    "    weights = np.linspace(-1, 5, 50)\n",
    "    biases = np.linspace(-3, 5, 50)\n",
    "    W, B = np.meshgrid(weights, biases)\n",
    "    \n",
    "    # Calculate loss for each combination\n",
    "    losses = np.zeros_like(W)\n",
    "    for i in range(len(weights)):\n",
    "        for j in range(len(biases)):\n",
    "            losses[j, i] = calculate_loss(weights[i], biases[j])\n",
    "    \n",
    "    # Create 3D surface plot\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 3D surface plot\n",
    "    ax1 = fig.add_subplot(131, projection='3d')\n",
    "    surf = ax1.plot_surface(W, B, losses, cmap='viridis', alpha=0.8)\n",
    "    ax1.set_xlabel('Weight')\n",
    "    ax1.set_ylabel('Bias')\n",
    "    ax1.set_zlabel('MSE Loss')\n",
    "    ax1.set_title('3D Loss Landscape')\n",
    "    \n",
    "    # Mark the optimal point (true parameters)\n",
    "    optimal_loss = calculate_loss(2, 1)\n",
    "    ax1.scatter([2], [1], [optimal_loss], color='red', s=100, label='Optimal (2, 1)')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2D contour plot\n",
    "    ax2 = fig.add_subplot(132)\n",
    "    contour = ax2.contour(W, B, losses, levels=20)\n",
    "    ax2.clabel(contour, inline=True, fontsize=8)\n",
    "    ax2.scatter([2], [1], color='red', s=100, marker='*', label='Optimal (2, 1)')\n",
    "    ax2.set_xlabel('Weight')\n",
    "    ax2.set_ylabel('Bias')\n",
    "    ax2.set_title('2D Loss Contours')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss comparison for different loss functions\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    weight_range = np.linspace(0, 4, 100)\n",
    "    mse_losses = [calculate_loss(w, 1, 'mse') for w in weight_range]\n",
    "    mae_losses = [calculate_loss(w, 1, 'mae') for w in weight_range]\n",
    "    huber_losses = [calculate_loss(w, 1, 'huber') for w in weight_range]\n",
    "    \n",
    "    ax3.plot(weight_range, mse_losses, label='MSE Loss', linewidth=2)\n",
    "    ax3.plot(weight_range, mae_losses, label='MAE Loss', linewidth=2)\n",
    "    ax3.plot(weight_range, huber_losses, label='Huber Loss', linewidth=2)\n",
    "    ax3.axvline(x=2, color='red', linestyle='--', alpha=0.7, label='Optimal weight')\n",
    "    ax3.set_xlabel('Weight (bias fixed at 1)')\n",
    "    ax3.set_ylabel('Loss')\n",
    "    ax3.set_title('Loss Functions Comparison')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Optimal parameters: weight=2, bias=1\")\n",
    "    print(f\"Loss at optimal point: {optimal_loss:.4f}\")\n",
    "\n",
    "# Display the static loss landscape first\n",
    "plot_loss_landscape()\n",
    "\n",
    "# Interactive loss exploration function\n",
    "def explore_loss_interactive(weight=1.0, bias=1.0, loss_type='mse'):\n",
    "    \"\"\"\n",
    "    Interactive function to explore different loss functions and parameter values\n",
    "    \"\"\"\n",
    "    # Calculate losses for different loss functions\n",
    "    mse_loss = calculate_loss(weight, bias, 'mse')\n",
    "    mae_loss = calculate_loss(weight, bias, 'mae')\n",
    "    huber_loss = calculate_loss(weight, bias, 'huber')\n",
    "    \n",
    "    # Calculate optimal losses for comparison\n",
    "    mse_optimal = calculate_loss(2, 1, 'mse')\n",
    "    mae_optimal = calculate_loss(2, 1, 'mae')\n",
    "    huber_optimal = calculate_loss(2, 1, 'huber')\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3))\n",
    "    \n",
    "    # Loss comparison bar chart\n",
    "    loss_names = ['MSE', 'MAE', 'Huber']\n",
    "    current_losses = [mse_loss, mae_loss, huber_loss]\n",
    "    optimal_losses = [mse_optimal, mae_optimal, huber_optimal]\n",
    "    \n",
    "    x_pos = np.arange(len(loss_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x_pos - width/2, current_losses, width, label=f'Current (w={weight:.2f}, b={bias:.2f})', alpha=0.8)\n",
    "    bars2 = ax1.bar(x_pos + width/2, optimal_losses, width, label='Optimal (w=2.0, b=1.0)', alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Loss Function')\n",
    "    ax1.set_ylabel('Loss Value')\n",
    "    ax1.set_title('Loss Function Comparison')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(loss_names)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight selected loss function\n",
    "    selected_idx = {'mse': 0, 'mae': 1, 'huber': 2}[loss_type]\n",
    "    bars1[selected_idx].set_color('red')\n",
    "    bars1[selected_idx].set_alpha(1.0)\n",
    "    \n",
    "    # Current loss details\n",
    "    current_loss = current_losses[selected_idx]\n",
    "    optimal_loss = optimal_losses[selected_idx]\n",
    "    difference = current_loss - optimal_loss\n",
    "    \n",
    "    # Parameter space visualization with current point\n",
    "    weights_grid = np.linspace(-1, 5, 30)\n",
    "    biases_grid = np.linspace(-3, 5, 30)\n",
    "    W_grid, B_grid = np.meshgrid(weights_grid, biases_grid)\n",
    "    \n",
    "    # Calculate loss surface for selected loss type\n",
    "    losses_grid = np.zeros_like(W_grid)\n",
    "    for i in range(len(weights_grid)):\n",
    "        for j in range(len(biases_grid)):\n",
    "            losses_grid[j, i] = calculate_loss(weights_grid[i], biases_grid[j], loss_type)\n",
    "    \n",
    "    # Plot contour map\n",
    "    contour = ax2.contour(W_grid, B_grid, losses_grid, levels=15, alpha=0.6)\n",
    "    ax2.clabel(contour, inline=True, fontsize=8)\n",
    "    \n",
    "    # Mark current point and optimal point\n",
    "    ax2.scatter([weight], [bias], color='red', s=150, marker='o', label=f'Current ({weight:.2f}, {bias:.2f})', zorder=5)\n",
    "    ax2.scatter([2], [1], color='blue', s=150, marker='*', label='Optimal (2.0, 1.0)', zorder=5)\n",
    "    \n",
    "    ax2.set_xlabel('Weight')\n",
    "    ax2.set_ylabel('Bias')\n",
    "    ax2.set_title(f'{loss_type.upper()} Loss Landscape')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed information\n",
    "    print(f\"üìä Loss Analysis for {loss_type.upper()}:\")\n",
    "    print(f\"Current loss: {current_loss:.6f}\")\n",
    "    print(f\"Optimal loss: {optimal_loss:.6f}\")\n",
    "    print(f\"Difference: {difference:+.6f}\")\n",
    "    print(f\"Distance from optimal: {np.sqrt((weight-2)**2 + (bias-1)**2):.3f}\")\n",
    "\n",
    "# Create interactive widget for loss exploration\n",
    "print(\"\\nüîç Interactive Loss Function Exploration\")\n",
    "print(\"Use the controls below to explore different loss functions and parameter values!\")\n",
    "\n",
    "# Create widgets for loss exploration\n",
    "weight_loss_slider = widgets.FloatSlider(\n",
    "    value=1.0,\n",
    "    min=-1.0,\n",
    "    max=5.0,\n",
    "    step=0.1,\n",
    "    description='Weight:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "bias_loss_slider = widgets.FloatSlider(\n",
    "    value=1.0,\n",
    "    min=-3.0,\n",
    "    max=5.0,\n",
    "    step=0.1,\n",
    "    description='Bias:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "loss_type_dropdown = widgets.Dropdown(\n",
    "    options=['mse', 'mae', 'huber'],\n",
    "    value='mse',\n",
    "    description='Loss Function:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Create the interactive widget for loss exploration\n",
    "interactive_loss = interactive(explore_loss_interactive, \n",
    "                              weight=weight_loss_slider, \n",
    "                              bias=bias_loss_slider,\n",
    "                              loss_type=loss_type_dropdown)\n",
    "\n",
    "display(interactive_loss)\n",
    "\n",
    "print(\"\\nüí° Interactive Loss Exploration Tips:\")\n",
    "print(\"‚Ä¢ Change the loss function to see how different loss types behave\")\n",
    "print(\"‚Ä¢ Move the weight and bias sliders to explore the loss landscape\")\n",
    "print(\"‚Ä¢ MSE: Penalizes large errors more heavily (squared difference)\")\n",
    "print(\"‚Ä¢ MAE: Linear penalty for errors (absolute difference)\")\n",
    "print(\"‚Ä¢ Huber: Combination of MSE and MAE (robust to outliers)\")\n",
    "print(\"‚Ä¢ Watch how the contour plots change for different loss functions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b09dc7",
   "metadata": {},
   "source": [
    "## 6. Training Loop Implementation\n",
    "\n",
    "Now let's implement the complete training pipeline with gradient descent, backpropagation, and parameter updates. This is where the magic of machine learning happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9694f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train, x_test, y_test, \n",
    "                learning_rate=0.01, num_epochs=100, optimizer_type='sgd'):\n",
    "    \"\"\"\n",
    "    Complete training loop with logging and visualization\n",
    "    \"\"\"\n",
    "    # Initialize optimizer\n",
    "    if optimizer_type == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_type == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_type == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Tracking lists\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    weights_history = []\n",
    "    biases_history = []\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        predictions = model(x_train)\n",
    "        train_loss = criterion(predictions, y_train)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        train_loss.backward()  # Compute gradients\n",
    "        optimizer.step()       # Update parameters\n",
    "        \n",
    "        # Evaluation on test set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_predictions = model(x_test)\n",
    "            test_loss = criterion(test_predictions, y_test)\n",
    "        model.train()\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss.item())\n",
    "        test_losses.append(test_loss.item())\n",
    "        weight, bias = model.get_parameters()\n",
    "        weights_history.append(weight)\n",
    "        biases_history.append(bias)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, '\n",
    "                  f'Test Loss: {test_loss.item():.4f}, Weight: {weight:.3f}, Bias: {bias:.3f}')\n",
    "    \n",
    "    return train_losses, test_losses, weights_history, biases_history\n",
    "\n",
    "# Reset model to random parameters\n",
    "model.reset_parameters()\n",
    "initial_weight, initial_bias = model.get_parameters()\n",
    "print(f\"Initial parameters: Weight={initial_weight:.4f}, Bias={initial_bias:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\\\nüöÄ Starting training...\")\n",
    "train_losses, test_losses, weights_history, biases_history = train_model(\n",
    "    model, x_train_tensor, y_train_tensor, x_test_tensor, y_test_tensor,\n",
    "    learning_rate=0.01, num_epochs=200, optimizer_type='sgd'\n",
    ")\n",
    "\n",
    "# Final parameters\n",
    "final_weight, final_bias = model.get_parameters()\n",
    "print(f\"\\\\n‚úÖ Training completed!\")\n",
    "print(f\"Final parameters: Weight={final_weight:.4f}, Bias={final_bias:.4f}\")\n",
    "print(f\"Target parameters: Weight=2.0000, Bias=1.0000\")\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final test loss: {test_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb9ec5e",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Metrics\n",
    "\n",
    "Let's evaluate our trained model using various metrics and create comprehensive visualizations to understand its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d353f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with multiple metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get predictions\n",
    "        train_pred = model(x_train).cpu().numpy()\n",
    "        test_pred = model(x_test).cpu().numpy()\n",
    "        \n",
    "        # Convert to numpy\n",
    "        y_train_np = y_train.cpu().numpy()\n",
    "        y_test_np = y_test.cpu().numpy()\n",
    "        x_train_np = x_train.cpu().numpy()\n",
    "        x_test_np = x_test.cpu().numpy()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_mse = mean_squared_error(y_train_np, train_pred)\n",
    "        test_mse = mean_squared_error(y_test_np, test_pred)\n",
    "        train_mae = mean_absolute_error(y_train_np, train_pred)\n",
    "        test_mae = mean_absolute_error(y_test_np, test_pred)\n",
    "        train_r2 = r2_score(y_train_np, train_pred)\n",
    "        test_r2 = r2_score(y_test_np, test_pred)\n",
    "        \n",
    "        # Create evaluation plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(7, 6))\n",
    "        \n",
    "        # 1. Predictions vs Actual (Training)\n",
    "        axes[0, 0].scatter(y_train_np, train_pred, alpha=0.7, color='blue')\n",
    "        axes[0, 0].plot([y_train_np.min(), y_train_np.max()], \n",
    "                       [y_train_np.min(), y_train_np.max()], 'r--', lw=2)\n",
    "        axes[0, 0].set_xlabel('Actual Values')\n",
    "        axes[0, 0].set_ylabel('Predicted Values')\n",
    "        axes[0, 0].set_title(f'Training Set: Predicted vs Actual\\\\nR¬≤ = {train_r2:.4f}')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Predictions vs Actual (Test)\n",
    "        axes[0, 1].scatter(y_test_np, test_pred, alpha=0.7, color='green')\n",
    "        axes[0, 1].plot([y_test_np.min(), y_test_np.max()], \n",
    "                       [y_test_np.min(), y_test_np.max()], 'r--', lw=2)\n",
    "        axes[0, 1].set_xlabel('Actual Values')\n",
    "        axes[0, 1].set_ylabel('Predicted Values')\n",
    "        axes[0, 1].set_title(f'Test Set: Predicted vs Actual\\\\nR¬≤ = {test_r2:.4f}')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Residuals plot (Training)\n",
    "        train_residuals = y_train_np.flatten() - train_pred.flatten()\n",
    "        axes[1, 0].scatter(train_pred, train_residuals, alpha=0.7, color='blue')\n",
    "        axes[1, 0].axhline(y=0, color='red', linestyle='--')\n",
    "        axes[1, 0].set_xlabel('Predicted Values')\n",
    "        axes[1, 0].set_ylabel('Residuals')\n",
    "        axes[1, 0].set_title('Training Set: Residuals Plot')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Residuals plot (Test)\n",
    "        test_residuals = y_test_np.flatten() - test_pred.flatten()\n",
    "        axes[1, 1].scatter(test_pred, test_residuals, alpha=0.7, color='green')\n",
    "        axes[1, 1].axhline(y=0, color='red', linestyle='--')\n",
    "        axes[1, 1].set_xlabel('Predicted Values')\n",
    "        axes[1, 1].set_ylabel('Residuals')\n",
    "        axes[1, 1].set_title('Test Set: Residuals Plot')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print metrics table\n",
    "        print(\"üìä MODEL EVALUATION METRICS\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"{'Metric':<15} {'Training':<12} {'Test':<12}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'MSE':<15} {train_mse:<12.4f} {test_mse:<12.4f}\")\n",
    "        print(f\"{'MAE':<15} {train_mae:<12.4f} {test_mae:<12.4f}\")\n",
    "        print(f\"{'R¬≤ Score':<15} {train_r2:<12.4f} {test_r2:<12.4f}\")\n",
    "        print(f\"{'RMSE':<15} {np.sqrt(train_mse):<12.4f} {np.sqrt(test_mse):<12.4f}\")\n",
    "        \n",
    "        # Parameter analysis\n",
    "        weight, bias = model.get_parameters()\n",
    "        print(f\"\\\\nüéØ PARAMETER ANALYSIS\")\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Learned weight: {weight:.4f} (target: 2.0000)\")\n",
    "        print(f\"Learned bias:   {bias:.4f} (target: 1.0000)\")\n",
    "        print(f\"Weight error:   {abs(weight - 2.0):.4f}\")\n",
    "        print(f\"Bias error:     {abs(bias - 1.0):.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'train_mse': train_mse, 'test_mse': test_mse,\n",
    "            'train_mae': train_mae, 'test_mae': test_mae,\n",
    "            'train_r2': train_r2, 'test_r2': test_r2,\n",
    "            'weight': weight, 'bias': bias\n",
    "        }\n",
    "\n",
    "# Evaluate the trained model\n",
    "evaluation_results = evaluate_model(model, x_train_tensor, y_train_tensor, \n",
    "                                  x_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create final prediction plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "# Plot data and predictions\n",
    "x_plot = np.linspace(-6, 6, 100)\n",
    "x_plot_tensor = torch.FloatTensor(x_plot).unsqueeze(1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_plot = model(x_plot_tensor).cpu().numpy().squeeze()\n",
    "\n",
    "# Training data\n",
    "plt.scatter(x_train, y_train, alpha=0.7, label='Training data', color='blue', s=60)\n",
    "# Test data\n",
    "plt.scatter(x_test, y_test, alpha=0.7, label='Test data', color='green', s=60)\n",
    "\n",
    "# True function\n",
    "y_true_plot = 2 * x_plot + 1\n",
    "plt.plot(x_plot, y_true_plot, 'r--', label='True: y = 2x + 1', linewidth=3)\n",
    "\n",
    "# Model prediction\n",
    "weight, bias = model.get_parameters()\n",
    "plt.plot(x_plot, y_pred_plot, 'orange', \n",
    "         label=f'Learned: y = {weight:.2f}x + {bias:.2f}', linewidth=3)\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Final Model Performance', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add performance text\n",
    "textstr = f'Test R¬≤ = {evaluation_results[\"test_r2\"]:.4f}\\\\nTest RMSE = {np.sqrt(evaluation_results[\"test_mse\"]):.4f}'\n",
    "props = dict(boxstyle='round', facecolor='lightblue', alpha=0.8)\n",
    "plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=11,\n",
    "         verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e5bd37",
   "metadata": {},
   "source": [
    "## 8. Interactive Training Visualization\n",
    "\n",
    "Let's create real-time visualizations showing how the loss decreases and parameters evolve during training. This will help you understand the learning process dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa0bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history from previous training\n",
    "def plot_training_history(train_losses, test_losses, weights_history, biases_history):\n",
    "    \"\"\"\n",
    "    Visualize the complete training history\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(7, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[0, 0].plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, test_losses, 'r-', label='Test Loss', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('MSE Loss')\n",
    "    axes[0, 0].set_title('Loss Curves')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    \n",
    "    # Parameter evolution\n",
    "    axes[0, 1].plot(epochs, weights_history, 'g-', label='Weight', linewidth=2)\n",
    "    axes[0, 1].axhline(y=2.0, color='red', linestyle='--', alpha=0.7, label='Target Weight (2.0)')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Weight Value')\n",
    "    axes[0, 1].set_title('Weight Evolution')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, 0].plot(epochs, biases_history, 'purple', label='Bias', linewidth=2)\n",
    "    axes[1, 0].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Target Bias (1.0)')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Bias Value')\n",
    "    axes[1, 0].set_title('Bias Evolution')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Parameter trajectory in 2D space\n",
    "    axes[1, 1].plot(weights_history, biases_history, 'orange', linewidth=2, alpha=0.7)\n",
    "    axes[1, 1].scatter(weights_history[0], biases_history[0], color='green', s=100, \n",
    "                      marker='o', label='Start', zorder=5)\n",
    "    axes[1, 1].scatter(weights_history[-1], biases_history[-1], color='red', s=100, \n",
    "                      marker='*', label='End', zorder=5)\n",
    "    axes[1, 1].scatter(2, 1, color='blue', s=100, marker='X', label='Target', zorder=5)\n",
    "    axes[1, 1].set_xlabel('Weight')\n",
    "    axes[1, 1].set_ylabel('Bias')\n",
    "    axes[1, 1].set_title('Parameter Trajectory')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history from the previous section\n",
    "if 'train_losses' in globals():\n",
    "    plot_training_history(train_losses, test_losses, weights_history, biases_history)\n",
    "\n",
    "# Interactive training function with real-time monitoring\n",
    "def train_interactive_model(learning_rate=0.01, num_epochs=100, optimizer_type='sgd', \n",
    "                           initial_weight=0.0, initial_bias=0.0, show_real_time=True):\n",
    "    \"\"\"\n",
    "    Interactive training function with real-time visualization\n",
    "    \"\"\"\n",
    "    # Create a new model for this experiment\n",
    "    model_exp = LinearModel().to(device)\n",
    "    model_exp.set_parameters(initial_weight, initial_bias)\n",
    "    \n",
    "    print(f\"üéØ Training Configuration:\")\n",
    "    print(f\"   Learning Rate: {learning_rate}\")\n",
    "    print(f\"   Epochs: {num_epochs}\")\n",
    "    print(f\"   Optimizer: {optimizer_type}\")\n",
    "    print(f\"   Initial Weight: {initial_weight}\")\n",
    "    print(f\"   Initial Bias: {initial_bias}\")\n",
    "    print(f\"   Target: Weight=2.0, Bias=1.0\")\n",
    "    print(f\"   Real-time visualization: {show_real_time}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train the model\n",
    "    train_losses_exp, test_losses_exp, weights_exp, biases_exp = train_model(\n",
    "        model_exp, x_train_tensor, y_train_tensor, x_test_tensor, y_test_tensor,\n",
    "        learning_rate=learning_rate, num_epochs=num_epochs, optimizer_type=optimizer_type\n",
    "    )\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    final_weight, final_bias = model_exp.get_parameters()\n",
    "    \n",
    "    if show_real_time:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n",
    "        \n",
    "        # Loss curves\n",
    "        epochs = range(1, len(train_losses_exp) + 1)\n",
    "        axes[0, 0].plot(epochs, train_losses_exp, 'b-', label='Training', linewidth=2)\n",
    "        axes[0, 0].plot(epochs, test_losses_exp, 'r-', label='Test', linewidth=2)\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_title('Loss Evolution')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].set_yscale('log')\n",
    "        \n",
    "        # Parameter evolution\n",
    "        axes[0, 1].plot(epochs, weights_exp, 'g-', label='Weight', linewidth=2)\n",
    "        axes[0, 1].plot(epochs, biases_exp, 'purple', label='Bias', linewidth=2)\n",
    "        axes[0, 1].axhline(y=2.0, color='red', linestyle='--', alpha=0.7, label='Target Weight')\n",
    "        axes[0, 1].axhline(y=1.0, color='orange', linestyle='--', alpha=0.7, label='Target Bias')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Parameter Value')\n",
    "        axes[0, 1].set_title('Parameter Evolution')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Parameter trajectory\n",
    "        axes[1, 0].plot(weights_exp, biases_exp, 'orange', linewidth=2, alpha=0.7)\n",
    "        axes[1, 0].scatter(weights_exp[0], biases_exp[0], color='green', s=100, marker='o', label='Start')\n",
    "        axes[1, 0].scatter(weights_exp[-1], biases_exp[-1], color='red', s=100, marker='*', label='End')\n",
    "        axes[1, 0].scatter(2, 1, color='blue', s=100, marker='X', label='Target')\n",
    "        axes[1, 0].set_xlabel('Weight')\n",
    "        axes[1, 0].set_ylabel('Bias')\n",
    "        axes[1, 0].set_title('Parameter Trajectory')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Final model performance\n",
    "        x_plot = np.linspace(-6, 6, 100)\n",
    "        x_plot_tensor = torch.FloatTensor(x_plot).unsqueeze(1).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred_plot = model_exp(x_plot_tensor).cpu().numpy().squeeze()\n",
    "        \n",
    "        axes[1, 1].scatter(x_train, y_train, alpha=0.6, color='blue', s=40, label='Training data')\n",
    "        axes[1, 1].scatter(x_test, y_test, alpha=0.6, color='green', s=40, label='Test data')\n",
    "        \n",
    "        y_true_plot = 2 * x_plot + 1\n",
    "        axes[1, 1].plot(x_plot, y_true_plot, 'r--', linewidth=3, label='True function')\n",
    "        axes[1, 1].plot(x_plot, y_pred_plot, 'orange', linewidth=3, \n",
    "                       label=f'Learned: y = {final_weight:.2f}x + {final_bias:.2f}')\n",
    "        \n",
    "        axes[1, 1].set_xlabel('x')\n",
    "        axes[1, 1].set_ylabel('y')\n",
    "        axes[1, 1].set_title('Final Model Performance')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Print final results\n",
    "    print(f\"\\n‚úÖ Training Results:\")\n",
    "    print(f\"   Final Weight: {final_weight:.4f} (target: 2.0000, error: {abs(final_weight-2.0):.4f})\")\n",
    "    print(f\"   Final Bias: {final_bias:.4f} (target: 1.0000, error: {abs(final_bias-1.0):.4f})\")\n",
    "    print(f\"   Final Train Loss: {train_losses_exp[-1]:.6f}\")\n",
    "    print(f\"   Final Test Loss: {test_losses_exp[-1]:.6f}\")\n",
    "    \n",
    "    return model_exp, train_losses_exp, test_losses_exp, weights_exp, biases_exp\n",
    "\n",
    "# Create interactive widget for training hyperparameter exploration\n",
    "print(\"üéÆ Interactive Training Configuration\")\n",
    "print(\"Experiment with different hyperparameters and see how they affect training!\")\n",
    "\n",
    "# Create widgets for hyperparameter exploration\n",
    "lr_slider = widgets.FloatLogSlider(\n",
    "    value=0.01,\n",
    "    base=10,\n",
    "    min=-4,  # 10^-4 = 0.0001\n",
    "    max=-1,  # 10^-1 = 0.1\n",
    "    step=0.1,\n",
    "    description='Learning Rate:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "epochs_slider = widgets.IntSlider(\n",
    "    value=100,\n",
    "    min=10,\n",
    "    max=500,\n",
    "    step=10,\n",
    "    description='Epochs:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "optimizer_dropdown = widgets.Dropdown(\n",
    "    options=['sgd', 'adam', 'rmsprop'],\n",
    "    value='adam',\n",
    "    description='Optimizer:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "init_weight_slider = widgets.FloatSlider(\n",
    "    value=0.0,\n",
    "    min=-3.0,\n",
    "    max=3.0,\n",
    "    step=0.1,\n",
    "    description='Initial Weight:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "init_bias_slider = widgets.FloatSlider(\n",
    "    value=0.0,\n",
    "    min=-3.0,\n",
    "    max=3.0,\n",
    "    step=0.1,\n",
    "    description='Initial Bias:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "show_plots_checkbox = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Show detailed plots',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Create the interactive training widget\n",
    "interactive_training = interactive(train_interactive_model,\n",
    "                                 learning_rate=lr_slider,\n",
    "                                 num_epochs=epochs_slider,\n",
    "                                 optimizer_type=optimizer_dropdown,\n",
    "                                 initial_weight=init_weight_slider,\n",
    "                                 initial_bias=init_bias_slider,\n",
    "                                 show_real_time=show_plots_checkbox)\n",
    "\n",
    "display(interactive_training)\n",
    "\n",
    "# Add quick preset buttons for common configurations\n",
    "def set_slow_training():\n",
    "    lr_slider.value = 0.001\n",
    "    epochs_slider.value = 200\n",
    "    optimizer_dropdown.value = 'sgd'\n",
    "\n",
    "def set_fast_training():\n",
    "    lr_slider.value = 0.05\n",
    "    epochs_slider.value = 50\n",
    "    optimizer_dropdown.value = 'adam'\n",
    "\n",
    "def set_conservative():\n",
    "    lr_slider.value = 0.01\n",
    "    epochs_slider.value = 150\n",
    "    optimizer_dropdown.value = 'adam'\n",
    "\n",
    "def randomize_init():\n",
    "    init_weight_slider.value = np.random.uniform(-2, 2)\n",
    "    init_bias_slider.value = np.random.uniform(-2, 2)\n",
    "\n",
    "# Create preset buttons for training configurations\n",
    "slow_button = widgets.Button(description=\"Slow & Steady\", button_style='info')\n",
    "fast_button = widgets.Button(description=\"Fast Training\", button_style='warning')\n",
    "conservative_button = widgets.Button(description=\"Conservative\", button_style='success')\n",
    "random_init_button = widgets.Button(description=\"Random Init\", button_style='')\n",
    "\n",
    "slow_button.on_click(lambda b: set_slow_training())\n",
    "fast_button.on_click(lambda b: set_fast_training())\n",
    "conservative_button.on_click(lambda b: set_conservative())\n",
    "random_init_button.on_click(lambda b: randomize_init())\n",
    "\n",
    "print(\"\\nüîß Training Presets:\")\n",
    "training_button_box = widgets.HBox([conservative_button, slow_button, fast_button, random_init_button])\n",
    "display(training_button_box)\n",
    "\n",
    "print(\"\\nüí° Interactive Training Tips:\")\n",
    "print(\"‚Ä¢ Adjust the learning rate to see how it affects convergence speed and stability\")\n",
    "print(\"‚Ä¢ Try different optimizers - Adam usually converges faster than SGD\")\n",
    "print(\"‚Ä¢ Watch how initial parameters affect the training trajectory\")\n",
    "print(\"‚Ä¢ Use the preset buttons for quick configuration changes\")\n",
    "print(\"‚Ä¢ Toggle detailed plots on/off to focus on different aspects\")\n",
    "print(\"‚Ä¢ Higher learning rates may cause instability or divergence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762cb4b7",
   "metadata": {},
   "source": [
    "## 9. Experiment with Different Model Architectures\n",
    "\n",
    "Now let's explore how different model architectures affect performance. We'll compare simple linear regression with polynomial features and multi-layer networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad67cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Polynomial regression model with degree 2: y = ax¬≤ + bx + c\n",
    "    \"\"\"\n",
    "    def __init__(self, degree=2):\n",
    "        super(PolynomialModel, self).__init__()\n",
    "        self.degree = degree\n",
    "        self.linear = nn.Linear(degree + 1, 1)\n",
    "        nn.init.normal_(self.linear.weight, mean=0.0, std=1.0)\n",
    "        nn.init.normal_(self.linear.bias, mean=0.0, std=1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Create polynomial features [1, x, x¬≤, ...]\n",
    "        features = torch.cat([x**i for i in range(self.degree + 1)], dim=1)\n",
    "        return self.linear(features)\n",
    "\n",
    "class MultiLayerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer neural network\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=10):\n",
    "        super(MultiLayerModel, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(1, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        for layer in self.network:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def interactive_architecture_comparison(architecture='Linear', hidden_size=10, poly_degree=2, \n",
    "                                      learning_rate=0.01, epochs=200):\n",
    "    \"\"\"\n",
    "    Interactive function to compare different model architectures\n",
    "    \"\"\"\n",
    "    print(f\"üèóÔ∏è Training {architecture} Architecture...\")\n",
    "    print(f\"Configuration: LR={learning_rate}, Epochs={epochs}\")\n",
    "    \n",
    "    # Create the selected model\n",
    "    if architecture == 'Linear':\n",
    "        model = LinearModel()\n",
    "        config_info = \"Simple linear regression: y = wx + b\"\n",
    "    elif architecture == 'Polynomial':\n",
    "        model = PolynomialModel(degree=poly_degree)\n",
    "        config_info = f\"Polynomial regression (degree {poly_degree})\"\n",
    "    elif architecture == 'Multi-layer':\n",
    "        model = MultiLayerModel(hidden_size=hidden_size)\n",
    "        config_info = f\"Neural network with {hidden_size} hidden units\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model: {config_info}\")\n",
    "    print(f\"Parameters: {num_params}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train the model\n",
    "    if architecture == 'Polynomial':\n",
    "        # For polynomial models, we need special training\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(x_train_tensor)\n",
    "            train_loss = criterion(predictions, y_train_tensor)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_predictions = model(x_test_tensor)\n",
    "                test_loss = criterion(test_predictions, y_test_tensor)\n",
    "            \n",
    "            train_losses.append(train_loss.item())\n",
    "            test_losses.append(test_loss.item())\n",
    "            \n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss.item():.6f}, '\n",
    "                      f'Test Loss: {test_loss.item():.6f}')\n",
    "    else:\n",
    "        # For linear and multi-layer models\n",
    "        train_losses, test_losses, _, _ = train_model(\n",
    "            model, x_train_tensor, y_train_tensor, x_test_tensor, y_test_tensor,\n",
    "            learning_rate=learning_rate, num_epochs=epochs, optimizer_type='adam'\n",
    "        )\n",
    "    \n",
    "    final_train_loss = train_losses[-1]\n",
    "    final_test_loss = test_losses[-1]\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n",
    "    \n",
    "    # Model predictions\n",
    "    x_plot = np.linspace(-6, 6, 200)\n",
    "    x_plot_tensor = torch.FloatTensor(x_plot).unsqueeze(1).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_plot = model(x_plot_tensor).cpu().numpy().squeeze()\n",
    "        train_pred = model(x_train_tensor).cpu().numpy().squeeze()\n",
    "        test_pred = model(x_test_tensor).cpu().numpy().squeeze()\n",
    "    \n",
    "    # 1. Model predictions\n",
    "    axes[0, 0].scatter(x_train, y_train, alpha=0.6, color='blue', s=40, label='Training data')\n",
    "    axes[0, 0].scatter(x_test, y_test, alpha=0.6, color='green', s=40, label='Test data')\n",
    "    \n",
    "    y_true_plot = 2 * x_plot + 1\n",
    "    axes[0, 0].plot(x_plot, y_true_plot, 'r--', linewidth=3, label='True: y = 2x + 1')\n",
    "    axes[0, 0].plot(x_plot, y_pred_plot, 'orange', linewidth=3, label=f'{architecture} Model')\n",
    "    \n",
    "    axes[0, 0].set_xlabel('x')\n",
    "    axes[0, 0].set_ylabel('y')\n",
    "    axes[0, 0].set_title(f'{architecture} Model Predictions')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_xlim(-6, 6)\n",
    "    axes[0, 0].set_ylim(-15, 15)\n",
    "    \n",
    "    # 2. Loss curves\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "    axes[0, 1].plot(epochs_range, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "    axes[0, 1].plot(epochs_range, test_losses, 'r-', label='Test Loss', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('MSE Loss')\n",
    "    axes[0, 1].set_title('Training Progress')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    \n",
    "    # 3. Predictions vs Actual\n",
    "    all_actual = np.concatenate([y_train, y_test])\n",
    "    all_pred = np.concatenate([train_pred, test_pred])\n",
    "    \n",
    "    axes[1, 0].scatter(y_train, train_pred, alpha=0.7, color='blue', label='Training', s=50)\n",
    "    axes[1, 0].scatter(y_test, test_pred, alpha=0.7, color='green', label='Test', s=50)\n",
    "    \n",
    "    min_val, max_val = all_actual.min(), all_actual.max()\n",
    "    axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect prediction')\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Actual Values')\n",
    "    axes[1, 0].set_ylabel('Predicted Values')\n",
    "    axes[1, 0].set_title('Predicted vs Actual')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Residuals analysis\n",
    "    train_residuals = y_train - train_pred\n",
    "    test_residuals = y_test - test_pred\n",
    "    \n",
    "    axes[1, 1].scatter(train_pred, train_residuals, alpha=0.7, color='blue', label='Training', s=50)\n",
    "    axes[1, 1].scatter(test_pred, test_residuals, alpha=0.7, color='green', label='Test', s=50)\n",
    "    axes[1, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Predicted Values')\n",
    "    axes[1, 1].set_ylabel('Residuals')\n",
    "    axes[1, 1].set_title('Residuals Analysis')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and display metrics\n",
    "    train_r2 = r2_score(y_train, train_pred)\n",
    "    test_r2 = r2_score(y_test, test_pred)\n",
    "    train_mae = mean_absolute_error(y_train, train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, test_pred)\n",
    "    \n",
    "    overfitting_ratio = final_test_loss / final_train_loss\n",
    "    \n",
    "    print(f\"\\nüìä {architecture} Model Performance:\")\n",
    "    print(f\"   Parameters: {num_params}\")\n",
    "    print(f\"   Train Loss (MSE): {final_train_loss:.6f}\")\n",
    "    print(f\"   Test Loss (MSE): {final_test_loss:.6f}\")\n",
    "    print(f\"   Train R¬≤: {train_r2:.4f}\")\n",
    "    print(f\"   Test R¬≤: {test_r2:.4f}\")\n",
    "    print(f\"   Train MAE: {train_mae:.4f}\")\n",
    "    print(f\"   Test MAE: {test_mae:.4f}\")\n",
    "    print(f\"   Overfitting ratio: {overfitting_ratio:.3f}\")\n",
    "    \n",
    "    if overfitting_ratio > 1.2:\n",
    "        print(\"   ‚ö†Ô∏è  Model may be overfitting (test loss >> train loss)\")\n",
    "    elif overfitting_ratio < 1.1:\n",
    "        print(\"   ‚úÖ Good generalization (test loss ‚âà train loss)\")\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è  Moderate generalization gap\")\n",
    "    \n",
    "    return model, final_train_loss, final_test_loss, num_params\n",
    "\n",
    "# Create interactive widget for architecture comparison\n",
    "print(\"üèóÔ∏è Interactive Architecture Comparison\")\n",
    "print(\"Experiment with different model architectures and their hyperparameters!\")\n",
    "\n",
    "# Create widgets for architecture comparison\n",
    "architecture_dropdown = widgets.Dropdown(\n",
    "    options=['Linear', 'Polynomial', 'Multi-layer'],\n",
    "    value='Linear',\n",
    "    description='Architecture:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "hidden_size_slider = widgets.IntSlider(\n",
    "    value=10,\n",
    "    min=5,\n",
    "    max=100,\n",
    "    step=5,\n",
    "    description='Hidden Size:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "poly_degree_slider = widgets.IntSlider(\n",
    "    value=2,\n",
    "    min=1,\n",
    "    max=5,\n",
    "    step=1,\n",
    "    description='Poly Degree:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "arch_lr_slider = widgets.FloatLogSlider(\n",
    "    value=0.01,\n",
    "    base=10,\n",
    "    min=-4,\n",
    "    max=-1,\n",
    "    step=0.1,\n",
    "    description='Learning Rate:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "arch_epochs_slider = widgets.IntSlider(\n",
    "    value=200,\n",
    "    min=50,\n",
    "    max=500,\n",
    "    step=50,\n",
    "    description='Epochs:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Create the interactive architecture widget\n",
    "interactive_arch = interactive(interactive_architecture_comparison,\n",
    "                              architecture=architecture_dropdown,\n",
    "                              hidden_size=hidden_size_slider,\n",
    "                              poly_degree=poly_degree_slider,\n",
    "                              learning_rate=arch_lr_slider,\n",
    "                              epochs=arch_epochs_slider)\n",
    "\n",
    "display(interactive_arch)\n",
    "\n",
    "# Add quick preset buttons for architecture exploration\n",
    "def set_simple_linear():\n",
    "    architecture_dropdown.value = 'Linear'\n",
    "    arch_lr_slider.value = 0.01\n",
    "    arch_epochs_slider.value = 200\n",
    "\n",
    "def set_polynomial():\n",
    "    architecture_dropdown.value = 'Polynomial'\n",
    "    poly_degree_slider.value = 2\n",
    "    arch_lr_slider.value = 0.01\n",
    "    arch_epochs_slider.value = 300\n",
    "\n",
    "def set_small_nn():\n",
    "    architecture_dropdown.value = 'Multi-layer'\n",
    "    hidden_size_slider.value = 10\n",
    "    arch_lr_slider.value = 0.01\n",
    "    arch_epochs_slider.value = 200\n",
    "\n",
    "def set_large_nn():\n",
    "    architecture_dropdown.value = 'Multi-layer'\n",
    "    hidden_size_slider.value = 50\n",
    "    arch_lr_slider.value = 0.001\n",
    "    arch_epochs_slider.value = 300\n",
    "\n",
    "# Create preset buttons for architecture exploration\n",
    "linear_button = widgets.Button(description=\"Simple Linear\", button_style='primary')\n",
    "poly_button = widgets.Button(description=\"Polynomial\", button_style='info')\n",
    "small_nn_button = widgets.Button(description=\"Small NN\", button_style='success')\n",
    "large_nn_button = widgets.Button(description=\"Large NN\", button_style='warning')\n",
    "\n",
    "linear_button.on_click(lambda b: set_simple_linear())\n",
    "poly_button.on_click(lambda b: set_polynomial())\n",
    "small_nn_button.on_click(lambda b: set_small_nn())\n",
    "large_nn_button.on_click(lambda b: set_large_nn())\n",
    "\n",
    "print(\"\\nüîß Architecture Presets:\")\n",
    "arch_button_box = widgets.HBox([linear_button, poly_button, small_nn_button, large_nn_button])\n",
    "display(arch_button_box)\n",
    "\n",
    "print(\"\\nÔøΩ Architecture Comparison Tips:\")\n",
    "print(\"‚Ä¢ Linear: Simple and interpretable, good baseline\")\n",
    "print(\"‚Ä¢ Polynomial: Can capture non-linear patterns, but may overfit\")\n",
    "print(\"‚Ä¢ Multi-layer: Very flexible, but needs more data and careful tuning\")\n",
    "print(\"‚Ä¢ Compare the overfitting ratio: Test Loss / Train Loss\")\n",
    "print(\"‚Ä¢ Watch how model complexity affects generalization\")\n",
    "print(\"‚Ä¢ Try different hidden sizes and polynomial degrees\")\n",
    "print(\"‚Ä¢ Use the preset buttons for quick architecture comparisons!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec1727",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways and Summary\n",
    "\n",
    "Congratulations! You've completed a comprehensive journey through linear model training with PyTorch. Here are the key concepts you've learned:\n",
    "\n",
    "### üìö **Core Concepts Covered:**\n",
    "\n",
    "1. **Data Generation**: Created synthetic noisy data following `y = 2x + 1`\n",
    "2. **Model Architecture**: Implemented linear regression using PyTorch's `nn.Module`\n",
    "3. **Loss Functions**: Explored MSE, MAE, and Huber loss with 3D visualizations\n",
    "4. **Training Loop**: Complete pipeline with forward pass, backpropagation, and optimization\n",
    "5. **Evaluation**: Comprehensive metrics including MSE, MAE, R¬≤, and residual analysis\n",
    "6. **Hyperparameter Tuning**: Interactive exploration of learning rates, optimizers, and epochs\n",
    "7. **Architecture Comparison**: Linear vs. polynomial vs. multi-layer networks\n",
    "\n",
    "### üîç **Key Insights:**\n",
    "\n",
    "- **Loss Landscape**: The loss function has a single global minimum for linear regression\n",
    "- **Learning Rate**: Too high ‚Üí divergence, too low ‚Üí slow convergence\n",
    "- **Model Complexity**: More parameters don't always mean better performance\n",
    "- **Overfitting**: Complex models can memorize training data but fail on test data\n",
    "- **Optimization**: Different optimizers (SGD, Adam, RMSprop) have different convergence properties\n",
    "\n",
    "### üéÆ **Interactive Learning Benefits:**\n",
    "\n",
    "- **Visual Understanding**: Saw how parameters affect predictions in real-time\n",
    "- **Experimental Mindset**: Tested different configurations to understand trade-offs\n",
    "- **Practical Experience**: Gained hands-on experience with PyTorch training loops\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: Machine learning is both an art and a science. The interactive visualizations in this notebook help build intuition, but real-world applications require careful consideration of data quality, model validation, and deployment constraints.\n",
    "\n",
    "Happy learning! üöÄü§ñ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
