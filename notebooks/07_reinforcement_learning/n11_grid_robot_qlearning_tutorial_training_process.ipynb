{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44c2060",
   "metadata": {},
   "source": [
    "# Q-Learning Tutorial: 2D Grid Robot Navigation\n",
    "\n",
    "This notebook provides a simple and intuitive introduction to Q-learning using a 2D grid world where a robot must navigate to reach rewards while avoiding traps.\n",
    "\n",
    "## Problem Description\n",
    "\n",
    "We have a robot in a 2D grid world that needs to:\n",
    "- Start at a specific position\n",
    "- Navigate to reach reward cells (+10 points)\n",
    "- Avoid trap cells (-10 points)\n",
    "- Each step costs -1 point (to encourage shorter paths)\n",
    "- Find the optimal path to maximize total reward\n",
    "\n",
    "## Grid World Setup\n",
    "\n",
    "- **Grid Size**: 5x5\n",
    "- **Robot**: Starts at position (0, 0)\n",
    "- **Goal/Reward**: Position (4, 4) gives +10 reward\n",
    "- **Traps**: Positions (1, 1), (2, 3), (3, 1) give -10 reward\n",
    "- **Actions**: Up, Down, Left, Right\n",
    "- **Step Cost**: -1 for each move\n",
    "\n",
    "## Q-Learning Basics\n",
    "\n",
    "Q-learning learns the \"quality\" (Q-value) of taking each action in each state:\n",
    "\n",
    "**Q-Update Rule**: $Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$\n",
    "\n",
    "Where:\n",
    "- $s$: current state (position)\n",
    "- $a$: action taken\n",
    "- $s'$: next state\n",
    "- $r$: immediate reward\n",
    "- $\\alpha$: learning rate (0.1)\n",
    "- $\\gamma$: discount factor (0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d2c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.colors import ListedColormap\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca924591",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a52caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.start_pos = (0, 0)\n",
    "        self.goal_pos = (4, 4)\n",
    "        self.traps = [(1, 1), (2, 3), (3, 1)]  # Trap positions\n",
    "        \n",
    "        # Define rewards\n",
    "        self.goal_reward = 10\n",
    "        self.trap_reward = -10\n",
    "        self.step_reward = -1\n",
    "        \n",
    "        # Actions: 0=Up, 1=Down, 2=Left, 3=Right\n",
    "        self.actions = ['Up', 'Down', 'Left', 'Right']\n",
    "        self.action_effects = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset robot to start position\"\"\"\n",
    "        self.robot_pos = list(self.start_pos)\n",
    "        return tuple(self.robot_pos)\n",
    "    \n",
    "    def is_valid_position(self, pos):\n",
    "        \"\"\"Check if position is within grid bounds\"\"\"\n",
    "        row, col = pos\n",
    "        return 0 <= row < self.size and 0 <= col < self.size\n",
    "    \n",
    "    def get_reward(self, pos):\n",
    "        \"\"\"Get reward for being in a specific position\"\"\"\n",
    "        if pos == self.goal_pos:\n",
    "            return self.goal_reward\n",
    "        elif pos in self.traps:\n",
    "            return self.trap_reward\n",
    "        else:\n",
    "            return self.step_reward\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return (next_state, reward, done)\"\"\"\n",
    "        # Calculate new position\n",
    "        dr, dc = self.action_effects[action]\n",
    "        new_pos = (self.robot_pos[0] + dr, self.robot_pos[1] + dc)\n",
    "        \n",
    "        # Check if new position is valid\n",
    "        if self.is_valid_position(new_pos):\n",
    "            self.robot_pos = list(new_pos)\n",
    "        # If invalid, robot stays in same position\n",
    "        \n",
    "        current_pos = tuple(self.robot_pos)\n",
    "        reward = self.get_reward(current_pos)\n",
    "        \n",
    "        # Episode ends if robot reaches goal or trap\n",
    "        done = (current_pos == self.goal_pos) or (current_pos in self.traps)\n",
    "        \n",
    "        return current_pos, reward, done\n",
    "    \n",
    "    def visualize(self, q_table=None, show_values=False):\n",
    "        \"\"\"Visualize the grid world\"\"\"\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        \n",
    "        # Create grid visualization\n",
    "        grid = np.zeros((self.size, self.size))\n",
    "        \n",
    "        # Mark special positions\n",
    "        for trap in self.traps:\n",
    "            grid[trap] = -1  # Traps\n",
    "        grid[self.goal_pos] = 1  # Goal\n",
    "        grid[self.start_pos] = 0.5  # Start\n",
    "        \n",
    "        # Color map: red for traps, green for goal, blue for start, white for empty\n",
    "        colors = ['red', 'white', 'lightblue', 'green']\n",
    "        cmap = ListedColormap(colors)\n",
    "        \n",
    "        im = ax.imshow(grid, cmap=cmap, vmin=-1, vmax=1)\n",
    "        \n",
    "        # Add grid lines\n",
    "        for i in range(self.size + 1):\n",
    "            ax.axhline(i - 0.5, color='black', linewidth=1)\n",
    "            ax.axvline(i - 0.5, color='black', linewidth=1)\n",
    "        \n",
    "        # Add labels\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if (i, j) == self.start_pos:\n",
    "                    ax.text(j, i, 'START', ha='center', va='center', fontweight='bold')\n",
    "                elif (i, j) == self.goal_pos:\n",
    "                    ax.text(j, i, 'GOAL\\n+10', ha='center', va='center', fontweight='bold')\n",
    "                elif (i, j) in self.traps:\n",
    "                    ax.text(j, i, 'TRAP\\n-10', ha='center', va='center', fontweight='bold')\n",
    "                elif show_values and q_table is not None:\n",
    "                    # Show best action value\n",
    "                    max_q = np.max(q_table[i, j])\n",
    "                    ax.text(j, i, f'{max_q:.1f}', ha='center', va='center', fontsize=10)\n",
    "        \n",
    "        # Add robot position\n",
    "        robot_circle = plt.Circle((self.robot_pos[1], self.robot_pos[0]), 0.3, \n",
    "                                color='orange', zorder=5)\n",
    "        ax.add_patch(robot_circle)\n",
    "        \n",
    "        ax.set_xlim(-0.5, self.size - 0.5)\n",
    "        ax.set_ylim(-0.5, self.size - 0.5)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_title('Grid World - Robot Navigation', fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create and visualize the environment\n",
    "env = GridWorld()\n",
    "print(\"Grid World Environment:\")\n",
    "print(f\"- Grid size: {env.size}x{env.size}\")\n",
    "print(f\"- Start position: {env.start_pos}\")\n",
    "print(f\"- Goal position: {env.goal_pos} (reward: +{env.goal_reward})\")\n",
    "print(f\"- Trap positions: {env.traps} (reward: {env.trap_reward} each)\")\n",
    "print(f\"- Step cost: {env.step_reward}\")\n",
    "print(f\"- Actions: {env.actions}\")\n",
    "\n",
    "env.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e664cab",
   "metadata": {},
   "source": [
    "## Q-Learning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adf7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, grid_size, n_actions, learning_rate=0.1, discount_factor=0.9, epsilon=1.0):\n",
    "        self.grid_size = grid_size\n",
    "        self.n_actions = n_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Initialize Q-table with zeros\n",
    "        # Shape: (grid_size, grid_size, n_actions)\n",
    "        self.q_table = np.zeros((grid_size, grid_size, n_actions))\n",
    "        \n",
    "        # Tracking for visualization and analysis\n",
    "        self.q_history = []  # Store Q-table snapshots\n",
    "        self.epsilon_history = []  # Track epsilon values\n",
    "        self.q_updates = []  # Store Q-update examples\n",
    "        self.action_counts = {'explore': 0, 'exploit': 0}  # Count exploration vs exploitation\n",
    "    \n",
    "    def choose_action(self, state, track_policy=False):\n",
    "        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "        explore = random.random() < self.epsilon\n",
    "        \n",
    "        if explore:\n",
    "            # Explore: choose random action\n",
    "            action = random.randint(0, self.n_actions - 1)\n",
    "            if track_policy:\n",
    "                self.action_counts['explore'] += 1\n",
    "        else:\n",
    "            # Exploit: choose best action\n",
    "            row, col = state\n",
    "            action = np.argmax(self.q_table[row, col])\n",
    "            if track_policy:\n",
    "                self.action_counts['exploit'] += 1\n",
    "        \n",
    "        if track_policy:\n",
    "            return action, explore\n",
    "        else:\n",
    "            return action\n",
    "    \n",
    "    def update_q_table(self, state, action, reward, next_state, done, track_update=False):\n",
    "        \"\"\"Update Q-table using Q-learning update rule\"\"\"\n",
    "        row, col = state\n",
    "        next_row, next_col = next_state\n",
    "        \n",
    "        # Current Q-value\n",
    "        current_q = self.q_table[row, col, action]\n",
    "        \n",
    "        # Maximum Q-value for next state (0 if terminal state)\n",
    "        if done:\n",
    "            max_next_q = 0\n",
    "        else:\n",
    "            max_next_q = np.max(self.q_table[next_row, next_col])\n",
    "        \n",
    "        # Q-learning update rule\n",
    "        td_target = reward + self.discount_factor * max_next_q\n",
    "        td_error = td_target - current_q\n",
    "        new_q = current_q + self.learning_rate * td_error\n",
    "        \n",
    "        # Store update information for tracking\n",
    "        if track_update:\n",
    "            update_info = {\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'reward': reward,\n",
    "                'next_state': next_state,\n",
    "                'current_q': current_q,\n",
    "                'max_next_q': max_next_q,\n",
    "                'td_target': td_target,\n",
    "                'td_error': td_error,\n",
    "                'new_q': new_q,\n",
    "                'done': done\n",
    "            }\n",
    "            self.q_updates.append(update_info)\n",
    "        \n",
    "        self.q_table[row, col, action] = new_q\n",
    "    \n",
    "    def decay_epsilon(self, decay_rate=0.995, min_epsilon=0.01):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        self.epsilon = max(self.epsilon * decay_rate, min_epsilon)\n",
    "        self.epsilon_history.append(self.epsilon)\n",
    "    \n",
    "    def save_q_snapshot(self, episode):\n",
    "        \"\"\"Save a snapshot of the current Q-table\"\"\"\n",
    "        self.q_history.append({\n",
    "            'episode': episode,\n",
    "            'q_table': self.q_table.copy(),\n",
    "            'epsilon': self.epsilon\n",
    "        })\n",
    "    \n",
    "    def get_policy(self):\n",
    "        \"\"\"Get the learned policy (best action for each state)\"\"\"\n",
    "        policy = np.zeros((self.grid_size, self.grid_size), dtype=int)\n",
    "        for i in range(self.grid_size):\n",
    "            for j in range(self.grid_size):\n",
    "                policy[i, j] = np.argmax(self.q_table[i, j])\n",
    "        return policy\n",
    "\n",
    "# Initialize the Q-learning agent\n",
    "agent = QLearningAgent(grid_size=5, n_actions=4, learning_rate=0.1, discount_factor=0.9, epsilon=1.0)\n",
    "\n",
    "print(\"Enhanced Q-Learning Agent initialized:\")\n",
    "print(f\"- Grid size: {agent.grid_size}x{agent.grid_size}\")\n",
    "print(f\"- Number of actions: {agent.n_actions}\")\n",
    "print(f\"- Learning rate (Œ±): {agent.learning_rate}\")\n",
    "print(f\"- Discount factor (Œ≥): {agent.discount_factor}\")\n",
    "print(f\"- Initial exploration rate (Œµ): {agent.epsilon}\")\n",
    "print(f\"- Q-table shape: {agent.q_table.shape}\")\n",
    "print(\"- Enhanced with tracking for Q-evolution, epsilon policy, and Q-updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d250fba",
   "metadata": {},
   "source": [
    "## Training the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe0040c",
   "metadata": {},
   "source": [
    "## Understanding Epsilon-Greedy Policy\n",
    "\n",
    "Before training, let's understand how the epsilon-greedy policy works in action selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197bc85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_epsilon_greedy_policy(agent, state=(2, 2), n_samples=1000):\n",
    "    \"\"\"Demonstrate epsilon-greedy policy in action\"\"\"\n",
    "    \n",
    "    # Test with different epsilon values\n",
    "    epsilon_values = [1.0, 0.5, 0.1, 0.0]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(6, 5))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    action_names = ['Up', 'Down', 'Left', 'Right']\n",
    "    \n",
    "    for idx, epsilon_test in enumerate(epsilon_values):\n",
    "        # Temporarily change epsilon\n",
    "        original_epsilon = agent.epsilon\n",
    "        agent.epsilon = epsilon_test\n",
    "        \n",
    "        # Count actions taken\n",
    "        action_counts = {i: 0 for i in range(4)}\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            action = agent.choose_action(state)\n",
    "            action_counts[action] += 1\n",
    "        \n",
    "        # Restore original epsilon\n",
    "        agent.epsilon = original_epsilon\n",
    "        \n",
    "        # Plot results\n",
    "        ax = axes[idx]\n",
    "        actions = list(action_counts.keys())\n",
    "        counts = [action_counts[i] for i in actions]\n",
    "        percentages = [count/n_samples*100 for count in counts]\n",
    "        \n",
    "        bars = ax.bar([action_names[i] for i in actions], percentages, \n",
    "                     color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
    "        \n",
    "        # Add percentage labels on bars\n",
    "        for bar, pct in zip(bars, percentages):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                   f'{pct:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        ax.set_title(f'Œµ = {epsilon_test} ({\"Pure Exploration\" if epsilon_test == 1.0 else \"Pure Exploitation\" if epsilon_test == 0.0 else f\"{epsilon_test*100}% Exploration\"})', \n",
    "                    fontweight='bold')\n",
    "        ax.set_ylabel('Selection Frequency (%)')\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Show Q-values for reference\n",
    "        row, col = state\n",
    "        q_vals = agent.q_table[row, col]\n",
    "        best_action = np.argmax(q_vals)\n",
    "        ax.text(0.02, 0.98, f'Q-values: {[f\"{q:.2f}\" for q in q_vals]}\\nBest action: {action_names[best_action]}', \n",
    "               transform=ax.transAxes, verticalalignment='top', fontsize=8,\n",
    "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.suptitle(f'Epsilon-Greedy Policy Demonstration\\nState: {state}, Sample size: {n_samples}', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Epsilon-Greedy Policy Explanation:\")\n",
    "    print(\"‚Ä¢ Œµ = 1.0: Pure exploration - all actions chosen randomly (25% each)\")\n",
    "    print(\"‚Ä¢ Œµ = 0.5: 50% exploration, 50% exploitation\")\n",
    "    print(\"‚Ä¢ Œµ = 0.1: 10% exploration, 90% exploitation\")\n",
    "    print(\"‚Ä¢ Œµ = 0.0: Pure exploitation - always choose best action\")\n",
    "    print()\n",
    "    print(\"This balance allows the agent to:\")\n",
    "    print(\"1. Explore new actions early in training (high Œµ)\")\n",
    "    print(\"2. Gradually shift to exploiting learned knowledge (decreasing Œµ)\")\n",
    "    print(\"3. Maintain some exploration to handle environment changes\")\n",
    "\n",
    "# Demonstrate epsilon-greedy policy\n",
    "demonstrate_epsilon_greedy_policy(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a666119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_episodes = 1000\n",
    "max_steps_per_episode = 50\n",
    "\n",
    "# Track training progress\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "success_rate = []\n",
    "\n",
    "# Episodes at which to save Q-table snapshots for evolution visualization\n",
    "snapshot_episodes = [0, 10, 50, 100, 200, 500, 999]\n",
    "\n",
    "print(\"Starting Q-learning training...\")\n",
    "print(f\"Episodes: {n_episodes}, Max steps per episode: {max_steps_per_episode}\")\n",
    "print(f\"Tracking Q-evolution at episodes: {snapshot_episodes}\")\n",
    "print()\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    # Reset environment\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    # Reset action counts for this episode\n",
    "    agent.action_counts = {'explore': 0, 'exploit': 0}\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Choose action (track policy for first few episodes)\n",
    "        track_policy = episode < 5\n",
    "        if track_policy:\n",
    "            action, explored = agent.choose_action(state, track_policy=True)\n",
    "        else:\n",
    "            action = agent.choose_action(state)\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # Update Q-table (track updates for first few episodes)\n",
    "        track_update = episode < 3 and len(agent.q_updates) < 20\n",
    "        agent.update_q_table(state, action, reward, next_state, done, track_update=track_update)\n",
    "        \n",
    "        # Update tracking variables\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Decay exploration rate\n",
    "    agent.decay_epsilon()\n",
    "    \n",
    "    # Save Q-table snapshot at specific episodes\n",
    "    if episode in snapshot_episodes:\n",
    "        agent.save_q_snapshot(episode)\n",
    "    \n",
    "    # Record episode statistics\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_lengths.append(steps)\n",
    "    \n",
    "    # Calculate success rate (reaching goal) over last 100 episodes\n",
    "    if episode >= 99:\n",
    "        recent_rewards = episode_rewards[-100:]\n",
    "        successes = sum(1 for r in recent_rewards if r > 0)  # Positive reward means reached goal\n",
    "        success_rate.append(successes / 100.0)\n",
    "    \n",
    "    # Print progress with epsilon-greedy policy info for first few episodes\n",
    "    if episode < 5:\n",
    "        explore_pct = agent.action_counts['explore'] / (agent.action_counts['explore'] + agent.action_counts['exploit']) * 100\n",
    "        print(f\"Episode {episode + 1:2d}: Reward = {total_reward:6.1f}, Steps = {steps:2d}, \"\n",
    "              f\"Œµ = {agent.epsilon:.3f}, Explore = {explore_pct:.1f}% ({agent.action_counts['explore']}/{agent.action_counts['explore'] + agent.action_counts['exploit']} actions)\")\n",
    "    elif (episode + 1) % 100 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-100:])\n",
    "        avg_length = np.mean(episode_lengths[-100:])\n",
    "        current_success_rate = success_rate[-1] if success_rate else 0\n",
    "        print(f\"Episode {episode + 1:4d}: Avg Reward = {avg_reward:6.2f}, \"\n",
    "              f\"Avg Length = {avg_length:5.1f}, Success Rate = {current_success_rate:.2%}, \"\n",
    "              f\"Œµ = {agent.epsilon:.3f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Total Q-table snapshots saved: {len(agent.q_history)}\")\n",
    "print(f\"Q-update examples tracked: {len(agent.q_updates)}\")\n",
    "print(f\"Epsilon decay history length: {len(agent.epsilon_history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ce772",
   "metadata": {},
   "source": [
    "## Visualizing Training Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f27f7",
   "metadata": {},
   "source": [
    "## Q-Update Examples During Training\n",
    "\n",
    "Let's examine specific Q-value updates that occurred during the first few episodes to understand how the agent learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac97f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_q_updates(agent, max_updates=10):\n",
    "    \"\"\"Display detailed Q-update examples from training\"\"\"\n",
    "    \n",
    "    if not agent.q_updates:\n",
    "        print(\"No Q-updates were tracked during training.\")\n",
    "        return\n",
    "    \n",
    "    print(\"üîç Q-Learning Update Examples\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Q-Update Rule: Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥‚ãÖmax Q(s',a') - Q(s,a)]\")\n",
    "    print(f\"Parameters: Œ± = {agent.learning_rate}, Œ≥ = {agent.discount_factor}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    action_names = ['Up', 'Down', 'Left', 'Right']\n",
    "    \n",
    "    for i, update in enumerate(agent.q_updates[:max_updates]):\n",
    "        print(f\"\\nüìç Update #{i+1}:\")\n",
    "        print(f\"   State: {update['state']} ‚Üí Action: {action_names[update['action']]} ‚Üí Next State: {update['next_state']}\")\n",
    "        print(f\"   Reward: {update['reward']}\")\n",
    "        print(f\"   Terminal: {'Yes' if update['done'] else 'No'}\")\n",
    "        \n",
    "        print(f\"\\n   üßÆ Calculation Details:\")\n",
    "        print(f\"   ‚Ä¢ Current Q(s,a) = {update['current_q']:.3f}\")\n",
    "        print(f\"   ‚Ä¢ Reward (r) = {update['reward']}\")\n",
    "        if update['done']:\n",
    "            print(f\"   ‚Ä¢ max Q(s',a') = 0 (terminal state)\")\n",
    "        else:\n",
    "            print(f\"   ‚Ä¢ max Q(s',a') = {update['max_next_q']:.3f}\")\n",
    "        print(f\"   ‚Ä¢ TD Target = r + Œ≥‚ãÖmax Q(s',a') = {update['reward']} + {agent.discount_factor}√ó{update['max_next_q']:.3f} = {update['td_target']:.3f}\")\n",
    "        print(f\"   ‚Ä¢ TD Error = {update['td_target']:.3f} - {update['current_q']:.3f} = {update['td_error']:.3f}\")\n",
    "        print(f\"   ‚Ä¢ New Q(s,a) = {update['current_q']:.3f} + {agent.learning_rate}√ó{update['td_error']:.3f} = {update['new_q']:.3f}\")\n",
    "        \n",
    "        change = update['new_q'] - update['current_q']\n",
    "        direction = \"‚ÜóÔ∏è\" if change > 0 else \"‚ÜòÔ∏è\" if change < 0 else \"‚û°Ô∏è\"\n",
    "        print(f\"   ‚Ä¢ Change: {change:+.3f} {direction}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Display Q-update examples\n",
    "display_q_updates(agent, max_updates=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da9bed",
   "metadata": {},
   "source": [
    "## Q-Function Evolution During Training\n",
    "\n",
    "Watch how the Q-values evolve from random initialization to learned optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e2f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_q_evolution(agent, env):\n",
    "    \"\"\"Visualize the evolution of Q-values during training\"\"\"\n",
    "    \n",
    "    if not agent.q_history:\n",
    "        print(\"No Q-table snapshots were saved during training.\")\n",
    "        return\n",
    "    \n",
    "    n_snapshots = len(agent.q_history)\n",
    "    action_names = ['Up', 'Down', 'Left', 'Right']\n",
    "    \n",
    "    # Create subplot grid\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    for idx, snapshot in enumerate(agent.q_history[:8]):  # Show up to 8 snapshots\n",
    "        row = idx // 4\n",
    "        col = idx % 4\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Calculate maximum Q-values for each state\n",
    "        max_q_values = np.max(snapshot['q_table'], axis=2)\n",
    "        \n",
    "        # Create heatmap\n",
    "        im = ax.imshow(max_q_values, cmap='RdYlGn', aspect='equal', vmin=-10, vmax=10)\n",
    "        \n",
    "        # Add grid lines\n",
    "        for i in range(env.size + 1):\n",
    "            ax.axhline(i - 0.5, color='black', linewidth=0.5)\n",
    "            ax.axvline(i - 0.5, color='black', linewidth=0.5)\n",
    "        \n",
    "        # Mark special positions\n",
    "        ax.add_patch(Rectangle((env.start_pos[1]-0.4, env.start_pos[0]-0.4), 0.8, 0.8, \n",
    "                              fill=False, edgecolor='blue', linewidth=3))\n",
    "        ax.add_patch(Rectangle((env.goal_pos[1]-0.4, env.goal_pos[0]-0.4), 0.8, 0.8, \n",
    "                              fill=False, edgecolor='green', linewidth=3))\n",
    "        for trap in env.traps:\n",
    "            ax.add_patch(Rectangle((trap[1]-0.4, trap[0]-0.4), 0.8, 0.8, \n",
    "                                  fill=False, edgecolor='red', linewidth=3))\n",
    "        \n",
    "        # Add Q-values as text\n",
    "        for i in range(env.size):\n",
    "            for j in range(env.size):\n",
    "                value = max_q_values[i, j]\n",
    "                color = 'white' if abs(value) > 5 else 'black'\n",
    "                ax.text(j, i, f'{value:.1f}', ha='center', va='center', \n",
    "                       color=color, fontweight='bold', fontsize=8)\n",
    "        \n",
    "        ax.set_title(f'Episode {snapshot[\"episode\"]}\\nŒµ = {snapshot[\"epsilon\"]:.3f}', \n",
    "                    fontweight='bold')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(agent.q_history), 8):\n",
    "        row = idx // 4\n",
    "        col = idx % 4\n",
    "        axes[row, col].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Q-Function Evolution: Maximum Q-Values per State', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = fig.colorbar(im, ax=axes, shrink=0.8, aspect=20)\n",
    "    cbar.set_label('Maximum Q-Value', rotation=270, labelpad=20)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Show epsilon decay\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(agent.epsilon_history, linewidth=2, color='purple')\n",
    "    plt.title('Epsilon Decay During Training', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon (Œµ)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=0.01, color='red', linestyle='--', alpha=0.7, label='Min epsilon (0.01)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üéØ Q-Function Evolution Insights:\")\n",
    "    print(f\"‚Ä¢ Started with all zeros (Episode 0)\")\n",
    "    print(f\"‚Ä¢ Gradually learned state values through experience\")\n",
    "    print(f\"‚Ä¢ Goal state area shows high positive values (green)\")\n",
    "    print(f\"‚Ä¢ Trap areas show negative values (red)\")\n",
    "    print(f\"‚Ä¢ Epsilon decayed from {agent.q_history[0]['epsilon']:.3f} to {agent.q_history[-1]['epsilon']:.3f}\")\n",
    "    print(f\"‚Ä¢ Values stabilized as exploration decreased\")\n",
    "\n",
    "# Visualize Q-function evolution\n",
    "visualize_q_evolution(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1547873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_action_q_evolution(agent, state=(2, 2)):\n",
    "    \"\"\"Visualize how Q-values for each action evolve at a specific state\"\"\"\n",
    "    \n",
    "    if not agent.q_history:\n",
    "        print(\"No Q-table snapshots were saved during training.\")\n",
    "        return\n",
    "    \n",
    "    action_names = ['Up', 'Down', 'Left', 'Right']\n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']\n",
    "    \n",
    "    # Extract Q-values for the specific state across episodes\n",
    "    episodes = [snapshot['episode'] for snapshot in agent.q_history]\n",
    "    q_evolution = {action: [] for action in range(4)}\n",
    "    \n",
    "    for snapshot in agent.q_history:\n",
    "        row, col = state\n",
    "        for action in range(4):\n",
    "            q_evolution[action].append(snapshot['q_table'][row, col, action])\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    for action in range(4):\n",
    "        plt.plot(episodes, q_evolution[action], marker='o', linewidth=2, \n",
    "                color=colors[action], label=f'{action_names[action]}', markersize=6)\n",
    "    \n",
    "    plt.title(f'Q-Value Evolution for State {state}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Q-Value')\n",
    "    plt.legend(title='Actions', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations for key insights\n",
    "    final_q_values = [q_evolution[action][-1] for action in range(4)]\n",
    "    best_action = np.argmax(final_q_values)\n",
    "    \n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.text(0.02, 0.98, f'Final Best Action: {action_names[best_action]}\\nFinal Q-values: {[f\"{q:.2f}\" for q in final_q_values]}', \n",
    "             transform=plt.gca().transAxes, verticalalignment='top', fontsize=10,\n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show a comparison table\n",
    "    print(f\"\\nüìä Q-Value Evolution Summary for State {state}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Episode':<10} {'Up':<8} {'Down':<8} {'Left':<8} {'Right':<8} {'Best Action'}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, episode in enumerate(episodes):\n",
    "        q_vals = [q_evolution[action][i] for action in range(4)]\n",
    "        best_action_idx = np.argmax(q_vals)\n",
    "        print(f\"{episode:<10} {q_vals[0]:<8.2f} {q_vals[1]:<8.2f} {q_vals[2]:<8.2f} {q_vals[3]:<8.2f} {action_names[best_action_idx]}\")\n",
    "\n",
    "# Show action-specific Q-value evolution for a central state\n",
    "visualize_action_q_evolution(agent, state=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f5320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced training progress plots\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(7, 5))\n",
    "\n",
    "# Plot 1: Episode rewards\n",
    "ax1.plot(episode_rewards, alpha=0.3, color='blue', label='Episode Reward')\n",
    "# Moving average\n",
    "if len(episode_rewards) > 50:\n",
    "    moving_avg = []\n",
    "    window = 50\n",
    "    for i in range(window, len(episode_rewards)):\n",
    "        moving_avg.append(np.mean(episode_rewards[i-window:i]))\n",
    "    ax1.plot(range(window, len(episode_rewards)), moving_avg, color='red', linewidth=2, label='Moving Average (50)')\n",
    "\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('Training Rewards Over Time')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Episode lengths\n",
    "ax2.plot(episode_lengths, alpha=0.3, color='green', label='Episode Length')\n",
    "# Moving average\n",
    "if len(episode_lengths) > 50:\n",
    "    moving_avg_length = []\n",
    "    for i in range(window, len(episode_lengths)):\n",
    "        moving_avg_length.append(np.mean(episode_lengths[i-window:i]))\n",
    "    ax2.plot(range(window, len(episode_lengths)), moving_avg_length, color='darkgreen', linewidth=2, label='Moving Average (50)')\n",
    "\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Steps to Complete')\n",
    "ax2.set_title('Episode Length Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Success rate and epsilon decay\n",
    "ax3_twin = ax3.twinx()\n",
    "\n",
    "# Success rate\n",
    "if success_rate:\n",
    "    line1 = ax3.plot(range(100, 100 + len(success_rate)), success_rate, color='purple', linewidth=2, label='Success Rate')\n",
    "    ax3.set_ylabel('Success Rate', color='purple')\n",
    "    ax3.tick_params(axis='y', labelcolor='purple')\n",
    "\n",
    "# Epsilon decay\n",
    "if agent.epsilon_history:\n",
    "    line2 = ax3_twin.plot(range(len(agent.epsilon_history)), agent.epsilon_history, color='orange', linewidth=2, label='Epsilon (Œµ)')\n",
    "    ax3_twin.set_ylabel('Epsilon (Œµ)', color='orange')\n",
    "    ax3_twin.tick_params(axis='y', labelcolor='orange')\n",
    "\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_title('Success Rate & Epsilon Decay')\n",
    "ax3.set_ylim(0, 1)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax3.get_legend_handles_labels()\n",
    "lines2, labels2 = ax3_twin.get_legend_handles_labels()\n",
    "ax3.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "\n",
    "# Plot 4: Q-values heatmap for one action (Right action)\n",
    "im = ax4.imshow(agent.q_table[:, :, 3], cmap='RdYlBu_r', aspect='equal')\n",
    "ax4.set_title('Final Q-Values Heatmap (Right Action)')\n",
    "ax4.set_xlabel('Column')\n",
    "ax4.set_ylabel('Row')\n",
    "\n",
    "# Add text annotations for Q-values\n",
    "for i in range(agent.grid_size):\n",
    "    for j in range(agent.grid_size):\n",
    "        text = ax4.text(j, i, f'{agent.q_table[i, j, 3]:.1f}', \n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add colorbar for the heatmap\n",
    "cbar = plt.colorbar(im, ax=ax4)\n",
    "cbar.set_label('Q-Value')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print enhanced final statistics\n",
    "print(\"Enhanced Training Statistics:\")\n",
    "print(f\"- Total episodes: {len(episode_rewards)}\")\n",
    "print(f\"- Average reward (last 100 episodes): {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "print(f\"- Average episode length (last 100 episodes): {np.mean(episode_lengths[-100:]):.1f} steps\")\n",
    "if success_rate:\n",
    "    print(f\"- Final success rate: {success_rate[-1]:.2%}\")\n",
    "print(f\"- Final exploration rate (Œµ): {agent.epsilon:.3f}\")\n",
    "print(f\"- Epsilon decay: {agent.epsilon_history[0]:.3f} ‚Üí {agent.epsilon_history[-1]:.3f}\")\n",
    "print(f\"- Q-updates tracked: {len(agent.q_updates)}\")\n",
    "print(f\"- Q-table snapshots saved: {len(agent.q_history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb2530b",
   "metadata": {},
   "source": [
    "## Visualizing the Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e4f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(agent, env):\n",
    "    \"\"\"Visualize the learned policy with arrows showing best actions\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    # Get the learned policy\n",
    "    policy = agent.get_policy()\n",
    "    \n",
    "    # Action symbols and directions\n",
    "    action_symbols = ['‚Üë', '‚Üì', '‚Üê', '‚Üí']\n",
    "    arrow_dirs = [(-0.3, 0), (0.3, 0), (0, -0.3), (0, 0.3)]  # (dy, dx) for arrows\n",
    "    \n",
    "    # Plot 1: Policy with arrows\n",
    "    grid = np.zeros((env.size, env.size))\n",
    "    for trap in env.traps:\n",
    "        grid[trap] = -1\n",
    "    grid[env.goal_pos] = 1\n",
    "    grid[env.start_pos] = 0.5\n",
    "    \n",
    "    colors = ['red', 'white', 'lightblue', 'green']\n",
    "    cmap = ListedColormap(colors)\n",
    "    \n",
    "    ax1.imshow(grid, cmap=cmap, vmin=-1, vmax=1)\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(env.size + 1):\n",
    "        ax1.axhline(i - 0.5, color='black', linewidth=1)\n",
    "        ax1.axvline(i - 0.5, color='black', linewidth=1)\n",
    "    \n",
    "    # Add policy arrows and labels\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            if (i, j) == env.start_pos:\n",
    "                ax1.text(j, i-0.3, 'START', ha='center', va='center', fontweight='bold', fontsize=8)\n",
    "            elif (i, j) == env.goal_pos:\n",
    "                ax1.text(j, i-0.3, 'GOAL', ha='center', va='center', fontweight='bold', fontsize=8)\n",
    "                ax1.text(j, i+0.3, '+10', ha='center', va='center', fontweight='bold', fontsize=8)\n",
    "            elif (i, j) in env.traps:\n",
    "                ax1.text(j, i-0.3, 'TRAP', ha='center', va='center', fontweight='bold', fontsize=8)\n",
    "                ax1.text(j, i+0.3, '-10', ha='center', va='center', fontweight='bold', fontsize=8)\n",
    "            else:\n",
    "                # Show best action with arrow\n",
    "                best_action = policy[i, j]\n",
    "                dy, dx = arrow_dirs[best_action]\n",
    "                ax1.arrow(j, i, dx, dy, head_width=0.1, head_length=0.1, \n",
    "                         fc='black', ec='black', linewidth=2)\n",
    "                # Show action symbol\n",
    "                ax1.text(j, i+0.4, action_symbols[best_action], ha='center', va='center', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax1.set_xlim(-0.5, env.size - 0.5)\n",
    "    ax1.set_ylim(-0.5, env.size - 0.5)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.set_title('Learned Policy (Best Actions)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    \n",
    "    # Plot 2: Q-values heatmap (maximum Q-value per state)\n",
    "    max_q_values = np.max(agent.q_table, axis=2)\n",
    "    im = ax2.imshow(max_q_values, cmap='RdYlGn', aspect='equal')\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(env.size + 1):\n",
    "        ax2.axhline(i - 0.5, color='black', linewidth=1)\n",
    "        ax2.axvline(i - 0.5, color='black', linewidth=1)\n",
    "    \n",
    "    # Add Q-value text\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            ax2.text(j, i, f'{max_q_values[i, j]:.1f}', ha=\"center\", va=\"center\", \n",
    "                    color=\"black\", fontweight='bold')\n",
    "    \n",
    "    ax2.set_title('Maximum Q-Values per State', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Column')\n",
    "    ax2.set_ylabel('Row')\n",
    "    plt.colorbar(im, ax=ax2, label='Q-Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the learned policy\n",
    "visualize_policy(agent, env)\n",
    "\n",
    "# Print policy interpretation\n",
    "print(\"Policy Interpretation:\")\n",
    "print(\"- Arrows show the best action to take from each state\")\n",
    "print(\"- Higher Q-values (green) indicate more valuable states\")\n",
    "print(\"- Lower Q-values (red) indicate less valuable or dangerous states\")\n",
    "print(\"- The agent learned to avoid traps and find efficient paths to the goal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd36b6b",
   "metadata": {},
   "source": [
    "## Testing the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69f3131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, env, n_episodes=10, verbose=True):\n",
    "    \"\"\"Test the trained agent's performance\"\"\"\n",
    "    test_rewards = []\n",
    "    test_lengths = []\n",
    "    success_count = 0\n",
    "    \n",
    "    # Temporarily disable exploration for testing\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0  # Pure exploitation\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Testing trained agent (no exploration):\")\n",
    "        print()\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        path = [state]\n",
    "        \n",
    "        for step in range(50):  # Max 50 steps\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            path.append(next_state)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                if next_state == env.goal_pos:\n",
    "                    success_count += 1\n",
    "                break\n",
    "        \n",
    "        test_rewards.append(total_reward)\n",
    "        test_lengths.append(steps)\n",
    "        \n",
    "        if verbose:\n",
    "            outcome = \"SUCCESS\" if state == env.goal_pos else \"TRAP\" if state in env.traps else \"TIMEOUT\"\n",
    "            print(f\"Episode {episode + 1:2d}: {outcome:7s} | Reward: {total_reward:6.1f} | Steps: {steps:2d} | Path: {' ‚Üí '.join([str(p) for p in path])}\")\n",
    "    \n",
    "    # Restore original epsilon\n",
    "    agent.epsilon = original_epsilon\n",
    "    \n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"Test Results:\")\n",
    "        print(f\"- Success rate: {success_count}/{n_episodes} ({success_count/n_episodes:.2%})\")\n",
    "        print(f\"- Average reward: {np.mean(test_rewards):.2f} ¬± {np.std(test_rewards):.2f}\")\n",
    "        print(f\"- Average steps: {np.mean(test_lengths):.1f} ¬± {np.std(test_lengths):.1f}\")\n",
    "        if success_count > 0:\n",
    "            successful_lengths = [test_lengths[i] for i in range(n_episodes) \n",
    "                                if test_rewards[i] > 0]\n",
    "            print(f\"- Average steps to success: {np.mean(successful_lengths):.1f}\")\n",
    "    \n",
    "    return test_rewards, test_lengths, success_count\n",
    "\n",
    "# Test the agent\n",
    "test_rewards, test_lengths, success_count = test_agent(agent, env, n_episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fb32eb",
   "metadata": {},
   "source": [
    "## Interactive Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_episode(agent, env, show_q_values=False):\n",
    "    \"\"\"Demonstrate one episode step by step\"\"\"\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    \n",
    "    # Disable exploration for demonstration\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0\n",
    "    \n",
    "    print(\"Step-by-step demonstration:\")\n",
    "    print(f\"Starting at position {state}\")\n",
    "    print()\n",
    "    \n",
    "    # Show initial state\n",
    "    env.visualize(agent.q_table if show_q_values else None, show_q_values)\n",
    "    \n",
    "    while step < 20:  # Limit to 20 steps for visualization\n",
    "        # Show Q-values for current state\n",
    "        row, col = state\n",
    "        q_values = agent.q_table[row, col]\n",
    "        action = agent.choose_action(state)\n",
    "        \n",
    "        print(f\"Step {step + 1}:\")\n",
    "        print(f\"  Current state: {state}\")\n",
    "        print(f\"  Q-values: [Up: {q_values[0]:.2f}, Down: {q_values[1]:.2f}, \"\n",
    "              f\"Left: {q_values[2]:.2f}, Right: {q_values[3]:.2f}]\")\n",
    "        print(f\"  Chosen action: {env.actions[action]}\")\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        \n",
    "        print(f\"  Reward: {reward}, New state: {next_state}\")\n",
    "        print(f\"  Total reward so far: {total_reward}\")\n",
    "        \n",
    "        if done:\n",
    "            if next_state == env.goal_pos:\n",
    "                print(\"  üéâ SUCCESS! Reached the goal!\")\n",
    "            else:\n",
    "                print(\"  üí• FAILURE! Hit a trap!\")\n",
    "            break\n",
    "        \n",
    "        print()\n",
    "        state = next_state\n",
    "        \n",
    "        # Show updated state\n",
    "        env.visualize(agent.q_table if show_q_values else None, show_q_values)\n",
    "    \n",
    "    # Restore original epsilon\n",
    "    agent.epsilon = original_epsilon\n",
    "    \n",
    "    print(f\"\\nEpisode completed in {step} steps with total reward: {total_reward}\")\n",
    "    return total_reward, step\n",
    "\n",
    "# Run demonstration\n",
    "print(\"=\" * 60)\n",
    "print(\"DEMONSTRATION: Watching the trained agent navigate\")\n",
    "print(\"=\" * 60)\n",
    "demonstrate_episode(agent, env, show_q_values=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfee052",
   "metadata": {},
   "source": [
    "## Summary and Key Insights\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Q-Learning Basics**: The agent learned to estimate the \"quality\" (Q-value) of taking each action in each state.\n",
    "\n",
    "2. **Exploration vs Exploitation**: The epsilon-greedy strategy balanced exploring new actions with exploiting known good actions.\n",
    "\n",
    "3. **Reward Shaping**: The combination of goal rewards (+10), trap penalties (-10), and step costs (-1) guided the agent toward efficient paths.\n",
    "\n",
    "4. **Convergence**: Over time, the agent's policy stabilized, and it learned to consistently reach the goal while avoiding traps.\n",
    "\n",
    "### Key Q-Learning Components\n",
    "\n",
    "- **Q-Table**: Stores the learned values for each state-action pair\n",
    "- **Update Rule**: $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$\n",
    "- **Exploration Strategy**: Epsilon-greedy policy for balancing exploration and exploitation\n",
    "- **Learning Rate (Œ±)**: Controls how quickly the agent updates its knowledge\n",
    "- **Discount Factor (Œ≥)**: Determines the importance of future rewards\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "This grid world problem is simplified, but the same Q-learning principles apply to:\n",
    "\n",
    "- **Robot Navigation**: Path planning in warehouses, autonomous vehicles\n",
    "- **Game Playing**: Board games, video games\n",
    "- **Resource Management**: Inventory control, energy management\n",
    "- **Process Control**: Industrial automation, chemical processes\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To extend this tutorial, you could:\n",
    "\n",
    "1. **Modify the Environment**: Add moving obstacles, multiple goals, or different reward structures\n",
    "2. **Try Different Algorithms**: SARSA, Double Q-Learning, or Deep Q-Networks (DQN)\n",
    "3. **Experiment with Parameters**: Different learning rates, discount factors, or exploration strategies\n",
    "4. **Add Complexity**: Larger grids, continuous states, or partial observability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
