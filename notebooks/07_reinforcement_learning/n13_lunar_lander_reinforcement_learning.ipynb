{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d524427",
   "metadata": {},
   "source": [
    "# Lunar Lander with Reinforcement Learning\n",
    "\n",
    "This notebook demonstrates reinforcement learning using the classic Lunar Lander environment from OpenAI Gym. We'll implement and compare different RL algorithms to train an agent to successfully land a spacecraft.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the Lunar Lander environment\n",
    "- Implement Deep Q-Network (DQN) algorithm\n",
    "- Train an agent to land successfully\n",
    "- Visualize training progress and performance\n",
    "- Compare different approaches and hyperparameters\n",
    "\n",
    "## Environment Description\n",
    "The Lunar Lander environment simulates landing a spacecraft on the moon. The agent must control the lander's engines to achieve a safe landing while minimizing fuel consumption.\n",
    "\n",
    "**State Space**: 8 continuous values representing position, velocity, angle, and contact sensors\n",
    "**Action Space**: 4 discrete actions (do nothing, fire left engine, fire main engine, fire right engine)\n",
    "**Reward**: +100 for landing safely, -100 for crashing, fuel penalty for engine use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016f9052",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e77427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "        print(f\"‚úì {package} installed successfully\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚úó Failed to install {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "def check_package(package_name, import_name=None):\n",
    "    \"\"\"Check if a package is installed\"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "    try:\n",
    "        importlib.import_module(import_name)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "# Define packages to install\n",
    "packages = [\n",
    "    ('gymnasium', 'gymnasium'),\n",
    "    ('Box2D', 'Box2D'),\n",
    "    ('torch', 'torch'), \n",
    "    ('numpy', 'numpy'),\n",
    "    ('matplotlib', 'matplotlib'),\n",
    "    ('seaborn', 'seaborn'),\n",
    "    ('tqdm', 'tqdm'),\n",
    "    ('opencv-python', 'cv2')\n",
    "]\n",
    "\n",
    "print(\"Checking and installing required packages...\\n\")\n",
    "\n",
    "for package, import_name in packages:\n",
    "    if check_package(import_name):\n",
    "        print(f\"‚úì {package} is already installed\")\n",
    "    else:\n",
    "        print(f\"Installing {package}...\")\n",
    "        if package == 'opencv-python':\n",
    "            install_package('opencv-python-headless')  # Use headless version for notebooks\n",
    "        else:\n",
    "            install_package(package)\n",
    "\n",
    "print(f\"\\nüéâ Package installation complete!\")\n",
    "print(f\"If you still encounter issues with Box2D, try running:\")\n",
    "print(f\"pip install 'gymnasium[box2d]' in your terminal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a654047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    print(\"‚úì Gymnasium imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing gymnasium: {e}\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import random\n",
    "    from collections import deque, namedtuple\n",
    "    from tqdm import tqdm\n",
    "    from IPython.display import HTML, display\n",
    "    import base64\n",
    "    import io\n",
    "    print(\"‚úì Basic packages imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing basic packages: {e}\")\n",
    "    raise\n",
    "\n",
    "# Try importing PyTorch (optional for Q-learning version)\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torch.nn.functional as F\n",
    "    print(\"‚úì PyTorch imported successfully\")\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è PyTorch not available - some advanced features will be disabled\")\n",
    "    TORCH_AVAILABLE = False\n",
    "\n",
    "# Try importing OpenCV (optional)\n",
    "try:\n",
    "    import cv2\n",
    "    print(\"‚úì OpenCV imported successfully\")\n",
    "    CV2_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è OpenCV not available - some visualization features may be limited\")\n",
    "    CV2_AVAILABLE = False\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "if TORCH_AVAILABLE:\n",
    "    torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')  # Use default instead of seaborn-v0_8 to avoid version issues\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(f\"\\nüéâ Libraries imported successfully!\")\n",
    "if TORCH_AVAILABLE:\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Gymnasium version: {gym.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "\n",
    "\n",
    "print(f\"\\nüí° Ready to proceed with reinforcement learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa13a2b5",
   "metadata": {},
   "source": [
    "## 2. Exploring the Lunar Lander Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8733af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and explore the environment\n",
    "try:\n",
    "    env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "    print(\"‚úì LunarLander environment created successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating environment: {e}\")\n",
    "    print(\"\\nTrying to install missing dependencies...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    try:\n",
    "        # Install Box2D dependencies\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'Box2D'])\n",
    "        print(\"‚úì Box2D installed successfully!\")\n",
    "        \n",
    "        # Try creating environment again\n",
    "        env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "        print(\"‚úì LunarLander environment created successfully after installing dependencies!\")\n",
    "    except Exception as install_error:\n",
    "        print(f\"‚ùå Failed to install dependencies: {install_error}\")\n",
    "        print(\"Please run: pip install 'gymnasium[box2d]' in your terminal\")\n",
    "        raise\n",
    "\n",
    "print(\"=== Lunar Lander Environment Information ===\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Number of actions: {env.action_space.n}\")\n",
    "\n",
    "# Action meanings\n",
    "action_meanings = {\n",
    "    0: \"Do nothing\",\n",
    "    1: \"Fire left orientation engine\",\n",
    "    2: \"Fire main engine\",\n",
    "    3: \"Fire right orientation engine\"\n",
    "}\n",
    "\n",
    "print(\"\\n=== Action Space ===\")\n",
    "for action, meaning in action_meanings.items():\n",
    "    print(f\"Action {action}: {meaning}\")\n",
    "\n",
    "# Observation space description\n",
    "obs_descriptions = [\n",
    "    \"Horizontal coordinate\",\n",
    "    \"Vertical coordinate\", \n",
    "    \"Horizontal velocity\",\n",
    "    \"Vertical velocity\",\n",
    "    \"Angle\",\n",
    "    \"Angular velocity\",\n",
    "    \"Left leg contact (boolean)\",\n",
    "    \"Right leg contact (boolean)\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Observation Space ===\")\n",
    "for i, desc in enumerate(obs_descriptions):\n",
    "    print(f\"Index {i}: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843ba12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a random episode\n",
    "def visualize_random_episode(env, max_steps=500):\n",
    "    \"\"\"Visualize a random episode to understand the environment\"\"\"\n",
    "    \n",
    "    observation, _ = env.reset()\n",
    "    frames = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    print(\"Running random episode...\")\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Take random action\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Capture frame\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            print(f\"Episode ended after {step + 1} steps\")\n",
    "            print(f\"Total reward: {total_reward:.2f}\")\n",
    "            print(f\"Final observation: {observation}\")\n",
    "            break\n",
    "    \n",
    "    # Show first and last frames\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    \n",
    "    ax1.imshow(frames[0])\n",
    "    ax1.set_title(\"Initial State\")\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2.imshow(frames[-1])\n",
    "    ax2.set_title(\"Final State\")\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return frames\n",
    "\n",
    "# Run visualization\n",
    "frames = visualize_random_episode(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7169e9",
   "metadata": {},
   "source": [
    "## 3. Deep Q-Network (DQN) Implementation\n",
    "\n",
    "Now we'll implement a Deep Q-Network to learn how to land the lunar lander. DQN uses a neural network to approximate the Q-function, which estimates the expected future reward for each action in a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c271e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience replay buffer\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for DQN\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add experience to buffer\"\"\"\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample random batch of experiences\"\"\"\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "print(\"Experience replay buffer implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a6d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-Network architecture\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"Deep Q-Network for Lunar Lander\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Test the network\n",
    "state_size = env.observation_space.shape[0]  # 8 for Lunar Lander\n",
    "action_size = env.action_space.n  # 4 for Lunar Lander\n",
    "\n",
    "test_dqn = DQN(state_size, action_size)\n",
    "print(f\"DQN Architecture:\")\n",
    "print(test_dqn)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in test_dqn.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df65182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Agent implementation\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent for Lunar Lander\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, lr=0.0005, gamma=0.99, \n",
    "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n",
    "                 buffer_size=100000, batch_size=64, update_freq=4, target_update_freq=1000):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.update_freq = update_freq\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        # Neural networks\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.q_network = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_network = DQN(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Experience replay\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Counters\n",
    "        self.step_count = 0\n",
    "        \n",
    "        print(f\"DQN Agent initialized with device: {self.device}\")\n",
    "    \n",
    "    def act(self, state, training=True):\n",
    "        \"\"\"Choose action using epsilon-greedy policy\"\"\"\n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        q_values = self.q_network(state_tensor)\n",
    "        return q_values.argmax().item()\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"Train the network on a batch of experiences\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch\n",
    "        experiences = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor([e.state for e in experiences]).to(self.device)\n",
    "        actions = torch.LongTensor([e.action for e in experiences]).to(self.device)\n",
    "        rewards = torch.FloatTensor([e.reward for e in experiences]).to(self.device)\n",
    "        next_states = torch.FloatTensor([e.next_state for e in experiences]).to(self.device)\n",
    "        dones = torch.BoolTensor([e.done for e in experiences]).to(self.device)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Target Q values\n",
    "        next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from main network to target network\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay epsilon for exploration\"\"\"\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "print(\"DQN Agent implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2dd03c",
   "metadata": {},
   "source": [
    "## 4. Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96777e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_dqn_agent(env, agent, n_episodes=2000, max_steps=1000, \n",
    "                   target_score=200, print_every=100, save_model=True):\n",
    "    \"\"\"Train DQN agent on Lunar Lander\"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    losses = []\n",
    "    epsilons = []\n",
    "    recent_scores = deque(maxlen=100)\n",
    "    \n",
    "    print(f\"Starting training for {n_episodes} episodes...\")\n",
    "    print(f\"Target average score: {target_score}\")\n",
    "    \n",
    "    for episode in tqdm(range(1, n_episodes + 1), desc=\"Training\"):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        episode_losses = []\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Choose action\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store experience\n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Learn\n",
    "            if agent.step_count % agent.update_freq == 0:\n",
    "                loss = agent.learn()\n",
    "                if loss is not None:\n",
    "                    episode_losses.append(loss)\n",
    "            \n",
    "            # Update target network\n",
    "            if agent.step_count % agent.target_update_freq == 0:\n",
    "                agent.update_target_network()\n",
    "            \n",
    "            agent.step_count += 1\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Record metrics\n",
    "        scores.append(total_reward)\n",
    "        recent_scores.append(total_reward)\n",
    "        epsilons.append(agent.epsilon)\n",
    "        \n",
    "        if episode_losses:\n",
    "            losses.append(np.mean(episode_losses))\n",
    "        else:\n",
    "            losses.append(0)\n",
    "        \n",
    "        # Print progress\n",
    "        if episode % print_every == 0:\n",
    "            avg_score = np.mean(recent_scores)\n",
    "            print(f\"\\nEpisode {episode:4d} | Avg Score: {avg_score:7.2f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f} | Loss: {losses[-1]:.4f}\")\n",
    "            \n",
    "            # Check if solved\n",
    "            if avg_score >= target_score:\n",
    "                print(f\"\\nüéâ Environment solved in {episode} episodes!\")\n",
    "                print(f\"Average score over last 100 episodes: {avg_score:.2f}\")\n",
    "                break\n",
    "    \n",
    "    # Save model\n",
    "    if save_model:\n",
    "        torch.save(agent.q_network.state_dict(), 'lunar_lander_dqn.pth')\n",
    "        print(\"\\nModel saved as 'lunar_lander_dqn.pth'\")\n",
    "    \n",
    "    return scores, losses, epsilons\n",
    "\n",
    "print(\"Training function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23abc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent and start training\n",
    "agent = DQNAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    lr=0.0005,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    buffer_size=100000,\n",
    "    batch_size=64,\n",
    "    update_freq=4,\n",
    "    target_update_freq=1000\n",
    ")\n",
    "\n",
    "# Start training (reduce episodes for demo)\n",
    "scores, losses, epsilons = train_dqn_agent(\n",
    "    env, agent, \n",
    "    n_episodes=10000,  # Reduced for demo\n",
    "    max_steps=100000,\n",
    "    target_score=200,\n",
    "    print_every=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbadfb7c",
   "metadata": {},
   "source": [
    "## 5. Training Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c0cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "def plot_training_results(scores, losses, epsilons):\n",
    "    \"\"\"Plot comprehensive training results\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(7, 5))\n",
    "    \n",
    "    # Plot 1: Episode scores\n",
    "    ax1.plot(scores, alpha=0.6, color='blue', label='Episode Score')\n",
    "    \n",
    "    # Calculate moving average\n",
    "    window = min(100, len(scores) // 10)\n",
    "    if len(scores) >= window:\n",
    "        moving_avg = []\n",
    "        for i in range(len(scores)):\n",
    "            start_idx = max(0, i - window + 1)\n",
    "            moving_avg.append(np.mean(scores[start_idx:i+1]))\n",
    "        ax1.plot(moving_avg, color='red', linewidth=2, label=f'Moving Avg ({window})')\n",
    "    \n",
    "    ax1.axhline(y=200, color='green', linestyle='--', label='Target Score (200)')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_title('Training Scores Over Time')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Training loss\n",
    "    if len(losses) > 0:\n",
    "        ax2.plot(losses, color='orange', alpha=0.7)\n",
    "        ax2.set_xlabel('Episode')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.set_title('Training Loss')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Epsilon decay\n",
    "    ax3.plot(epsilons, color='purple')\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Epsilon')\n",
    "    ax3.set_title('Exploration Rate (Epsilon) Decay')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Score distribution\n",
    "    ax4.hist(scores, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax4.axvline(np.mean(scores), color='red', linestyle='--', label=f'Mean: {np.mean(scores):.1f}')\n",
    "    ax4.axvline(200, color='green', linestyle='--', label='Target: 200')\n",
    "    ax4.set_xlabel('Score')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Score Distribution')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n=== Training Summary ===\")\n",
    "    print(f\"Total episodes: {len(scores)}\")\n",
    "    print(f\"Average score: {np.mean(scores):.2f}\")\n",
    "    print(f\"Best score: {np.max(scores):.2f}\")\n",
    "    print(f\"Worst score: {np.min(scores):.2f}\")\n",
    "    print(f\"Standard deviation: {np.std(scores):.2f}\")\n",
    "    \n",
    "    # Success rate (scores >= 200)\n",
    "    success_rate = np.mean(np.array(scores) >= 200) * 100\n",
    "    print(f\"Success rate (score >= 200): {success_rate:.1f}%\")\n",
    "    \n",
    "    # Last 100 episodes average\n",
    "    if len(scores) >= 100:\n",
    "        last_100_avg = np.mean(scores[-100:])\n",
    "        print(f\"Last 100 episodes average: {last_100_avg:.2f}\")\n",
    "\n",
    "# Plot results\n",
    "plot_training_results(scores, losses, epsilons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3bd845",
   "metadata": {},
   "source": [
    "## 6. Testing the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0e890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent\n",
    "def test_agent(env, agent, n_episodes=10, render=True):\n",
    "    \"\"\"Test the trained agent\"\"\"\n",
    "    \n",
    "    test_scores = []\n",
    "    test_episodes = []\n",
    "    \n",
    "    print(f\"Testing agent for {n_episodes} episodes...\")\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        frames = []\n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "            # Choose action (no exploration)\n",
    "            action = agent.act(state, training=False)\n",
    "            \n",
    "            # Take action\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if render and episode < 3:  # Only render first 3 episodes\n",
    "                frames.append(env.render())\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        test_scores.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Score = {total_reward:.2f}, Steps = {steps}\")\n",
    "        \n",
    "        # Store episode data for visualization\n",
    "        if episode < 3:\n",
    "            test_episodes.append({\n",
    "                'episode': episode + 1,\n",
    "                'score': total_reward,\n",
    "                'steps': steps,\n",
    "                'frames': frames\n",
    "            })\n",
    "    \n",
    "    print(f\"\\n=== Test Results ===\")\n",
    "    print(f\"Average score: {np.mean(test_scores):.2f}\")\n",
    "    print(f\"Best score: {np.max(test_scores):.2f}\")\n",
    "    print(f\"Worst score: {np.min(test_scores):.2f}\")\n",
    "    print(f\"Success rate: {np.mean(np.array(test_scores) >= 200) * 100:.1f}%\")\n",
    "    \n",
    "    return test_scores, test_episodes\n",
    "\n",
    "# Test the agent\n",
    "test_scores, test_episodes = test_agent(env, agent, n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test episodes\n",
    "def visualize_test_episodes(test_episodes):\n",
    "    \"\"\"Visualize the first few test episodes\"\"\"\n",
    "    \n",
    "    n_episodes = len(test_episodes)\n",
    "    fig, axes = plt.subplots(2, n_episodes, figsize=(5 * n_episodes, 8))\n",
    "    \n",
    "    if n_episodes == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i, episode_data in enumerate(test_episodes):\n",
    "        frames = episode_data['frames']\n",
    "        \n",
    "        if len(frames) > 0:\n",
    "            # Show initial state\n",
    "            axes[0, i].imshow(frames[0])\n",
    "            axes[0, i].set_title(f\"Episode {episode_data['episode']} - Start\")\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Show final state\n",
    "            axes[1, i].imshow(frames[-1])\n",
    "            axes[1, i].set_title(f\"Score: {episode_data['score']:.1f} | Steps: {episode_data['steps']}\")\n",
    "            axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize test episodes\n",
    "visualize_test_episodes(test_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552c5e7f",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Analysis and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c09560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different hyperparameters\n",
    "def compare_hyperparameters(env, configs, n_episodes=500):\n",
    "    \"\"\"Compare different hyperparameter configurations\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, config in configs.items():\n",
    "        print(f\"\\n=== Testing configuration: {name} ===\")\n",
    "        print(f\"Config: {config}\")\n",
    "        \n",
    "        # Create agent with config\n",
    "        agent = DQNAgent(\n",
    "            state_size=state_size,\n",
    "            action_size=action_size,\n",
    "            **config\n",
    "        )\n",
    "        \n",
    "        # Train agent\n",
    "        scores, losses, epsilons = train_dqn_agent(\n",
    "            env, agent,\n",
    "            n_episodes=n_episodes,\n",
    "            print_every=100,\n",
    "            save_model=False\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'scores': scores,\n",
    "            'losses': losses,\n",
    "            'epsilons': epsilons,\n",
    "            'avg_score': np.mean(scores[-100:]),  # Last 100 episodes\n",
    "            'success_rate': np.mean(np.array(scores[-100:]) >= 200) * 100\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define configurations to compare\n",
    "configs = {\n",
    "    'Default': {\n",
    "        'lr': 0.0005,\n",
    "        'gamma': 0.99,\n",
    "        'epsilon_decay': 0.995\n",
    "    },\n",
    "    'Higher LR': {\n",
    "        'lr': 0.001,\n",
    "        'gamma': 0.99,\n",
    "        'epsilon_decay': 0.995\n",
    "    },\n",
    "    'Lower Gamma': {\n",
    "        'lr': 0.0005,\n",
    "        'gamma': 0.95,\n",
    "        'epsilon_decay': 0.995\n",
    "    },\n",
    "    'Faster Decay': {\n",
    "        'lr': 0.0005,\n",
    "        'gamma': 0.99,\n",
    "        'epsilon_decay': 0.99\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter comparison configurations ready!\")\n",
    "print(\"Note: Running full comparison would take significant time.\")\n",
    "print(\"Uncomment the line below to run comparison:\")\n",
    "# results = compare_hyperparameters(env, configs, n_episodes=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfba61f",
   "metadata": {},
   "source": [
    "## 8. Advanced Techniques and Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double DQN implementation\n",
    "class DoubleDQNAgent(DQNAgent):\n",
    "    \"\"\"Double DQN Agent - reduces overestimation bias\"\"\"\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"Train using Double DQN update rule\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch\n",
    "        experiences = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor([e.state for e in experiences]).to(self.device)\n",
    "        actions = torch.LongTensor([e.action for e in experiences]).to(self.device)\n",
    "        rewards = torch.FloatTensor([e.reward for e in experiences]).to(self.device)\n",
    "        next_states = torch.FloatTensor([e.next_state for e in experiences]).to(self.device)\n",
    "        dones = torch.BoolTensor([e.done for e in experiences]).to(self.device)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # Double DQN: Use main network to select actions, target network to evaluate\n",
    "        next_actions = self.q_network(next_states).argmax(1).unsqueeze(1)\n",
    "        next_q_values = self.target_network(next_states).gather(1, next_actions).squeeze(1).detach()\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "print(\"Double DQN agent implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179454bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prioritized Experience Replay Buffer\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"Prioritized Experience Replay Buffer\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.max_priority = 1.0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add experience with maximum priority\"\"\"\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.position] = experience\n",
    "        \n",
    "        self.priorities[self.position] = self.max_priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        \"\"\"Sample batch with priorities\"\"\"\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.position]\n",
    "        \n",
    "        # Calculate sampling probabilities\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        # Sample indices\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        \n",
    "        # Calculate importance sampling weights\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        experiences = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        return experiences, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"Update priorities for sampled experiences\"\"\"\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "print(\"Prioritized Experience Replay implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9033fdca",
   "metadata": {},
   "source": [
    "## 9. Performance Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c110fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze agent performance\n",
    "def analyze_agent_behavior(env, agent, n_episodes=5):\n",
    "    \"\"\"Analyze agent's decision making and behavior patterns\"\"\"\n",
    "    \n",
    "    print(\"=== Agent Behavior Analysis ===\")\n",
    "    \n",
    "    action_counts = {i: 0 for i in range(4)}\n",
    "    state_action_data = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_data = []\n",
    "        \n",
    "        while True:\n",
    "            action = agent.act(state, training=False)\n",
    "            action_counts[action] += 1\n",
    "            \n",
    "            # Store state-action data\n",
    "            episode_data.append({\n",
    "                'state': state.copy(),\n",
    "                'action': action,\n",
    "                'position': (state[0], state[1]),\n",
    "                'velocity': (state[2], state[3]),\n",
    "                'angle': state[4],\n",
    "                'legs_contact': (state[6], state[7])\n",
    "            })\n",
    "            \n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        state_action_data.extend(episode_data)\n",
    "    \n",
    "    # Plot action distribution\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(7, 5))\n",
    "    \n",
    "    # Action frequency\n",
    "    actions = list(action_counts.keys())\n",
    "    counts = list(action_counts.values())\n",
    "    colors = ['gray', 'blue', 'red', 'orange']\n",
    "    \n",
    "    bars = ax1.bar(actions, counts, color=colors, alpha=0.7)\n",
    "    ax1.set_xlabel('Action')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.set_title('Action Usage Distribution')\n",
    "    ax1.set_xticks(actions)\n",
    "    ax1.set_xticklabels(['Do Nothing', 'Left Engine', 'Main Engine', 'Right Engine'])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                f'{count}', ha='center', va='bottom')\n",
    "    \n",
    "    # State analysis\n",
    "    positions = [data['position'] for data in state_action_data]\n",
    "    actions_taken = [data['action'] for data in state_action_data]\n",
    "    \n",
    "    # Position vs action\n",
    "    x_positions = [pos[0] for pos in positions]\n",
    "    y_positions = [pos[1] for pos in positions]\n",
    "    \n",
    "    scatter = ax2.scatter(x_positions, y_positions, c=actions_taken, \n",
    "                         cmap='viridis', alpha=0.6, s=20)\n",
    "    ax2.set_xlabel('Horizontal Position')\n",
    "    ax2.set_ylabel('Vertical Position')\n",
    "    ax2.set_title('Actions vs Position')\n",
    "    plt.colorbar(scatter, ax=ax2, label='Action')\n",
    "    \n",
    "    # Velocity analysis\n",
    "    velocities = [data['velocity'] for data in state_action_data]\n",
    "    x_velocities = [vel[0] for vel in velocities]\n",
    "    y_velocities = [vel[1] for vel in velocities]\n",
    "    \n",
    "    scatter2 = ax3.scatter(x_velocities, y_velocities, c=actions_taken,\n",
    "                          cmap='viridis', alpha=0.6, s=20)\n",
    "    ax3.set_xlabel('Horizontal Velocity')\n",
    "    ax3.set_ylabel('Vertical Velocity')\n",
    "    ax3.set_title('Actions vs Velocity')\n",
    "    plt.colorbar(scatter2, ax=ax3, label='Action')\n",
    "    \n",
    "    # Angle analysis\n",
    "    angles = [data['angle'] for data in state_action_data]\n",
    "    ax4.hist(angles, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax4.set_xlabel('Angle (radians)')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Angle Distribution')\n",
    "    ax4.axvline(0, color='red', linestyle='--', label='Upright')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print insights\n",
    "    total_actions = sum(action_counts.values())\n",
    "    print(f\"\\n=== Insights ===\")\n",
    "    print(f\"Total actions analyzed: {total_actions}\")\n",
    "    print(f\"Most used action: {max(action_counts, key=action_counts.get)} \"\n",
    "          f\"({action_counts[max(action_counts, key=action_counts.get)]/total_actions*100:.1f}%)\")\n",
    "    print(f\"Average angle: {np.mean(angles):.3f} radians\")\n",
    "    print(f\"Angle standard deviation: {np.std(angles):.3f} radians\")\n",
    "\n",
    "# Analyze the trained agent\n",
    "analyze_agent_behavior(env, agent, n_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13b7d9",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways and Conclusion\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "1. **Environment Understanding**: The Lunar Lander environment provides a challenging continuous control problem with sparse rewards\n",
    "\n",
    "2. **DQN Implementation**: We successfully implemented a Deep Q-Network with:\n",
    "   - Experience replay for stable learning\n",
    "   - Target network for reduced correlation\n",
    "   - Epsilon-greedy exploration strategy\n",
    "\n",
    "3. **Training Dynamics**: \n",
    "   - Initial random exploration with high epsilon\n",
    "   - Gradual improvement as the agent learns\n",
    "   - Importance of hyperparameter tuning\n",
    "\n",
    "4. **Performance Analysis**: Successful agents learn to:\n",
    "   - Control orientation with side engines\n",
    "   - Use main engine for controlled descent\n",
    "   - Balance fuel efficiency with safety\n",
    "\n",
    "### Advanced Techniques Covered\n",
    "- Double DQN for reduced overestimation\n",
    "- Prioritized Experience Replay for better sample efficiency\n",
    "- Comprehensive performance analysis\n",
    "\n",
    "### Next Steps\n",
    "- Try other RL algorithms (PPO, A3C, SAC)\n",
    "- Experiment with different network architectures\n",
    "- Apply techniques to other environments\n",
    "- Implement multi-agent scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d42503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model and create summary\n",
    "def create_training_summary(scores, losses, epsilons):\n",
    "    \"\"\"Create comprehensive training summary\"\"\"\n",
    "    \n",
    "    summary = {\n",
    "        'total_episodes': len(scores),\n",
    "        'final_average_score': np.mean(scores[-100:]) if len(scores) >= 100 else np.mean(scores),\n",
    "        'best_score': np.max(scores),\n",
    "        'worst_score': np.min(scores),\n",
    "        'success_rate': np.mean(np.array(scores) >= 200) * 100,\n",
    "        'episodes_to_solve': None,\n",
    "        'final_epsilon': epsilons[-1] if epsilons else None\n",
    "    }\n",
    "    \n",
    "    # Find when environment was \"solved\" (avg score >= 200 for 100 episodes)\n",
    "    if len(scores) >= 100:\n",
    "        for i in range(100, len(scores) + 1):\n",
    "            if np.mean(scores[i-100:i]) >= 200:\n",
    "                summary['episodes_to_solve'] = i\n",
    "                break\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"           LUNAR LANDER TRAINING SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"üìä Total Episodes Trained: {summary['total_episodes']}\")\n",
    "    print(f\"üéØ Final Average Score: {summary['final_average_score']:.2f}\")\n",
    "    print(f\"üèÜ Best Score Achieved: {summary['best_score']:.2f}\")\n",
    "    print(f\"üí• Worst Score: {summary['worst_score']:.2f}\")\n",
    "    print(f\"‚úÖ Success Rate (‚â•200): {summary['success_rate']:.1f}%\")\n",
    "    \n",
    "    if summary['episodes_to_solve']:\n",
    "        print(f\"üéâ Environment Solved in: {summary['episodes_to_solve']} episodes\")\n",
    "    else:\n",
    "        print(f\"‚ùå Environment not yet solved (need avg ‚â•200 for 100 episodes)\")\n",
    "    \n",
    "    if summary['final_epsilon']:\n",
    "        print(f\"üîç Final Exploration Rate: {summary['final_epsilon']:.4f}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Create final summary\n",
    "final_summary = create_training_summary(scores, losses, epsilons)\n",
    "\n",
    "# Clean up\n",
    "env.close()\n",
    "print(\"\\nüöÄ Lunar Lander RL training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
