{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e46b6d2",
   "metadata": {},
   "source": [
    "# ðŸŒº Classification Tutorial: Machine Learning with the Iris Dataset\n",
    "\n",
    "## **Master Classification Algorithms with Interactive PyTorch Implementation**\n",
    "\n",
    "### **Main Goal:**\n",
    "Learn how to classify iris flowers into three species (Setosa, Versicolor, Virginica) based on their physical measurements, while understanding every step of the classification pipeline through interactive visualizations.\n",
    "\n",
    "### **Key Learning Objectives:**\n",
    "- **Classification Fundamentals**: Understand the difference between regression and classification\n",
    "- **Data Exploration**: Analyze features, distributions, and class separability\n",
    "- **Model Implementation**: Build neural network classifiers with PyTorch\n",
    "- **Performance Evaluation**: Use accuracy, precision, recall, F1-score, and confusion matrices\n",
    "- **Decision Boundaries**: Visualize how models separate different classes\n",
    "\n",
    "### **Industrial Relevance:**\n",
    "Apply to **quality control**, **fault detection**, **process monitoring**, and **automated inspection** in industrial applications.\n",
    "\n",
    "### **Interactive Features:**\n",
    "ðŸŽ® Real-time model training | ðŸ“Š Decision boundary visualization | âš¡ Confusion matrix analysis | ðŸ”¬ Feature importance exploration\n",
    "\n",
    "**Dataset**: Iris flower dataset with 4 features (sepal/petal length/width) and 3 classes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa38619",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll use PyTorch, scikit-learn, NumPy, Matplotlib, and Seaborn for this classification tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4783d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "%matplotlib widget\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63be0c24",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Iris Dataset\n",
    "\n",
    "Let's start by loading the famous iris dataset and understanding its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['species'] = [class_names[i] for i in y]\n",
    "df['target'] = y\n",
    "\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"Number of classes:\", len(class_names))\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nDataset statistics:\")\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['species'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8522505e",
   "metadata": {},
   "source": [
    "## 3. Data Visualization and Exploration\n",
    "\n",
    "Let's visualize the data to understand the relationships between features and classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f091337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of the dataset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7, 6))\n",
    "\n",
    "# Pairwise scatter plots for the first two features\n",
    "for i, (feat1, feat2) in enumerate([(0, 1), (0, 2), (0, 3), (1, 2)]):\n",
    "    ax = axes[i//2, i%2]\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        mask = y == class_idx\n",
    "        ax.scatter(X[mask, feat1], X[mask, feat2], \n",
    "                  label=class_name, alpha=0.7, s=50)\n",
    "    \n",
    "    ax.set_xlabel(feature_names[feat1])\n",
    "    ax.set_ylabel(feature_names[feat2])\n",
    "    ax.set_title(f'{feature_names[feat1]} vs {feature_names[feat2]}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# plt.savefig('iris_pairwise_scatter.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of each feature by class\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        mask = y == class_idx\n",
    "        axes[i].hist(X[mask, i], alpha=0.6, label=class_name, bins=15)\n",
    "    \n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_title(f'Distribution of {feature}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbed8b0",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Prepare the data for training by splitting and scaling the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d83fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale the features (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.LongTensor(y_train).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_test_tensor = torch.LongTensor(y_test).to(device)\n",
    "\n",
    "print(f\"Training set size: {X_train_tensor.shape}\")\n",
    "print(f\"Test set size: {X_test_tensor.shape}\")\n",
    "print(f\"Number of features: {X_train_tensor.shape[1]}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d3501d",
   "metadata": {},
   "source": [
    "## 5. Define Neural Network Classifier\n",
    "\n",
    "Let's create a simple neural network for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa598e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisClassifier(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_size=10, num_classes=3):\n",
    "        super(IrisClassifier, self).__init__()\n",
    "        if hidden_size >0:\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "            self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(input_size, num_classes)\n",
    "            self.fc2 = None\n",
    "            self.fc3 = None\n",
    "            self.dropout = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.fc2 is None:\n",
    "            x = self.fc1(x)\n",
    "            return x\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)  # No activation here - will use CrossEntropyLoss\n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "model = IrisClassifier(hidden_size=0).to(device)\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b01d1",
   "metadata": {},
   "source": [
    "## 6. Training Function\n",
    "\n",
    "Define a comprehensive training function that tracks multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, X_train, y_train, X_test, y_test, \n",
    "                    lr=0.01, epochs=200, print_every=50):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        train_outputs = model(X_train)\n",
    "        train_loss = criterion(train_outputs, y_train)\n",
    "        \n",
    "        # Backward pass\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate training accuracy\n",
    "        _, train_predicted = torch.max(train_outputs.data, 1)\n",
    "        train_accuracy = (train_predicted == y_train).float().mean()\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_outputs = model(X_test)\n",
    "            test_loss = criterion(test_outputs, y_test)\n",
    "            _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "            test_accuracy = (test_predicted == y_test).float().mean()\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss.item())\n",
    "        test_losses.append(test_loss.item())\n",
    "        train_accuracies.append(train_accuracy.item())\n",
    "        test_accuracies.append(test_accuracy.item())\n",
    "        \n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}]')\n",
    "            print(f'Train Loss: {train_loss.item():.4f}, Train Acc: {train_accuracy.item():.4f}')\n",
    "            print(f'Test Loss: {test_loss.item():.4f}, Test Acc: {test_accuracy.item():.4f}')\n",
    "            print('-' * 50)\n",
    "    \n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the classifier...\")\n",
    "train_losses, test_losses, train_accs, test_accs = train_classifier(\n",
    "    model, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcad2423",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Progress\n",
    "\n",
    "Let's plot the training and testing metrics to understand model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(train_losses, label='Training Loss', color='blue', linewidth=2)\n",
    "ax1.plot(test_losses, label='Test Loss', color='red', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Test Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(train_accs, label='Training Accuracy', color='blue', linewidth=2)\n",
    "ax2.plot(test_accs, label='Test Accuracy', color='red', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Training Accuracy: {train_accs[-1]:.4f}\")\n",
    "print(f\"Final Test Accuracy: {test_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a094431",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Metrics\n",
    "\n",
    "Let's evaluate our model using various classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c631888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_outputs = model(X_train_tensor)\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    \n",
    "    _, train_predicted = torch.max(train_outputs, 1)\n",
    "    _, test_predicted = torch.max(test_outputs, 1)\n",
    "\n",
    "# Convert to numpy for sklearn metrics\n",
    "y_train_np = y_train_tensor.cpu().numpy()\n",
    "y_test_np = y_test_tensor.cpu().numpy()\n",
    "train_pred_np = train_predicted.cpu().numpy()\n",
    "test_pred_np = test_predicted.cpu().numpy()\n",
    "\n",
    "# Calculate detailed metrics\n",
    "train_accuracy = accuracy_score(y_train_np, train_pred_np)\n",
    "test_accuracy = accuracy_score(y_test_np, test_pred_np)\n",
    "\n",
    "train_precision = precision_score(y_train_np, train_pred_np, average='weighted')\n",
    "test_precision = precision_score(y_test_np, test_pred_np, average='weighted')\n",
    "\n",
    "train_recall = recall_score(y_train_np, train_pred_np, average='weighted')\n",
    "test_recall = recall_score(y_test_np, test_pred_np, average='weighted')\n",
    "\n",
    "train_f1 = f1_score(y_train_np, train_pred_np, average='weighted')\n",
    "test_f1 = f1_score(y_test_np, test_pred_np, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(\"CLASSIFICATION METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Metric':<15} {'Training':<12} {'Test':<12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Accuracy':<15} {train_accuracy:<12.4f} {test_accuracy:<12.4f}\")\n",
    "print(f\"{'Precision':<15} {train_precision:<12.4f} {test_precision:<12.4f}\")\n",
    "print(f\"{'Recall':<15} {train_recall:<12.4f} {test_recall:<12.4f}\")\n",
    "print(f\"{'F1-Score':<15} {train_f1:<12.4f} {test_f1:<12.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6da416",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrix Visualization\n",
    "\n",
    "The confusion matrix shows how well our model distinguishes between different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c00707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices\n",
    "train_cm = confusion_matrix(y_train_np, train_pred_np)\n",
    "test_cm = confusion_matrix(y_test_np, test_pred_np)\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3))\n",
    "\n",
    "# Training confusion matrix\n",
    "sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax1)\n",
    "ax1.set_title('Training Confusion Matrix')\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "\n",
    "# Test confusion matrix\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names, ax=ax2)\n",
    "ax2.set_title('Test Confusion Matrix')\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nDETAILED CLASSIFICATION REPORT (Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test_np, test_pred_np, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e5434",
   "metadata": {},
   "source": [
    "## 10. Decision Boundary Visualization (2D Projection)\n",
    "\n",
    "Let's visualize how our model separates the classes using the two most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4927ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary_2d(model, scaler, feature_idx1=0, feature_idx2=1):\n",
    "    \"\"\"Plot decision boundary using two features\"\"\"\n",
    "    \n",
    "    # Create a mesh of points\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, feature_idx1].min() - 1, X[:, feature_idx1].max() + 1\n",
    "    y_min, y_max = X[:, feature_idx2].min() - 1, X[:, feature_idx2].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Create feature matrix for prediction (using mean values for other features)\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    full_features = np.zeros((mesh_points.shape[0], 4))\n",
    "    full_features[:, feature_idx1] = mesh_points[:, 0]\n",
    "    full_features[:, feature_idx2] = mesh_points[:, 1]\n",
    "    \n",
    "    # Use mean values for other features\n",
    "    for i in range(4):\n",
    "        if i not in [feature_idx1, feature_idx2]:\n",
    "            full_features[:, i] = X[:, i].mean()\n",
    "    \n",
    "    # Scale the features\n",
    "    full_features_scaled = scaler.transform(full_features)\n",
    "    mesh_tensor = torch.FloatTensor(full_features_scaled).to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(mesh_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        Z = predicted.cpu().numpy()\n",
    "    \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.6, cmap=plt.cm.RdYlBu)\n",
    "    \n",
    "    # Plot the actual data points\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        mask = y == i\n",
    "        plt.scatter(X[mask, feature_idx1], X[mask, feature_idx2],\n",
    "                   c=colors[i], label=class_name, s=50, edgecolors='black')\n",
    "    \n",
    "    plt.xlabel(feature_names[feature_idx1])\n",
    "    plt.ylabel(feature_names[feature_idx2])\n",
    "    plt.title(f'Decision Boundary: {feature_names[feature_idx1]} vs {feature_names[feature_idx2]}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundary for petal length vs petal width (most separable features)\n",
    "plot_decision_boundary_2d(model, scaler, feature_idx1=2, feature_idx2=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74979121",
   "metadata": {},
   "source": [
    "## 11. Interactive Model Architecture Exploration\n",
    "\n",
    "Let's create an interactive widget to experiment with different model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_training(hidden_size=20, num_layers=2, learning_rate=0.01, epochs=200):\n",
    "    \"\"\"Interactive function to train models with different parameters\"\"\"\n",
    "    \n",
    "    # Create model with specified architecture\n",
    "    class DynamicClassifier(nn.Module):\n",
    "        def __init__(self, input_size=4, hidden_size=hidden_size, \n",
    "                     num_layers=num_layers, num_classes=3):\n",
    "            super().__init__()\n",
    "            layers = []\n",
    "            \n",
    "            # Input layer\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            \n",
    "            # Hidden layers\n",
    "            for _ in range(num_layers - 1):\n",
    "                layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(0.2))\n",
    "            \n",
    "            # Output layer\n",
    "            layers.append(nn.Linear(hidden_size, num_classes))\n",
    "            \n",
    "            self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.network(x)\n",
    "    \n",
    "    # Create and train model\n",
    "    interactive_model = DynamicClassifier().to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in interactive_model.parameters())\n",
    "    \n",
    "    # Train the model\n",
    "    train_losses, test_losses, train_accs, test_accs = train_classifier(\n",
    "        interactive_model, X_train_tensor, y_train_tensor, \n",
    "        X_test_tensor, y_test_tensor, lr=learning_rate, \n",
    "        epochs=epochs, print_every=epochs//4\n",
    "    )\n",
    "    \n",
    "    # Make final predictions\n",
    "    interactive_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = interactive_model(X_test_tensor)\n",
    "        _, test_predicted = torch.max(test_outputs, 1)\n",
    "    \n",
    "    test_pred_np = test_predicted.cpu().numpy()\n",
    "    test_accuracy_final = accuracy_score(y_test_tensor.cpu().numpy(), test_pred_np)\n",
    "    \n",
    "    # Create plots\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(7, 6))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(train_losses, label='Training Loss', color='blue', linewidth=2)\n",
    "    ax1.plot(test_losses, label='Test Loss', color='red', linewidth=2)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(f'Training Progress\\nParameters: {total_params}')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(train_accs, label='Training Accuracy', color='blue', linewidth=2)\n",
    "    ax2.plot(test_accs, label='Test Accuracy', color='red', linewidth=2)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title(f'Model Performance\\nFinal Test Accuracy: {test_accuracy_final:.3f}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test_tensor.cpu().numpy(), test_pred_np)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names, ax=ax3)\n",
    "    ax3.set_title('Confusion Matrix')\n",
    "    ax3.set_xlabel('Predicted')\n",
    "    ax3.set_ylabel('Actual')\n",
    "    \n",
    "    # Feature importance (using gradients)\n",
    "    interactive_model.eval()\n",
    "    X_sample = X_test_tensor[0:1].clone().detach().requires_grad_(True)\n",
    "    output = interactive_model(X_sample)\n",
    "    loss = output.max()\n",
    "    loss.backward()\n",
    "    \n",
    "    feature_importance = X_sample.grad.abs().mean(0).cpu().numpy()\n",
    "    ax4.bar(range(len(feature_names)), feature_importance)\n",
    "    ax4.set_xlabel('Features')\n",
    "    ax4.set_ylabel('Importance')\n",
    "    ax4.set_title('Feature Importance (Gradient-based)')\n",
    "    ax4.set_xticks(range(len(feature_names)))\n",
    "    ax4.set_xticklabels([name.split()[0] for name in feature_names], rotation=45)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return interactive_model\n",
    "\n",
    "# Create interactive widget\n",
    "interact(interactive_training, \n",
    "         hidden_size=(5, 50, 5),\n",
    "         num_layers=(1, 4, 1),\n",
    "         learning_rate=(0.001, 0.1, 0.001),\n",
    "         epochs=(100, 500, 50));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ce52f",
   "metadata": {},
   "source": [
    "## 12. Model Prediction on New Data\n",
    "\n",
    "Let's see how our trained model performs on new, unseen iris measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b756e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_iris_species(sepal_length, sepal_width, petal_length, petal_width):\n",
    "    \"\"\"Predict iris species for given measurements\"\"\"\n",
    "    \n",
    "    # Create input array\n",
    "    new_data = np.array([[sepal_length, sepal_width, petal_length, petal_width]])\n",
    "    \n",
    "    # Scale the input using our fitted scaler\n",
    "    new_data_scaled = scaler.transform(new_data)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    new_tensor = torch.FloatTensor(new_data_scaled).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(new_tensor)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "    \n",
    "    predicted_class = predicted.item()\n",
    "    predicted_species = class_names[predicted_class]\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Input measurements:\")\n",
    "    print(f\"  Sepal Length: {sepal_length} cm\")\n",
    "    print(f\"  Sepal Width:  {sepal_width} cm\")\n",
    "    print(f\"  Petal Length: {petal_length} cm\")\n",
    "    print(f\"  Petal Width:  {petal_width} cm\")\n",
    "    print()\n",
    "    print(f\"Predicted Species: {predicted_species}\")\n",
    "    print(f\"Confidence: {confidence:.3f}\")\n",
    "    print()\n",
    "    print(\"Probabilities for each class:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        prob = probabilities[0][i].item()\n",
    "        print(f\"  {class_name}: {prob:.3f}\")\n",
    "\n",
    "# Test with some example measurements\n",
    "print(\"EXAMPLE PREDICTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example 1: Typical Setosa (small petals)\n",
    "print(\"Example 1 - Typical Setosa characteristics:\")\n",
    "predict_iris_species(5.1, 3.5, 1.4, 0.2)\n",
    "print()\n",
    "\n",
    "# Example 2: Typical Versicolor\n",
    "print(\"Example 2 - Typical Versicolor characteristics:\")\n",
    "predict_iris_species(6.0, 2.7, 4.0, 1.3)\n",
    "print()\n",
    "\n",
    "# Example 3: Typical Virginica (large petals)\n",
    "print(\"Example 3 - Typical Virginica characteristics:\")\n",
    "predict_iris_species(6.5, 3.0, 5.8, 2.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8317889",
   "metadata": {},
   "source": [
    "## 13. Key Insights and Industrial Applications\n",
    "\n",
    "Let's summarize what we've learned and discuss real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a0e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of model performance\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.1%}\")\n",
    "print(f\"Model Parameters: {total_params}\")\n",
    "print(f\"Training Time: ~{len(train_losses)} epochs\")\n",
    "print()\n",
    "\n",
    "print(\"CLASSIFICATION INSIGHTS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"âœ… Setosa is easily separable from other species\")\n",
    "print(\"âœ… Petal measurements are more discriminative than sepal measurements\")\n",
    "print(\"âœ… Neural network achieved high accuracy with simple architecture\")\n",
    "print(\"âœ… Model generalizes well to unseen data\")\n",
    "print()\n",
    "\n",
    "print(\"INDUSTRIAL APPLICATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"ðŸ­ Quality Control: Classify products as defective/non-defective\")\n",
    "print(\"ðŸ”§ Fault Detection: Identify equipment failure modes\")\n",
    "print(\"ðŸ“Š Process Monitoring: Categorize process states (normal/abnormal)\")\n",
    "print(\"ðŸ¤– Automated Inspection: Sort materials by type or grade\")\n",
    "print(\"âš¡ Predictive Maintenance: Classify equipment condition levels\")\n",
    "print(\"ðŸŽ¯ Production Optimization: Classify optimal operating conditions\")\n",
    "\n",
    "# Feature importance analysis\n",
    "model.eval()\n",
    "feature_gradients = []\n",
    "\n",
    "for i in range(len(X_test_tensor)):\n",
    "    x_sample = X_test_tensor[i:i+1].clone().detach().requires_grad_(True)\n",
    "    output = model(x_sample)\n",
    "    loss = output.max()\n",
    "    loss.backward()\n",
    "    feature_gradients.append(x_sample.grad.abs().cpu().numpy())\n",
    "\n",
    "# Average feature importance\n",
    "avg_importance = np.mean(feature_gradients, axis=0).flatten()\n",
    "\n",
    "print(\"\\nFEATURE IMPORTANCE RANKING:\")\n",
    "print(\"-\" * 40)\n",
    "feature_importance_pairs = list(zip(feature_names, avg_importance))\n",
    "feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (feature, importance) in enumerate(feature_importance_pairs, 1):\n",
    "    print(f\"{i}. {feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c590a0ea",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "### **Classification vs Regression:**\n",
    "- **Classification**: Predicts discrete categories (species, quality grades, fault types)\n",
    "- **Regression**: Predicts continuous values (temperature, pressure, flow rate)\n",
    "\n",
    "### **Model Architecture:**\n",
    "- Simple neural networks can achieve excellent results on well-separated data\n",
    "- Feature scaling is crucial for neural network performance\n",
    "- Dropout helps prevent overfitting in classification tasks\n",
    "\n",
    "### **Evaluation Metrics:**\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Precision**: How many predicted positives were actually positive\n",
    "- **Recall**: How many actual positives were correctly identified\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **Confusion Matrix**: Detailed breakdown of classification performance\n",
    "\n",
    "### **Industrial Relevance:**\n",
    "- Classification is fundamental to quality control and automation\n",
    "- Real-time classification enables immediate decision making\n",
    "- Multiple classes allow for nuanced categorization of industrial processes\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps: Try modifying the network architecture, experimenting with different features, or applying this approach to your own industrial classification problems!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
