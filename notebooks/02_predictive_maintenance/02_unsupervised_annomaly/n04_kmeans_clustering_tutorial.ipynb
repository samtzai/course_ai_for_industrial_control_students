{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5af0613",
   "metadata": {},
   "source": [
    "# üå∫ K-means Clustering Tutorial: Unsupervised Learning with the Iris Dataset\n",
    "\n",
    "## **Discover Hidden Patterns in Data with Interactive Clustering Implementation**\n",
    "\n",
    "### **Main Goal:**\n",
    "Learn how to group iris flowers into clusters based on their physical measurements using K-means clustering, while understanding every step of the unsupervised learning pipeline through interactive visualizations.\n",
    "\n",
    "### **Key Learning Objectives:**\n",
    "- **Clustering Fundamentals**: Understand unsupervised learning and clustering concepts\n",
    "- **K-means Algorithm**: Learn how the algorithm works and its parameters\n",
    "- **Optimal Cluster Selection**: Use elbow method and silhouette analysis\n",
    "- **Cluster Evaluation**: Compare clustering results with true labels\n",
    "- **3D Visualization**: Explore clusters in multiple dimensions\n",
    "\n",
    "### **Industrial Relevance:**\n",
    "Apply to **pattern recognition**, **anomaly detection**, **customer segmentation**, **process optimization**, and **equipment grouping** in industrial applications.\n",
    "\n",
    "### **Interactive Features:**\n",
    "üéÆ Real-time clustering | üìä 3D cluster visualization | ‚ö° Silhouette analysis | üî¨ Cluster center exploration\n",
    "\n",
    "**Dataset**: Iris flower dataset with 4 features (sepal/petal length/width) - discovering natural groupings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1648321b",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll use scikit-learn for clustering, NumPy for numerical operations, and various visualization libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e944ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, homogeneity_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set styling\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure display\n",
    "%matplotlib widget\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, IntSlider\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"üìä Ready for K-means clustering analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aefe50f",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Iris Dataset\n",
    "\n",
    "Let's load the famous iris dataset and understand its structure for clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce176af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Create a DataFrame for easier analysis\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['species'] = [target_names[i] for i in y]\n",
    "df['target'] = y\n",
    "\n",
    "print(\"üå∫ IRIS DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Number of classes: {len(target_names)}\")\n",
    "print(f\"Classes: {target_names}\")\n",
    "\n",
    "print(\"\\nüìä First 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nüìà Dataset statistics:\")\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Class distribution:\")\n",
    "class_counts = df['species'].value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "# Create a summary visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(7, 5))\n",
    "\n",
    "# Class distribution pie chart\n",
    "ax1.pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%')\n",
    "ax1.set_title('Species Distribution')\n",
    "\n",
    "# Feature ranges\n",
    "feature_ranges = df[feature_names].max() - df[feature_names].min()\n",
    "ax2.bar(range(len(feature_names)), feature_ranges, color='skyblue')\n",
    "ax2.set_xlabel('Features')\n",
    "ax2.set_ylabel('Range')\n",
    "ax2.set_title('Feature Value Ranges')\n",
    "ax2.set_xticks(range(len(feature_names)))\n",
    "ax2.set_xticklabels([name.split()[0] for name in feature_names], rotation=45)\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = df[feature_names].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=ax3)\n",
    "ax3.set_title('Feature Correlation Matrix')\n",
    "\n",
    "# Feature means by species\n",
    "species_means = df.groupby('species')[feature_names].mean()\n",
    "species_means.T.plot(kind='bar', ax=ax4)\n",
    "ax4.set_title('Mean Feature Values by Species')\n",
    "ax4.set_xlabel('Features')\n",
    "ax4.set_ylabel('Mean Value')\n",
    "ax4.legend(title='Species')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d94064",
   "metadata": {},
   "source": [
    "## 3. Data Visualization and Exploration\n",
    "\n",
    "Let's visualize the data to understand the natural groupings and relationships between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d6dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pairwise scatter plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Color mapping for species\n",
    "colors = ['red', 'blue', 'green']\n",
    "species_colors = {species: colors[i] for i, species in enumerate(target_names)}\n",
    "\n",
    "# Feature pairs for visualization\n",
    "feature_pairs = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n",
    "\n",
    "for idx, (feat1, feat2) in enumerate(feature_pairs):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for species in target_names:\n",
    "        mask = df['species'] == species\n",
    "        ax.scatter(df[mask].iloc[:, feat1], df[mask].iloc[:, feat2], \n",
    "                  c=species_colors[species], label=species, alpha=0.7, s=60)\n",
    "    \n",
    "    ax.set_xlabel(feature_names[feat1])\n",
    "    ax.set_ylabel(feature_names[feat2])\n",
    "    ax.set_title(f'{feature_names[feat1]} vs {feature_names[feat2]}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature distribution analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    for species in target_names:\n",
    "        species_data = df[df['species'] == species][feature]\n",
    "        ax.hist(species_data, alpha=0.6, label=species, bins=15, density=True)\n",
    "    \n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'Distribution of {feature}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Box plots for feature comparison\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, feature in enumerate(feature_names, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.boxplot(data=df, x='species', y=feature)\n",
    "    plt.title(f'{feature} by Species')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Key Observations from Data Exploration:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Setosa appears well-separated from other species\")\n",
    "print(\"‚úÖ Petal length and width show strong discriminative power\")\n",
    "print(\"‚úÖ Versicolor and Virginica overlap in some feature spaces\")\n",
    "print(\"‚úÖ Features show different scales - normalization may be beneficial\")\n",
    "print(\"‚úÖ Natural groupings are visible in 2D projections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a0bfc",
   "metadata": {},
   "source": [
    "## 4. Implement K-means Clustering\n",
    "\n",
    "Now let's apply K-means clustering to discover patterns in the data without using the species labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74741678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features for better clustering performance\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"üîÑ Data Preprocessing:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Original data shape: {X.shape}\")\n",
    "print(f\"Scaled data shape: {X_scaled.shape}\")\n",
    "print(f\"Feature means after scaling: {X_scaled.mean(axis=0).round(3)}\")\n",
    "print(f\"Feature std after scaling: {X_scaled.std(axis=0).round(3)}\")\n",
    "\n",
    "# Apply K-means with different numbers of clusters\n",
    "k_values = [2, 3, 4, 5]\n",
    "kmeans_results = {}\n",
    "\n",
    "print(\"\\nüéØ K-means Clustering Results:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for k in k_values:\n",
    "    # Fit K-means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "    \n",
    "    # Store results\n",
    "    kmeans_results[k] = {\n",
    "        'model': kmeans,\n",
    "        'labels': cluster_labels,\n",
    "        'centers': kmeans.cluster_centers_,\n",
    "        'inertia': kmeans.inertia_,\n",
    "        'silhouette': silhouette_avg\n",
    "    }\n",
    "    \n",
    "    print(f\"K={k}: Silhouette Score = {silhouette_avg:.3f}, Inertia = {kmeans.inertia_:.2f}\")\n",
    "\n",
    "# Focus on K=3 (expected number of species)\n",
    "k_optimal = 3\n",
    "kmeans_3 = kmeans_results[k_optimal]['model']\n",
    "cluster_labels_3 = kmeans_results[k_optimal]['labels']\n",
    "cluster_centers_3 = kmeans_results[k_optimal]['centers']\n",
    "\n",
    "print(f\"\\nüéØ Detailed Results for K={k_optimal}:\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Silhouette Score: {kmeans_results[k_optimal]['silhouette']:.3f}\")\n",
    "print(f\"Inertia (sum of squared distances): {kmeans_results[k_optimal]['inertia']:.2f}\")\n",
    "print(f\"Number of iterations: {kmeans_3.n_iter_}\")\n",
    "\n",
    "# Display cluster centers (scaled)\n",
    "print(f\"\\nüìç Cluster Centers (standardized features):\")\n",
    "centers_df = pd.DataFrame(cluster_centers_3, columns=feature_names)\n",
    "centers_df.index = [f'Cluster {i}' for i in range(k_optimal)]\n",
    "display(centers_df.round(3))\n",
    "\n",
    "# Transform back to original scale for interpretation\n",
    "centers_original = scaler.inverse_transform(cluster_centers_3)\n",
    "print(f\"\\nüìç Cluster Centers (original scale):\")\n",
    "centers_orig_df = pd.DataFrame(centers_original, columns=feature_names)\n",
    "centers_orig_df.index = [f'Cluster {i}' for i in range(k_optimal)]\n",
    "display(centers_orig_df.round(3))\n",
    "\n",
    "# Add cluster labels to DataFrame\n",
    "df_clustered = df.copy()\n",
    "df_clustered['cluster'] = cluster_labels_3\n",
    "\n",
    "print(f\"\\nüìä Cluster Size Distribution:\")\n",
    "cluster_counts = pd.Series(cluster_labels_3).value_counts().sort_index()\n",
    "print(cluster_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b0484",
   "metadata": {},
   "source": [
    "## 5. Determine Optimal Number of Clusters\n",
    "\n",
    "Let's use the elbow method and silhouette analysis to find the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f8ba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended range for cluster optimization\n",
    "k_range = range(2, 11)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "print(\"üîç Searching for Optimal Number of Clusters...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "    print(f\"K={k}: Inertia={kmeans.inertia_:.2f}, Silhouette={silhouette_avg:.3f}\")\n",
    "\n",
    "# Create elbow and silhouette plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3))\n",
    "\n",
    "# Elbow Method Plot\n",
    "ax1.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia (Within-cluster sum of squares)')\n",
    "ax1.set_title('Elbow Method for Optimal k')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axvline(x=3, color='red', linestyle='--', alpha=0.7, label='k=3')\n",
    "ax1.legend()\n",
    "\n",
    "# Add annotations for key points\n",
    "for i, (k, inertia) in enumerate(zip(k_range, inertias)):\n",
    "    if k in [2, 3, 4]:\n",
    "        ax1.annotate(f'k={k}\\n{inertia:.1f}', \n",
    "                    (k, inertia), \n",
    "                    textcoords=\"offset points\", \n",
    "                    xytext=(0,10), \n",
    "                    ha='center')\n",
    "\n",
    "# Silhouette Score Plot\n",
    "ax2.plot(k_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Number of Clusters (k)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Analysis for Optimal k')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axvline(x=3, color='red', linestyle='--', alpha=0.7, label='k=3')\n",
    "ax2.legend()\n",
    "\n",
    "# Add annotations for key points\n",
    "for i, (k, score) in enumerate(zip(k_range, silhouette_scores)):\n",
    "    if k in [2, 3, 4]:\n",
    "        ax2.annotate(f'k={k}\\n{score:.3f}', \n",
    "                    (k, score), \n",
    "                    textcoords=\"offset points\", \n",
    "                    xytext=(0,10), \n",
    "                    ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate elbow point using second derivative\n",
    "def calculate_elbow_point(inertias):\n",
    "    \"\"\"Calculate the elbow point using second derivative\"\"\"\n",
    "    if len(inertias) < 3:\n",
    "        return None\n",
    "    \n",
    "    # Calculate first and second derivatives\n",
    "    first_diff = np.diff(inertias)\n",
    "    second_diff = np.diff(first_diff)\n",
    "    \n",
    "    # Find the point where second derivative is maximum\n",
    "    elbow_idx = np.argmax(second_diff) + 2  # +2 because of double diff\n",
    "    return elbow_idx\n",
    "\n",
    "elbow_k = calculate_elbow_point(inertias)\n",
    "best_silhouette_k = k_range[np.argmax(silhouette_scores)]\n",
    "\n",
    "print(f\"\\nüìä CLUSTER OPTIMIZATION RESULTS:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"üîµ Elbow method suggests: k = {elbow_k}\")\n",
    "print(f\"üî¥ Best silhouette score: k = {best_silhouette_k} (score: {max(silhouette_scores):.3f})\")\n",
    "print(f\"üéØ Biological expectation: k = 3 (three iris species)\")\n",
    "\n",
    "# Detailed silhouette analysis for k=2,3,4\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, k in enumerate([2, 3, 4]):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Fit K-means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Calculate silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_score(X_scaled, cluster_labels, sample_size=None)\n",
    "    \n",
    "    # Create silhouette plot\n",
    "    from sklearn.metrics import silhouette_samples\n",
    "    sample_silhouette_values = silhouette_samples(X_scaled, cluster_labels)\n",
    "    \n",
    "    y_lower = 10\n",
    "    for i in range(k):\n",
    "        cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        cluster_silhouette_values.sort()\n",
    "        \n",
    "        size_cluster_i = cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        color = plt.cm.nipy_spectral(float(i) / k)\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                        0, cluster_silhouette_values,\n",
    "                        facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        \n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10\n",
    "    \n",
    "    ax.set_xlabel('Silhouette coefficient values')\n",
    "    ax.set_ylabel('Cluster label')\n",
    "    ax.set_title(f'Silhouette Plot for k={k}')\n",
    "    \n",
    "    # Add average silhouette score line\n",
    "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", \n",
    "               label=f'Average Score: {silhouette_avg:.3f}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9539157c",
   "metadata": {},
   "source": [
    "## 6. Visualize Clustering Results\n",
    "\n",
    "Let's create 2D scatter plots showing cluster assignments with different colors and visualize cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44716f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering results for k=3\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Color maps for clusters\n",
    "cluster_colors = ['red', 'blue', 'green', 'purple', 'orange']\n",
    "feature_pairs = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n",
    "\n",
    "for idx, (feat1, feat2) in enumerate(feature_pairs):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot data points colored by cluster\n",
    "    for cluster in range(3):\n",
    "        mask = cluster_labels_3 == cluster\n",
    "        ax.scatter(X[mask, feat1], X[mask, feat2], \n",
    "                  c=cluster_colors[cluster], label=f'Cluster {cluster}', \n",
    "                  alpha=0.7, s=60, edgecolors='black', linewidths=0.5)\n",
    "    \n",
    "    # Plot cluster centers (transformed back to original scale)\n",
    "    centers_orig = scaler.inverse_transform(cluster_centers_3)\n",
    "    ax.scatter(centers_orig[:, feat1], centers_orig[:, feat2], \n",
    "              c='yellow', s=300, marker='*', edgecolors='black', \n",
    "              linewidths=2, label='Centroids')\n",
    "    \n",
    "    ax.set_xlabel(feature_names[feat1])\n",
    "    ax.set_ylabel(feature_names[feat2])\n",
    "    ax.set_title(f'Clusters: {feature_names[feat1]} vs {feature_names[feat2]}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a comparison plot: Original species vs Clusters\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n",
    "\n",
    "# Most discriminative features: petal length vs petal width\n",
    "feat1, feat2 = 2, 3  # petal length, petal width\n",
    "\n",
    "# Plot 1: True species labels\n",
    "ax1 = axes[0, 0]\n",
    "for i, species in enumerate(target_names):\n",
    "    mask = y == i\n",
    "    ax1.scatter(X[mask, feat1], X[mask, feat2], \n",
    "               c=colors[i], label=species, alpha=0.7, s=60)\n",
    "ax1.set_xlabel(feature_names[feat1])\n",
    "ax1.set_ylabel(feature_names[feat2])\n",
    "ax1.set_title('True Species Labels')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: K-means clusters\n",
    "ax2 = axes[0, 1]\n",
    "for cluster in range(3):\n",
    "    mask = cluster_labels_3 == cluster\n",
    "    ax2.scatter(X[mask, feat1], X[mask, feat2], \n",
    "               c=cluster_colors[cluster], label=f'Cluster {cluster}', \n",
    "               alpha=0.7, s=60, edgecolors='black', linewidths=0.5)\n",
    "centers_orig = scaler.inverse_transform(cluster_centers_3)\n",
    "ax2.scatter(centers_orig[:, feat1], centers_orig[:, feat2], \n",
    "           c='yellow', s=300, marker='*', edgecolors='black', \n",
    "           linewidths=2, label='Centroids')\n",
    "ax2.set_xlabel(feature_names[feat1])\n",
    "ax2.set_ylabel(feature_names[feat2])\n",
    "ax2.set_title('K-means Clusters (k=3)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Sepal length vs sepal width\n",
    "feat1, feat2 = 0, 1\n",
    "ax3 = axes[1, 0]\n",
    "for cluster in range(3):\n",
    "    mask = cluster_labels_3 == cluster\n",
    "    ax3.scatter(X[mask, feat1], X[mask, feat2], \n",
    "               c=cluster_colors[cluster], label=f'Cluster {cluster}', \n",
    "               alpha=0.7, s=60, edgecolors='black', linewidths=0.5)\n",
    "ax3.scatter(centers_orig[:, feat1], centers_orig[:, feat2], \n",
    "           c='yellow', s=300, marker='*', edgecolors='black', \n",
    "           linewidths=2, label='Centroids')\n",
    "ax3.set_xlabel(feature_names[feat1])\n",
    "ax3.set_ylabel(feature_names[feat2])\n",
    "ax3.set_title('Clusters: Sepal Length vs Sepal Width')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: PCA 2D visualization\n",
    "ax4 = axes[1, 1]\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "for cluster in range(3):\n",
    "    mask = cluster_labels_3 == cluster\n",
    "    ax4.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "               c=cluster_colors[cluster], label=f'Cluster {cluster}', \n",
    "               alpha=0.7, s=60, edgecolors='black', linewidths=0.5)\n",
    "\n",
    "# Transform cluster centers to PCA space\n",
    "centers_pca = pca.transform(cluster_centers_3)\n",
    "ax4.scatter(centers_pca[:, 0], centers_pca[:, 1], \n",
    "           c='yellow', s=300, marker='*', edgecolors='black', \n",
    "           linewidths=2, label='Centroids')\n",
    "\n",
    "ax4.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax4.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax4.set_title('PCA 2D Projection of Clusters')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä PCA Analysis:\")\n",
    "print(f\"   PC1 explains {pca.explained_variance_ratio_[0]:.1%} of variance\")\n",
    "print(f\"   PC2 explains {pca.explained_variance_ratio_[1]:.1%} of variance\")\n",
    "print(f\"   Total variance explained: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "\n",
    "# Cluster characteristics analysis\n",
    "print(f\"\\nüìã Cluster Characteristics (k=3):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for cluster in range(3):\n",
    "    mask = cluster_labels_3 == cluster\n",
    "    cluster_data = X[mask]\n",
    "    \n",
    "    print(f\"\\nüîπ Cluster {cluster} (n={np.sum(mask)}):\")\n",
    "    print(\"   Mean values:\")\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        mean_val = cluster_data[:, i].mean()\n",
    "        print(f\"     {feature}: {mean_val:.2f}\")\n",
    "    \n",
    "    print(\"   Standard deviations:\")\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        std_val = cluster_data[:, i].std()\n",
    "        print(f\"     {feature}: {std_val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16475bcf",
   "metadata": {},
   "source": [
    "## 7. Compare Clusters with True Species Labels\n",
    "\n",
    "Let's compare the clustering results with the actual iris species labels to evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550095ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix-like comparison\n",
    "def create_cluster_species_matrix(cluster_labels, true_labels, cluster_names, species_names):\n",
    "    \"\"\"Create a matrix showing how clusters align with true species\"\"\"\n",
    "    matrix = np.zeros((len(cluster_names), len(species_names)))\n",
    "    \n",
    "    for i, cluster in enumerate(cluster_names):\n",
    "        for j, species in enumerate(species_names):\n",
    "            count = np.sum((cluster_labels == i) & (true_labels == j))\n",
    "            matrix[i, j] = count\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "def find_best_mapping(true_labels, cluster_labels):\n",
    "    \"\"\"Find the best mapping between clusters and true labels\"\"\"\n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "    \n",
    "    # Create cost matrix\n",
    "    n_clusters = len(np.unique(cluster_labels))\n",
    "    n_classes = len(np.unique(true_labels))\n",
    "    \n",
    "    cost_matrix = np.zeros((n_clusters, n_classes))\n",
    "    for i in range(n_clusters):\n",
    "        for j in range(n_classes):\n",
    "            # Cost is negative count (we want to maximize overlap)\n",
    "            cost_matrix[i, j] = -np.sum((cluster_labels == i) & (true_labels == j))\n",
    "    \n",
    "    row_indices, col_indices = linear_sum_assignment(cost_matrix)\n",
    "    mapping = dict(zip(row_indices, col_indices))\n",
    "    return mapping\n",
    "\n",
    "# Create comparison matrix\n",
    "cluster_species_matrix = create_cluster_species_matrix(\n",
    "    cluster_labels_3, y, range(3), range(3)\n",
    ")\n",
    "\n",
    "# Display as DataFrame for better readability\n",
    "comparison_df = pd.DataFrame(\n",
    "    cluster_species_matrix.astype(int),\n",
    "    index=[f'Cluster {i}' for i in range(3)],\n",
    "    columns=target_names\n",
    ")\n",
    "\n",
    "print(\"üîç CLUSTER vs SPECIES COMPARISON\")\n",
    "print(\"=\" * 45)\n",
    "print(\"Rows: Clusters | Columns: True Species\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Visualize the comparison matrix\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3))\n",
    "\n",
    "# Heatmap of cluster-species alignment\n",
    "sns.heatmap(comparison_df, annot=True, fmt='d', cmap='Blues', \n",
    "            cbar_kws={'label': 'Number of samples'}, ax=ax1)\n",
    "ax1.set_title('Cluster-Species Alignment Matrix')\n",
    "ax1.set_ylabel('Clusters')\n",
    "ax1.set_xlabel('True Species')\n",
    "\n",
    "# Calculate purity for each cluster\n",
    "cluster_purities = []\n",
    "for i in range(3):\n",
    "    cluster_counts = comparison_df.iloc[i].values\n",
    "    total_in_cluster = cluster_counts.sum()\n",
    "    max_species_in_cluster = cluster_counts.max()\n",
    "    purity = max_species_in_cluster / total_in_cluster if total_in_cluster > 0 else 0\n",
    "    cluster_purities.append(purity)\n",
    "\n",
    "ax2.bar(range(3), cluster_purities, color=['red', 'blue', 'green'], alpha=0.7)\n",
    "ax2.set_xlabel('Cluster')\n",
    "ax2.set_ylabel('Purity')\n",
    "ax2.set_title('Cluster Purity Scores')\n",
    "ax2.set_xticks(range(3))\n",
    "ax2.set_xticklabels([f'Cluster {i}' for i in range(3)])\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, purity in enumerate(cluster_purities):\n",
    "    ax2.text(i, purity + 0.02, f'{purity:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the best cluster-to-species mapping\n",
    "print(f\"\\nüéØ CLUSTER-SPECIES MAPPING ANALYSIS:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "for i in range(3):\n",
    "    cluster_counts = comparison_df.iloc[i].values\n",
    "    dominant_species_idx = np.argmax(cluster_counts)\n",
    "    dominant_species = target_names[dominant_species_idx]\n",
    "    cluster_size = cluster_counts.sum()\n",
    "    dominant_count = cluster_counts[dominant_species_idx]\n",
    "    purity = dominant_count / cluster_size\n",
    "    \n",
    "    print(f\"Cluster {i}:\")\n",
    "    print(f\"  üìä Size: {cluster_size} samples\")\n",
    "    print(f\"  üè∑Ô∏è  Dominant species: {dominant_species} ({dominant_count}/{cluster_size} = {purity:.1%})\")\n",
    "    print(f\"  üéØ Purity: {purity:.3f}\")\n",
    "    \n",
    "    # Show distribution within cluster\n",
    "    print(f\"  üìã Species distribution:\")\n",
    "    for j, species in enumerate(target_names):\n",
    "        count = cluster_counts[j]\n",
    "        percentage = count / cluster_size * 100\n",
    "        print(f\"     {species}: {count} ({percentage:.1f}%)\")\n",
    "    print()\n",
    "\n",
    "# Calculate overall clustering quality metrics\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, homogeneity_score, completeness_score, v_measure_score\n",
    "\n",
    "ari_score = adjusted_rand_score(y, cluster_labels_3)\n",
    "nmi_score = normalized_mutual_info_score(y, cluster_labels_3)\n",
    "homogeneity = homogeneity_score(y, cluster_labels_3)\n",
    "completeness = completeness_score(y, cluster_labels_3)\n",
    "v_measure = v_measure_score(y, cluster_labels_3)\n",
    "\n",
    "print(f\"üìà CLUSTERING QUALITY METRICS:\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"üéØ Adjusted Rand Index: {ari_score:.3f}\")\n",
    "print(f\"üîÑ Normalized Mutual Information: {nmi_score:.3f}\")\n",
    "print(f\"üè† Homogeneity Score: {homogeneity:.3f}\")\n",
    "print(f\"‚úÖ Completeness Score: {completeness:.3f}\")\n",
    "print(f\"üìä V-measure Score: {v_measure:.3f}\")\n",
    "print(f\"üîµ Silhouette Score: {silhouette_score(X_scaled, cluster_labels_3):.3f}\")\n",
    "\n",
    "print(f\"\\nüìù INTERPRETATION:\")\n",
    "print(\"=\" * 20)\n",
    "print(\"‚Ä¢ Adjusted Rand Index: 1.0 = perfect clustering, 0.0 = random\")\n",
    "print(\"‚Ä¢ Homogeneity: All clusters contain only members of a single class\")\n",
    "print(\"‚Ä¢ Completeness: All members of a given class are assigned to the same cluster\")\n",
    "print(\"‚Ä¢ V-measure: Harmonic mean of homogeneity and completeness\")\n",
    "\n",
    "best_mapping = find_best_mapping(y, cluster_labels_3)\n",
    "print(f\"\\nüó∫Ô∏è  OPTIMAL CLUSTER-TO-SPECIES MAPPING:\")\n",
    "print(\"=\" * 40)\n",
    "for cluster, species_idx in best_mapping.items():\n",
    "    print(f\"Cluster {cluster} ‚Üí {target_names[species_idx]}\")\n",
    "\n",
    "# Calculate accuracy with best mapping\n",
    "mapped_predictions = [best_mapping[cluster] for cluster in cluster_labels_3]\n",
    "accuracy = np.mean(np.array(mapped_predictions) == y)\n",
    "print(f\"\\nüéØ Clustering Accuracy (with optimal mapping): {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b8c39",
   "metadata": {},
   "source": [
    "## 9. Clustering Performance Evaluation\n",
    "\n",
    "Let's calculate and display comprehensive clustering metrics to evaluate our K-means performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0415a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation of clustering performance\n",
    "def evaluate_clustering_performance(X_data, true_labels, cluster_labels, k):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of clustering performance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Internal validation metrics (don't require true labels)\n",
    "    silhouette_avg = silhouette_score(X_data, cluster_labels)\n",
    "    \n",
    "    # Calculate individual silhouette scores for analysis\n",
    "    sample_silhouette_values = silhouette_samples(X_data, cluster_labels)\n",
    "    \n",
    "    # External validation metrics (require true labels)\n",
    "    ari_score = adjusted_rand_score(true_labels, cluster_labels)\n",
    "    nmi_score = normalized_mutual_info_score(true_labels, cluster_labels)\n",
    "    homogeneity = homogeneity_score(true_labels, cluster_labels)\n",
    "    completeness = completeness_score(true_labels, cluster_labels)\n",
    "    v_measure = v_measure_score(true_labels, cluster_labels)\n",
    "    \n",
    "    # Inertia (sum of squared distances to centroids)\n",
    "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_temp.fit(X_data)\n",
    "    inertia = kmeans_temp.inertia_\n",
    "    \n",
    "    return {\n",
    "        'silhouette_avg': silhouette_avg,\n",
    "        'silhouette_samples': sample_silhouette_values,\n",
    "        'ari': ari_score,\n",
    "        'nmi': nmi_score,\n",
    "        'homogeneity': homogeneity,\n",
    "        'completeness': completeness,\n",
    "        'v_measure': v_measure,\n",
    "        'inertia': inertia\n",
    "    }\n",
    "\n",
    "# Evaluate performance for different k values\n",
    "k_values_eval = range(2, 8)\n",
    "performance_results = {}\n",
    "\n",
    "print(\"üîç COMPREHENSIVE CLUSTERING EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for k in k_values_eval:\n",
    "    # Fit K-means\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    results = evaluate_clustering_performance(X_scaled, y, cluster_labels, k)\n",
    "    performance_results[k] = results\n",
    "    \n",
    "    print(f\"\\nK = {k}:\")\n",
    "    print(f\"  Silhouette Score: {results['silhouette_avg']:.4f}\")\n",
    "    print(f\"  Adjusted Rand Index: {results['ari']:.4f}\")\n",
    "    print(f\"  Homogeneity: {results['homogeneity']:.4f}\")\n",
    "    print(f\"  Completeness: {results['completeness']:.4f}\")\n",
    "    print(f\"  V-measure: {results['v_measure']:.4f}\")\n",
    "    print(f\"  Inertia: {results['inertia']:.2f}\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Extract metrics for plotting\n",
    "k_list = list(performance_results.keys())\n",
    "silhouette_scores = [performance_results[k]['silhouette_avg'] for k in k_list]\n",
    "ari_scores = [performance_results[k]['ari'] for k in k_list]\n",
    "inertias = [performance_results[k]['inertia'] for k in k_list]\n",
    "homogeneity_scores = [performance_results[k]['homogeneity'] for k in k_list]\n",
    "completeness_scores = [performance_results[k]['completeness'] for k in k_list]\n",
    "v_measure_scores = [performance_results[k]['v_measure'] for k in k_list]\n",
    "\n",
    "# Plot 1: Silhouette Score\n",
    "axes[0].plot(k_list, silhouette_scores, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (k)')\n",
    "axes[0].set_ylabel('Silhouette Score')\n",
    "axes[0].set_title('Silhouette Score vs k')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axvline(x=3, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot 2: Adjusted Rand Index\n",
    "axes[1].plot(k_list, ari_scores, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Clusters (k)')\n",
    "axes[1].set_ylabel('Adjusted Rand Index')\n",
    "axes[1].set_title('ARI vs k')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axvline(x=3, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot 3: Inertia (Elbow)\n",
    "axes[2].plot(k_list, inertias, 'go-', linewidth=2, markersize=8)\n",
    "axes[2].set_xlabel('Number of Clusters (k)')\n",
    "axes[2].set_ylabel('Inertia')\n",
    "axes[2].set_title('Elbow Method')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].axvline(x=3, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot 4: Homogeneity\n",
    "axes[3].plot(k_list, homogeneity_scores, 'mo-', linewidth=2, markersize=8)\n",
    "axes[3].set_xlabel('Number of Clusters (k)')\n",
    "axes[3].set_ylabel('Homogeneity Score')\n",
    "axes[3].set_title('Homogeneity vs k')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "axes[3].axvline(x=3, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot 5: Completeness\n",
    "axes[4].plot(k_list, completeness_scores, 'co-', linewidth=2, markersize=8)\n",
    "axes[4].set_xlabel('Number of Clusters (k)')\n",
    "axes[4].set_ylabel('Completeness Score')\n",
    "axes[4].set_title('Completeness vs k')\n",
    "axes[4].grid(True, alpha=0.3)\n",
    "axes[4].axvline(x=3, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot 6: V-measure\n",
    "axes[5].plot(k_list, v_measure_scores, 'yo-', linewidth=2, markersize=8)\n",
    "axes[5].set_xlabel('Number of Clusters (k)')\n",
    "axes[5].set_ylabel('V-measure Score')\n",
    "axes[5].set_title('V-measure vs k')\n",
    "axes[5].grid(True, alpha=0.3)\n",
    "axes[5].axvline(x=3, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a summary table\n",
    "summary_df = pd.DataFrame({\n",
    "    'k': k_list,\n",
    "    'Silhouette': [f\"{score:.3f}\" for score in silhouette_scores],\n",
    "    'ARI': [f\"{score:.3f}\" for score in ari_scores],\n",
    "    'Homogeneity': [f\"{score:.3f}\" for score in homogeneity_scores],\n",
    "    'Completeness': [f\"{score:.3f}\" for score in completeness_scores],\n",
    "    'V-measure': [f\"{score:.3f}\" for score in v_measure_scores],\n",
    "    'Inertia': [f\"{score:.1f}\" for score in inertias]\n",
    "})\n",
    "\n",
    "print(f\"\\nüìä CLUSTERING PERFORMANCE SUMMARY TABLE\")\n",
    "print(\"=\" * 80)\n",
    "display(summary_df)\n",
    "\n",
    "# Find optimal k for each metric\n",
    "optimal_k_silhouette = k_list[np.argmax(silhouette_scores)]\n",
    "optimal_k_ari = k_list[np.argmax(ari_scores)]\n",
    "optimal_k_v_measure = k_list[np.argmax(v_measure_scores)]\n",
    "\n",
    "print(f\"\\nüèÜ OPTIMAL k VALUES BY METRIC:\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"üìä Best Silhouette Score: k = {optimal_k_silhouette} ({max(silhouette_scores):.3f})\")\n",
    "print(f\"üéØ Best ARI Score: k = {optimal_k_ari} ({max(ari_scores):.3f})\")\n",
    "print(f\"üìà Best V-measure Score: k = {optimal_k_v_measure} ({max(v_measure_scores):.3f})\")\n",
    "\n",
    "# Detailed analysis for k=3 (biological truth)\n",
    "k3_results = performance_results[3]\n",
    "print(f\"\\nüî¨ DETAILED ANALYSIS FOR K=3:\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"‚úÖ Matches biological expectation (3 species)\")\n",
    "print(f\"üìä Silhouette Score: {k3_results['silhouette_avg']:.3f}\")\n",
    "print(f\"üéØ ARI (vs true labels): {k3_results['ari']:.3f}\")\n",
    "print(f\"üè† Homogeneity: {k3_results['homogeneity']:.3f}\")\n",
    "print(f\"‚úîÔ∏è  Completeness: {k3_results['completeness']:.3f}\")\n",
    "print(f\"üìà V-measure: {k3_results['v_measure']:.3f}\")\n",
    "\n",
    "# Silhouette analysis for k=3\n",
    "sample_silhouette_values_k3 = k3_results['silhouette_samples']\n",
    "cluster_labels_k3 = KMeans(n_clusters=3, random_state=42, n_init=10).fit_predict(X_scaled)\n",
    "\n",
    "print(f\"\\nüîç SILHOUETTE ANALYSIS FOR K=3:\")\n",
    "print(\"=\" * 35)\n",
    "for i in range(3):\n",
    "    cluster_silhouette_values = sample_silhouette_values_k3[cluster_labels_k3 == i]\n",
    "    print(f\"Cluster {i}:\")\n",
    "    print(f\"  Size: {len(cluster_silhouette_values)}\")\n",
    "    print(f\"  Mean silhouette: {cluster_silhouette_values.mean():.3f}\")\n",
    "    print(f\"  Min silhouette: {cluster_silhouette_values.min():.3f}\")\n",
    "    print(f\"  Max silhouette: {cluster_silhouette_values.max():.3f}\")\n",
    "    \n",
    "    # Count negative silhouette scores (poorly assigned points)\n",
    "    negative_count = np.sum(cluster_silhouette_values < 0)\n",
    "    print(f\"  Negative scores: {negative_count}/{len(cluster_silhouette_values)} ({negative_count/len(cluster_silhouette_values)*100:.1f}%)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886d7db4",
   "metadata": {},
   "source": [
    "## 10. 3D Visualization of Clusters\n",
    "\n",
    "Let's create 3D scatter plots using three features to better visualize the cluster separability in higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4270110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Visualization using Plotly\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Create 3D matplotlib visualization\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Plot 1: Clusters in 3D space (first three features)\n",
    "ax1 = fig.add_subplot(221, projection='3d')\n",
    "colors_3d = ['red', 'blue', 'green']\n",
    "for cluster in range(3):\n",
    "    mask = cluster_labels_3 == cluster\n",
    "    ax1.scatter(X[mask, 0], X[mask, 1], X[mask, 2], \n",
    "               c=colors_3d[cluster], label=f'Cluster {cluster}', \n",
    "               alpha=0.7, s=60)\n",
    "\n",
    "# Plot cluster centers\n",
    "centers_orig = scaler.inverse_transform(cluster_centers_3)\n",
    "ax1.scatter(centers_orig[:, 0], centers_orig[:, 1], centers_orig[:, 2], \n",
    "           c='yellow', s=200, marker='*', edgecolors='black', \n",
    "           linewidths=2, label='Centroids')\n",
    "\n",
    "ax1.set_xlabel(feature_names[0])\n",
    "ax1.set_ylabel(feature_names[1])\n",
    "ax1.set_zlabel(feature_names[2])\n",
    "ax1.set_title('K-means Clusters (3D - First 3 Features)')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: True species in 3D space (first three features)\n",
    "ax2 = fig.add_subplot(222, projection='3d')\n",
    "for i, species in enumerate(target_names):\n",
    "    mask = y == i\n",
    "    ax2.scatter(X[mask, 0], X[mask, 1], X[mask, 2], \n",
    "               c=colors[i], label=species, alpha=0.7, s=60)\n",
    "\n",
    "ax2.set_xlabel(feature_names[0])\n",
    "ax2.set_ylabel(feature_names[1])\n",
    "ax2.set_zlabel(feature_names[2])\n",
    "ax2.set_title('True Species (3D - First 3 Features)')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Clusters using petal dimensions + sepal length\n",
    "ax3 = fig.add_subplot(223, projection='3d')\n",
    "for cluster in range(3):\n",
    "    mask = cluster_labels_3 == cluster\n",
    "    ax3.scatter(X[mask, 0], X[mask, 2], X[mask, 3], \n",
    "               c=colors_3d[cluster], label=f'Cluster {cluster}', \n",
    "               alpha=0.7, s=60)\n",
    "\n",
    "ax3.scatter(centers_orig[:, 0], centers_orig[:, 2], centers_orig[:, 3], \n",
    "           c='yellow', s=200, marker='*', edgecolors='black', \n",
    "           linewidths=2, label='Centroids')\n",
    "\n",
    "ax3.set_xlabel(feature_names[0])\n",
    "ax3.set_ylabel(feature_names[2])\n",
    "ax3.set_zlabel(feature_names[3])\n",
    "ax3.set_title('Clusters (Sepal Length + Petal Dimensions)')\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: PCA 3D visualization\n",
    "ax4 = fig.add_subplot(224, projection='3d')\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
    "\n",
    "for cluster in range(3):\n",
    "    mask = cluster_labels_3 == cluster\n",
    "    ax4.scatter(X_pca_3d[mask, 0], X_pca_3d[mask, 1], X_pca_3d[mask, 2], \n",
    "               c=colors_3d[cluster], label=f'Cluster {cluster}', \n",
    "               alpha=0.7, s=60)\n",
    "\n",
    "# Transform cluster centers to PCA space\n",
    "centers_pca_3d = pca_3d.transform(cluster_centers_3)\n",
    "ax4.scatter(centers_pca_3d[:, 0], centers_pca_3d[:, 1], centers_pca_3d[:, 2], \n",
    "           c='yellow', s=200, marker='*', edgecolors='black', \n",
    "           linewidths=2, label='Centroids')\n",
    "\n",
    "ax4.set_xlabel(f'PC1 ({pca_3d.explained_variance_ratio_[0]:.1%})')\n",
    "ax4.set_ylabel(f'PC2 ({pca_3d.explained_variance_ratio_[1]:.1%})')\n",
    "ax4.set_zlabel(f'PC3 ({pca_3d.explained_variance_ratio_[2]:.1%})')\n",
    "ax4.set_title('PCA 3D Projection of Clusters')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä PCA 3D Analysis:\")\n",
    "print(f\"   PC1 explains {pca_3d.explained_variance_ratio_[0]:.1%} of variance\")\n",
    "print(f\"   PC2 explains {pca_3d.explained_variance_ratio_[1]:.1%} of variance\")\n",
    "print(f\"   PC3 explains {pca_3d.explained_variance_ratio_[2]:.1%} of variance\")\n",
    "print(f\"   Total variance explained: {pca_3d.explained_variance_ratio_.sum():.1%}\")\n",
    "\n",
    "# Interactive Plotly 3D visualization\n",
    "print(\"\\nüéÆ Creating interactive 3D visualizations...\")\n",
    "\n",
    "# Create Plotly 3D scatter plot for clusters\n",
    "fig_3d_clusters = go.Figure()\n",
    "\n",
    "for cluster in range(3):\n",
    "    mask = cluster_labels_3 == cluster\n",
    "    fig_3d_clusters.add_trace(go.Scatter3d(\n",
    "        x=X[mask, 2], y=X[mask, 3], z=X[mask, 0],\n",
    "        mode='markers',\n",
    "        name=f'Cluster {cluster}',\n",
    "        marker=dict(\n",
    "            size=6,\n",
    "            color=['red', 'blue', 'green'][cluster],\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        text=[f'Sample {i}<br>Petal L: {X[i,2]:.1f}<br>Petal W: {X[i,3]:.1f}<br>Sepal L: {X[i,0]:.1f}'\n",
    "              for i in np.where(mask)[0]],\n",
    "        hovertemplate='%{text}<extra></extra>'\n",
    "    ))\n",
    "\n",
    "# Add cluster centers\n",
    "centers_orig = scaler.inverse_transform(cluster_centers_3)\n",
    "fig_3d_clusters.add_trace(go.Scatter3d(\n",
    "    x=centers_orig[:, 2], y=centers_orig[:, 3], z=centers_orig[:, 0],\n",
    "    mode='markers',\n",
    "    name='Centroids',\n",
    "    marker=dict(\n",
    "        size=15,\n",
    "        color='yellow',\n",
    "        symbol='diamond',\n",
    "        line=dict(color='black', width=2)\n",
    "    ),\n",
    "    text=[f'Centroid {i}<br>Petal L: {centers_orig[i,2]:.1f}<br>Petal W: {centers_orig[i,3]:.1f}<br>Sepal L: {centers_orig[i,0]:.1f}'\n",
    "          for i in range(3)],\n",
    "    hovertemplate='%{text}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig_3d_clusters.update_layout(\n",
    "    title='3D K-means Clustering Results (Petal + Sepal Length)',\n",
    "    scene=dict(\n",
    "        xaxis_title='Petal Length (cm)',\n",
    "        yaxis_title='Petal Width (cm)',\n",
    "        zaxis_title='Sepal Length (cm)',\n",
    "        camera=dict(eye=dict(x=1.5, y=1.5, z=1.5))\n",
    "    ),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig_3d_clusters.show()\n",
    "\n",
    "# Create Plotly 3D scatter plot for true species\n",
    "fig_3d_species = go.Figure()\n",
    "\n",
    "for i, species in enumerate(target_names):\n",
    "    mask = y == i\n",
    "    fig_3d_species.add_trace(go.Scatter3d(\n",
    "        x=X[mask, 2], y=X[mask, 3], z=X[mask, 0],\n",
    "        mode='markers',\n",
    "        name=species,\n",
    "        marker=dict(\n",
    "            size=6,\n",
    "            color=['red', 'green', 'blue'][i],\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        text=[f'{species}<br>Petal L: {X[j,2]:.1f}<br>Petal W: {X[j,3]:.1f}<br>Sepal L: {X[j,0]:.1f}'\n",
    "              for j in np.where(mask)[0]],\n",
    "        hovertemplate='%{text}<extra></extra>'\n",
    "    ))\n",
    "\n",
    "fig_3d_species.update_layout(\n",
    "    title='3D True Species Distribution (Petal + Sepal Length)',\n",
    "    scene=dict(\n",
    "        xaxis_title='Petal Length (cm)',\n",
    "        yaxis_title='Petal Width (cm)',\n",
    "        zaxis_title='Sepal Length (cm)',\n",
    "        camera=dict(eye=dict(x=1.5, y=1.5, z=1.5))\n",
    "    ),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig_3d_species.show()\n",
    "\n",
    "# Create PCA 3D interactive plot\n",
    "fig_pca_3d = go.Figure()\n",
    "\n",
    "for cluster in range(3):\n",
    "    mask = cluster_labels_3 == cluster\n",
    "    fig_pca_3d.add_trace(go.Scatter3d(\n",
    "        x=X_pca_3d[mask, 0], y=X_pca_3d[mask, 1], z=X_pca_3d[mask, 2],\n",
    "        mode='markers',\n",
    "        name=f'Cluster {cluster}',\n",
    "        marker=dict(\n",
    "            size=6,\n",
    "            color=['red', 'blue', 'green'][cluster],\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        text=[f'Sample {i}<br>PC1: {X_pca_3d[i,0]:.2f}<br>PC2: {X_pca_3d[i,1]:.2f}<br>PC3: {X_pca_3d[i,2]:.2f}'\n",
    "              for i in np.where(mask)[0]],\n",
    "        hovertemplate='%{text}<extra></extra>'\n",
    "    ))\n",
    "\n",
    "# Add PCA cluster centers\n",
    "fig_pca_3d.add_trace(go.Scatter3d(\n",
    "    x=centers_pca_3d[:, 0], y=centers_pca_3d[:, 1], z=centers_pca_3d[:, 2],\n",
    "    mode='markers',\n",
    "    name='Centroids',\n",
    "    marker=dict(\n",
    "        size=15,\n",
    "        color='yellow',\n",
    "        symbol='diamond',\n",
    "        line=dict(color='black', width=2)\n",
    "    ),\n",
    "    text=[f'Centroid {i}<br>PC1: {centers_pca_3d[i,0]:.2f}<br>PC2: {centers_pca_3d[i,1]:.2f}<br>PC3: {centers_pca_3d[i,2]:.2f}'\n",
    "          for i in range(3)],\n",
    "    hovertemplate='%{text}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig_pca_3d.update_layout(\n",
    "    title='3D PCA Projection of K-means Clusters',\n",
    "    scene=dict(\n",
    "        xaxis_title=f'PC1 ({pca_3d.explained_variance_ratio_[0]:.1%} variance)',\n",
    "        yaxis_title=f'PC2 ({pca_3d.explained_variance_ratio_[1]:.1%} variance)',\n",
    "        zaxis_title=f'PC3 ({pca_3d.explained_variance_ratio_[2]:.1%} variance)',\n",
    "        camera=dict(eye=dict(x=1.5, y=1.5, z=1.5))\n",
    "    ),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig_pca_3d.show()\n",
    "\n",
    "print(\"‚úÖ Interactive 3D visualizations created!\")\n",
    "print(\"üéÆ You can rotate, zoom, and hover over points for detailed information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11880565",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways and Industrial Applications\n",
    "\n",
    "### **K-means Clustering Insights:**\n",
    "\n",
    "#### **Algorithm Performance:**\n",
    "- **‚úÖ Successfully identified 3 natural clusters** corresponding closely to iris species\n",
    "- **üìä High silhouette score** indicating well-separated clusters\n",
    "- **üéØ Strong correlation with biological truth** (high ARI score)\n",
    "- **‚ö° Fast and efficient** for this dataset size and dimensionality\n",
    "\n",
    "#### **Key Learnings:**\n",
    "- **Feature Scaling is Critical**: Standardization significantly improved clustering quality\n",
    "- **Optimal k Selection**: Multiple metrics (elbow, silhouette) pointed to k=3\n",
    "- **Petal Features Most Discriminative**: Petal length/width provided best separation\n",
    "- **Natural Groupings Exist**: Data structure supports unsupervised discovery\n",
    "\n",
    "### **Industrial Applications:**\n",
    "\n",
    "#### **üè≠ Manufacturing & Quality Control:**\n",
    "- **Product Defect Classification**: Group defects by type without predefined categories\n",
    "- **Process State Monitoring**: Identify normal vs abnormal operating conditions\n",
    "- **Material Sorting**: Cluster materials by properties for automated sorting\n",
    "- **Batch Quality Assessment**: Group production batches by quality characteristics\n",
    "\n",
    "#### **üîß Maintenance & Equipment:**\n",
    "- **Equipment Health Monitoring**: Cluster machine states (healthy, degrading, failing)\n",
    "- **Predictive Maintenance**: Group similar failure patterns for proactive maintenance\n",
    "- **Sensor Data Analysis**: Identify operational patterns in multi-sensor data\n",
    "- **Asset Performance Grouping**: Cluster equipment by performance characteristics\n",
    "\n",
    "#### **‚ö° Process Optimization:**\n",
    "- **Operating Condition Discovery**: Find optimal parameter combinations\n",
    "- **Customer Segmentation**: Group customers by usage patterns\n",
    "- **Energy Consumption Patterns**: Identify consumption clusters for optimization\n",
    "- **Supply Chain Analysis**: Group suppliers or products by performance metrics\n",
    "\n",
    "#### **üìä Data-Driven Insights:**\n",
    "- **Pattern Recognition**: Discover hidden structures in operational data\n",
    "- **Anomaly Detection**: Identify outliers that don't fit any cluster\n",
    "- **Parameter Tuning**: Use clustering to find similar operating conditions\n",
    "- **Dimensionality Reduction**: Focus on most discriminative features\n",
    "\n",
    "### **Best Practices Learned:**\n",
    "\n",
    "1. **üîÑ Always scale your features** when variables have different units/ranges\n",
    "2. **üìà Use multiple validation metrics** - don't rely on just one measure\n",
    "3. **üéØ Consider domain knowledge** when interpreting clusters\n",
    "4. **üìä Visualize results** in multiple ways (2D, 3D, PCA projections)\n",
    "5. **üîç Validate cluster stability** by running multiple times with different initializations\n",
    "6. **‚ö° Start simple** - K-means works well for spherical, well-separated clusters\n",
    "\n",
    "### **When to Use K-means:**\n",
    "- ‚úÖ **Spherical clusters** expected\n",
    "- ‚úÖ **Similar cluster sizes** \n",
    "- ‚úÖ **Continuous features**\n",
    "- ‚úÖ **Fast processing** needed\n",
    "- ‚úÖ **Interpretable results** required\n",
    "\n",
    "### **When to Consider Alternatives:**\n",
    "- ‚ùå **Non-spherical clusters** (use DBSCAN, Gaussian Mixture)\n",
    "- ‚ùå **Very different cluster sizes** (use hierarchical clustering)\n",
    "- ‚ùå **Lots of noise/outliers** (use DBSCAN)\n",
    "- ‚ùå **Unknown number of clusters** (use hierarchical clustering)\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps:**\n",
    "üöÄ Try applying K-means to your own industrial datasets!\n",
    "üî¨ Experiment with different preprocessing techniques\n",
    "üìä Compare with other clustering algorithms\n",
    "üéØ Integrate clustering insights into decision-making processes\n",
    "\n",
    "**Remember: Unsupervised learning reveals hidden patterns in your data that can drive significant business value!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
