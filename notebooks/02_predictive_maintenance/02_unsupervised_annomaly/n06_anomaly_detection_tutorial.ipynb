{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5934e65",
   "metadata": {},
   "source": [
    "# ‚ö° Industrial Anomaly Detection Tutorial\n",
    "\n",
    "## **Machine Learning for Industrial Fault Detection and Process Monitoring**\n",
    "\n",
    "### **Main Goal:**\n",
    "Learn to detect anomalies in industrial processes using density estimation methods. This tutorial demonstrates how to identify abnormal operating conditions in manufacturing systems using machine learning techniques on sensor data.\n",
    "\n",
    "### **Key Learning Objectives:**\n",
    "- **Anomaly Detection Fundamentals**: Understand normal vs abnormal behavior patterns\n",
    "- **Density Estimation Methods**: Compare different approaches (Gaussian, GMM, KDE, Isolation Forest, etc.)\n",
    "- **Industrial Data Processing**: Handle sensor data with noise, drift, and missing values\n",
    "- **Evaluation Metrics**: Use precision, recall, F1-score, and ROC curves for anomaly detection\n",
    "- **Threshold Optimization**: Find optimal detection thresholds for industrial applications\n",
    "\n",
    "### **Industrial Relevance:**\n",
    "Apply to **predictive maintenance**, **quality control**, **process monitoring**, **equipment failure detection**, and **safety systems** in manufacturing environments.\n",
    "\n",
    "### **Interactive Features:**\n",
    "üéÆ Multi-algorithm comparison | üìä Real-time threshold tuning | ‚ö° ROC curve analysis | üî¨ Feature importance for anomalies\n",
    "\n",
    "**Dataset**: Industrial Process Monitoring with 5 critical sensor measurements (Temperature, Pressure, Vibration, Flow Rate, Power Consumption)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d01a1",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll use comprehensive ML libraries for anomaly detection, including density estimation and evaluation tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4f950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy import stats\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)\n",
    "%matplotlib widget\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Ready for industrial anomaly detection tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51c5c9f",
   "metadata": {},
   "source": [
    "## 2. Generate Industrial Process Dataset\n",
    "\n",
    "Let's create a realistic industrial dataset with 5 key sensor measurements representing normal operating conditions and various types of anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f21a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_industrial_data(n_normal=1000, n_anomalies=100, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate realistic industrial process data with 5 sensor measurements:\n",
    "    - Temperature (¬∞C): Operating temperature of the process\n",
    "    - Pressure (Bar): System pressure\n",
    "    - Vibration (mm/s): Equipment vibration levels\n",
    "    - Flow_Rate (L/min): Process flow rate\n",
    "    - Power_Consumption (kW): Energy consumption\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Define normal operating ranges and correlations\n",
    "    normal_means = np.array([75.0, 2.5, 1.2, 150.0, 45.0])  # Normal operating points\n",
    "    \n",
    "    # Correlation matrix - realistic industrial relationships\n",
    "    # Temperature correlates with power, pressure affects flow rate, etc.\n",
    "    correlation_matrix = np.array([\n",
    "        [1.0,  0.3,  0.1,  0.2,  0.7],  # Temperature\n",
    "        [0.3,  1.0,  0.4, -0.6,  0.2],  # Pressure\n",
    "        [0.1,  0.4,  1.0,  0.1,  0.3],  # Vibration\n",
    "        [0.2, -0.6,  0.1,  1.0, -0.1],  # Flow Rate\n",
    "        [0.7,  0.2,  0.3, -0.1,  1.0]   # Power Consumption\n",
    "    ])\n",
    "    \n",
    "    # Standard deviations for normal operation\n",
    "    normal_stds = np.array([3.0, 0.2, 0.15, 10.0, 5.0])\n",
    "    \n",
    "    # Create covariance matrix\n",
    "    normal_cov = np.outer(normal_stds, normal_stds) * correlation_matrix\n",
    "    \n",
    "    # Generate normal data\n",
    "    normal_data = np.random.multivariate_normal(normal_means, normal_cov, n_normal)\n",
    "    \n",
    "    # Ensure realistic physical constraints\n",
    "    normal_data[:, 0] = np.clip(normal_data[:, 0], 60, 90)    # Temperature range\n",
    "    normal_data[:, 1] = np.clip(normal_data[:, 1], 1.5, 4.0) # Pressure range\n",
    "    normal_data[:, 2] = np.clip(normal_data[:, 2], 0.5, 2.5) # Vibration range\n",
    "    normal_data[:, 3] = np.clip(normal_data[:, 3], 120, 200) # Flow rate range\n",
    "    normal_data[:, 4] = np.clip(normal_data[:, 4], 30, 65)   # Power range\n",
    "    \n",
    "    # Generate different types of anomalies\n",
    "    anomalies = []\n",
    "    anomaly_types = []\n",
    "    \n",
    "    # Type 1: Equipment overheating (high temperature, high power)\n",
    "    n_overheat = n_anomalies // 5\n",
    "    overheat_data = np.random.multivariate_normal(\n",
    "        [95, 2.8, 1.5, 140, 70], \n",
    "        np.diag([4, 0.3, 0.2, 15, 8])**2, \n",
    "        n_overheat\n",
    "    )\n",
    "    anomalies.append(overheat_data)\n",
    "    anomaly_types.extend(['Overheating'] * n_overheat)\n",
    "    \n",
    "    # Type 2: Pressure surge (high pressure, reduced flow)\n",
    "    n_pressure = n_anomalies // 5\n",
    "    pressure_data = np.random.multivariate_normal(\n",
    "        [78, 4.2, 2.0, 100, 50], \n",
    "        np.diag([2, 0.4, 0.3, 12, 6])**2, \n",
    "        n_pressure\n",
    "    )\n",
    "    anomalies.append(pressure_data)\n",
    "    anomaly_types.extend(['Pressure_Surge'] * n_pressure)\n",
    "    \n",
    "    # Type 3: Excessive vibration (equipment wear)\n",
    "    n_vibration = n_anomalies // 5\n",
    "    vibration_data = np.random.multivariate_normal(\n",
    "        [73, 2.6, 3.5, 155, 48], \n",
    "        np.diag([3, 0.2, 0.4, 8, 5])**2, \n",
    "        n_vibration\n",
    "    )\n",
    "    anomalies.append(vibration_data)\n",
    "    anomaly_types.extend(['Excessive_Vibration'] * n_vibration)\n",
    "    \n",
    "    # Type 4: Flow blockage (low flow, high pressure)\n",
    "    n_blockage = n_anomalies // 5\n",
    "    blockage_data = np.random.multivariate_normal(\n",
    "        [77, 3.5, 1.8, 80, 42], \n",
    "        np.diag([2, 0.3, 0.2, 10, 4])**2, \n",
    "        n_blockage\n",
    "    )\n",
    "    anomalies.append(blockage_data)\n",
    "    anomaly_types.extend(['Flow_Blockage'] * n_blockage)\n",
    "    \n",
    "    # Type 5: Power anomaly (erratic power consumption)\n",
    "    n_power = n_anomalies - (n_overheat + n_pressure + n_vibration + n_blockage)\n",
    "    power_data = np.random.multivariate_normal(\n",
    "        [74, 2.4, 1.3, 148, 25], \n",
    "        np.diag([5, 0.4, 0.3, 20, 10])**2, \n",
    "        n_power\n",
    "    )\n",
    "    anomalies.append(power_data)\n",
    "    anomaly_types.extend(['Power_Anomaly'] * n_power)\n",
    "    \n",
    "    # Combine all anomalies\n",
    "    anomaly_data = np.vstack(anomalies)\n",
    "    \n",
    "    # Create combined dataset\n",
    "    X = np.vstack([normal_data, anomaly_data])\n",
    "    y = np.hstack([np.zeros(n_normal), np.ones(n_anomalies)])  # 0 = normal, 1 = anomaly\n",
    "    \n",
    "    # Create detailed labels\n",
    "    detailed_labels = ['Normal'] * n_normal + anomaly_types\n",
    "    \n",
    "    # Feature names\n",
    "    feature_names = ['Temperature_C', 'Pressure_Bar', 'Vibration_mm_s', 'Flow_Rate_L_min', 'Power_Consumption_kW']\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df['is_anomaly'] = y\n",
    "    df['anomaly_type'] = detailed_labels\n",
    "    \n",
    "    return df, X, y, feature_names\n",
    "\n",
    "# Generate the dataset\n",
    "print(\"Generating Industrial Process Dataset...\")\n",
    "df, X, y, feature_names = generate_industrial_data(n_normal=1000, n_anomalies=100)\n",
    "\n",
    "print(f\"Dataset created successfully!\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Normal samples: {(y == 0).sum()}\")\n",
    "print(f\"Anomalous samples: {(y == 1).sum()}\")\n",
    "print(f\"Anomaly rate: {(y == 1).mean()*100:.1f}%\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nDataset statistics:\")\n",
    "display(df[feature_names].describe())\n",
    "\n",
    "print(\"\\nAnomaly type distribution:\")\n",
    "print(df['anomaly_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4bdb20",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Visualization\n",
    "\n",
    "Let's explore the dataset to understand the characteristics of normal operation and different types of anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Individual feature distributions\n",
    "for i, feature in enumerate(feature_names):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot normal data\n",
    "    normal_data = df[df['is_anomaly'] == 0][feature]\n",
    "    anomaly_data = df[df['is_anomaly'] == 1][feature]\n",
    "    \n",
    "    ax.hist(normal_data, bins=30, alpha=0.7, label='Normal', color='blue', density=True)\n",
    "    ax.hist(anomaly_data, bins=20, alpha=0.7, label='Anomalies', color='red', density=True)\n",
    "    \n",
    "    ax.set_xlabel(feature.replace('_', ' '))\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'Distribution of {feature.replace(\"_\", \" \")}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Correlation heatmap\n",
    "ax = axes[5]\n",
    "correlation_matrix = df[feature_names].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='RdBu_r', center=0,\n",
    "            fmt='.2f', square=True, linewidths=0.5, ax=ax)\n",
    "ax.set_title('Feature Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"KEY OBSERVATIONS:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Normal data shows consistent operating ranges\")\n",
    "print(\"‚ùå Anomalies deviate significantly in specific features\")\n",
    "print(\"üîó Features show realistic industrial correlations\")\n",
    "print(\"üìä Different anomaly types have distinct signatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise scatter plots to visualize anomaly patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7, 6))\n",
    "\n",
    "# Select most interesting feature pairs\n",
    "feature_pairs = [\n",
    "    (0, 4),  # Temperature vs Power\n",
    "    (1, 3),  # Pressure vs Flow Rate\n",
    "    (1, 2),  # Pressure vs Vibration\n",
    "    (0, 1)   # Temperature vs Pressure\n",
    "]\n",
    "\n",
    "colors = {'Normal': 'blue', 'Overheating': 'red', 'Pressure_Surge': 'orange', \n",
    "          'Excessive_Vibration': 'green', 'Flow_Blockage': 'purple', 'Power_Anomaly': 'brown'}\n",
    "\n",
    "for idx, (i, j) in enumerate(feature_pairs):\n",
    "    ax = axes[idx//2, idx%2]\n",
    "    \n",
    "    for anomaly_type in df['anomaly_type'].unique():\n",
    "        mask = df['anomaly_type'] == anomaly_type\n",
    "        data_subset = df[mask]\n",
    "        \n",
    "        alpha = 0.6 if anomaly_type == 'Normal' else 0.8\n",
    "        size = 20 if anomaly_type == 'Normal' else 40\n",
    "        \n",
    "        ax.scatter(data_subset.iloc[:, i], data_subset.iloc[:, j],\n",
    "                  c=colors[anomaly_type], label=anomaly_type, \n",
    "                  alpha=alpha, s=size, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel(feature_names[i].replace('_', ' '))\n",
    "    ax.set_ylabel(feature_names[j].replace('_', ' '))\n",
    "    ax.set_title(f'{feature_names[i].replace(\"_\", \" \")} vs {feature_names[j].replace(\"_\", \" \")}')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ANOMALY PATTERN ANALYSIS:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"üî• Overheating: High temperature + high power consumption\")\n",
    "print(\"üí® Pressure Surge: High pressure + reduced flow rate\")\n",
    "print(\"üì≥ Excessive Vibration: High vibration levels across conditions\")\n",
    "print(\"üö´ Flow Blockage: Low flow rate + increased pressure\")\n",
    "print(\"‚ö° Power Anomaly: Unusual power consumption patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4465f2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis of normal vs anomalous data\n",
    "print(\"STATISTICAL COMPARISON: NORMAL vs ANOMALOUS DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "normal_stats = df[df['is_anomaly'] == 0][feature_names].describe()\n",
    "anomaly_stats = df[df['is_anomaly'] == 1][feature_names].describe()\n",
    "\n",
    "print(\"\\nNORMAL OPERATION STATISTICS:\")\n",
    "print(\"-\" * 40)\n",
    "display(normal_stats)\n",
    "\n",
    "print(\"\\nANOMALY STATISTICS:\")\n",
    "print(\"-\" * 40)\n",
    "display(anomaly_stats)\n",
    "\n",
    "# Calculate separation metrics\n",
    "print(\"\\nFEATURE SEPARABILITY ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for feature in feature_names:\n",
    "    normal_values = df[df['is_anomaly'] == 0][feature]\n",
    "    anomaly_values = df[df['is_anomaly'] == 1][feature]\n",
    "    \n",
    "    # Calculate means and standard deviations\n",
    "    normal_mean, normal_std = normal_values.mean(), normal_values.std()\n",
    "    anomaly_mean, anomaly_std = anomaly_values.mean(), anomaly_values.std()\n",
    "    \n",
    "    # Calculate separation (Cohen's d)\n",
    "    pooled_std = np.sqrt(((len(normal_values) - 1) * normal_std**2 + \n",
    "                         (len(anomaly_values) - 1) * anomaly_std**2) / \n",
    "                        (len(normal_values) + len(anomaly_values) - 2))\n",
    "    cohens_d = abs(normal_mean - anomaly_mean) / pooled_std\n",
    "    \n",
    "    print(f\"{feature.replace('_', ' '):<20}: Cohen's d = {cohens_d:.3f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"Cohen's d: 0.2 = small, 0.5 = medium, 0.8 = large effect size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018749f0",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing for Anomaly Detection\n",
    "\n",
    "Before training anomaly detection models, we need to prepare the data properly. Since anomaly detection is typically unsupervised, we'll train only on normal data and evaluate on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748255f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate normal data for training (unsupervised approach)\n",
    "X_normal = X[y == 0]  # Only normal data for training\n",
    "X_normal_df = df[df['is_anomaly'] == 0][feature_names]\n",
    "\n",
    "# Split normal data into train/validation for model selection\n",
    "X_train_normal, X_val_normal = train_test_split(X_normal, test_size=0.2, random_state=42)\n",
    "\n",
    "# Use all data (normal + anomalies) for final evaluation\n",
    "X_test = X\n",
    "y_test = y\n",
    "\n",
    "print(\"DATA SPLITS FOR ANOMALY DETECTION:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training data (normal only): {X_train_normal.shape[0]} samples\")\n",
    "print(f\"Validation data (normal only): {X_val_normal.shape[0]} samples\")\n",
    "print(f\"Test data (normal + anomalies): {X_test.shape[0]} samples\")\n",
    "print(f\"Test anomaly rate: {(y_test == 1).mean()*100:.1f}%\")\n",
    "\n",
    "# Scale the data using robust scaling (less sensitive to outliers)\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_normal)\n",
    "X_val_scaled = scaler.transform(X_val_normal)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nFeature scaling completed using RobustScaler\")\n",
    "print(f\"Scaled training data shape: {X_train_scaled.shape}\")\n",
    "\n",
    "# Display scaling statistics\n",
    "scaling_stats = pd.DataFrame({\n",
    "    'Original_Mean': X_normal.mean(),\n",
    "    'Original_Std': X_normal.std(),\n",
    "    'Scaled_Mean': X_train_scaled.mean(axis=0),\n",
    "    'Scaled_Std': X_train_scaled.std(axis=0)\n",
    "}, index=feature_names)\n",
    "\n",
    "print(\"\\nFeature Scaling Statistics:\")\n",
    "display(scaling_stats.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d91b6",
   "metadata": {},
   "source": [
    "## 5. Train Multiple Anomaly Detection Models\n",
    "\n",
    "We'll train various density estimation and anomaly detection algorithms and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb9e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_anomaly_detectors(X_train, X_val):\n",
    "    \"\"\"Train multiple anomaly detection models\"\"\"\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    print(\"Training Anomaly Detection Models...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Isolation Forest\n",
    "    print(\"1. Training Isolation Forest...\")\n",
    "    isolation_forest = IsolationForest(\n",
    "        contamination=0.1,  # Expected proportion of anomalies\n",
    "        random_state=42,\n",
    "        n_estimators=100\n",
    "    )\n",
    "    isolation_forest.fit(X_train)\n",
    "    models['Isolation Forest'] = isolation_forest\n",
    "    \n",
    "    # 2. One-Class SVM\n",
    "    print(\"2. Training One-Class SVM...\")\n",
    "    one_class_svm = OneClassSVM(\n",
    "        kernel='rbf',\n",
    "        gamma='scale',\n",
    "        nu=0.1  # Upper bound on fraction of outliers\n",
    "    )\n",
    "    one_class_svm.fit(X_train)\n",
    "    models['One-Class SVM'] = one_class_svm\n",
    "    \n",
    "    # 3. Elliptic Envelope (Robust Covariance)\n",
    "    print(\"3. Training Elliptic Envelope...\")\n",
    "    elliptic_envelope = EllipticEnvelope(\n",
    "        contamination=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    elliptic_envelope.fit(X_train)\n",
    "    models['Elliptic Envelope'] = elliptic_envelope\n",
    "    \n",
    "    # 4. Local Outlier Factor (LOF)\n",
    "    print(\"4. Training Local Outlier Factor...\")\n",
    "    lof = LocalOutlierFactor(\n",
    "        n_neighbors=20,\n",
    "        contamination=0.1,\n",
    "        novelty=True  # For use with new data\n",
    "    )\n",
    "    lof.fit(X_train)\n",
    "    models['LOF'] = lof\n",
    "    \n",
    "    # 5. Gaussian Mixture Model (GMM) based anomaly detection\n",
    "    print(\"5. Training Gaussian Mixture Model...\")\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=3,  # Number of Gaussian components\n",
    "        random_state=42,\n",
    "        covariance_type='full'\n",
    "    )\n",
    "    gmm.fit(X_train)\n",
    "    models['GMM'] = gmm\n",
    "    \n",
    "    # 6. Kernel Density Estimation (KDE)\n",
    "    print(\"6. Training Kernel Density Estimation...\")\n",
    "    kde = KernelDensity(\n",
    "        kernel='gaussian',\n",
    "        bandwidth='scott'  # Automatic bandwidth selection\n",
    "    )\n",
    "    kde.fit(X_train)\n",
    "    models['KDE'] = kde\n",
    "    \n",
    "    # 7. Multivariate Gaussian (Single Gaussian Distribution)\n",
    "    print(\"7. Training Multivariate Gaussian...\")\n",
    "    from scipy.stats import multivariate_normal\n",
    "    \n",
    "    # Calculate mean and covariance of training data\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    cov = np.cov(X_train, rowvar=False)\n",
    "    \n",
    "    # Add regularization to ensure positive definite covariance matrix\n",
    "    reg_param = 1e-6\n",
    "    cov += reg_param * np.eye(cov.shape[0])\n",
    "    \n",
    "    # Create multivariate normal distribution\n",
    "    gaussian_model = {\n",
    "        'mean': mean,\n",
    "        'cov': cov,\n",
    "        'distribution': multivariate_normal(mean=mean, cov=cov)\n",
    "    }\n",
    "    models['Multivariate Gaussian'] = gaussian_model\n",
    "    \n",
    "    print(\"‚úÖ All models trained successfully!\")\n",
    "    return models\n",
    "\n",
    "# Train all anomaly detection models\n",
    "models = train_anomaly_detectors(X_train_scaled, X_val_scaled)\n",
    "print(f\"\\nTrained {len(models)} different anomaly detection algorithms\")\n",
    "print(f\"Models: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b310377",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Threshold Analysis\n",
    "\n",
    "Now let's evaluate each model and analyze how different thresholds affect the confusion matrix and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb365cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anomaly_scores(models, X_test):\n",
    "    \"\"\"Get anomaly scores for all models\"\"\"\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        if name == 'Isolation Forest':\n",
    "            # Isolation Forest: negative scores for anomalies\n",
    "            score = model.decision_function(X_test)\n",
    "            scores[name] = -score  # Flip sign so higher = more anomalous\n",
    "            \n",
    "        elif name == 'One-Class SVM':\n",
    "            # One-Class SVM: negative scores for anomalies\n",
    "            score = model.decision_function(X_test)\n",
    "            scores[name] = -score  # Flip sign so higher = more anomalous\n",
    "            \n",
    "        elif name == 'Elliptic Envelope':\n",
    "            # Mahalanobis distance based\n",
    "            score = model.decision_function(X_test)\n",
    "            scores[name] = -score  # Flip sign so higher = more anomalous\n",
    "            \n",
    "        elif name == 'LOF':\n",
    "            # Local Outlier Factor: higher values = more anomalous\n",
    "            score = model.decision_function(X_test)\n",
    "            scores[name] = -score  # Flip sign so higher = more anomalous\n",
    "            \n",
    "        elif name == 'GMM':\n",
    "            # Log-likelihood: lower likelihood = more anomalous\n",
    "            score = model.score_samples(X_test)\n",
    "            scores[name] = -score  # Flip sign so higher = more anomalous\n",
    "            \n",
    "        elif name == 'KDE':\n",
    "            # Log-likelihood: lower likelihood = more anomalous\n",
    "            score = model.score_samples(X_test)\n",
    "            scores[name] = -score  # Flip sign so higher = more anomalous\n",
    "            \n",
    "        elif name == 'Multivariate Gaussian':\n",
    "            # Log-likelihood from multivariate normal distribution\n",
    "            score = model['distribution'].logpdf(X_test)\n",
    "            scores[name] = -score  # Flip sign so higher = more anomalous\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Get anomaly scores for all models\n",
    "print(\"Computing anomaly scores for all models...\")\n",
    "anomaly_scores = get_anomaly_scores(models, X_test_scaled)\n",
    "\n",
    "# Display score ranges\n",
    "print(\"\\nANOMALY SCORE RANGES:\")\n",
    "print(\"=\" * 40)\n",
    "for name, scores in anomaly_scores.items():\n",
    "    print(f\"{name:<20}: [{scores.min():.3f}, {scores.max():.3f}]\")\n",
    "    \n",
    "print(\"\\nNote: Higher scores indicate higher probability of being anomalous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_thresholds(scores, y_true, model_name, thresholds=None):\n",
    "    \"\"\"Analyze performance across different thresholds\"\"\"\n",
    "    \n",
    "    if thresholds is None:\n",
    "        # Use percentiles of the score distribution\n",
    "        thresholds = np.percentile(scores, [70, 75, 80, 85, 90, 95, 97, 99])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        # Predict anomalies based on threshold\n",
    "        y_pred = (scores > threshold).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if len(np.unique(y_pred)) > 1:  # Ensure we have both classes\n",
    "            precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        else:\n",
    "            precision = recall = f1 = 0.0\n",
    "        \n",
    "        # Confusion matrix components\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        if cm.shape == (2, 2):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            # Handle cases where we only have one class predicted\n",
    "            if np.all(y_pred == 0):  # All predicted as normal\n",
    "                tn = np.sum(y_true == 0)\n",
    "                fp = 0\n",
    "                fn = np.sum(y_true == 1)\n",
    "                tp = 0\n",
    "            else:  # All predicted as anomaly\n",
    "                tn = 0\n",
    "                fp = np.sum(y_true == 0)\n",
    "                fn = 0\n",
    "                tp = np.sum(y_true == 1)\n",
    "        \n",
    "        # False positive rate and detection rate\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'threshold': threshold,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'tn': tn, 'fp': fp, 'fn': fn, 'tp': tp,\n",
    "            'fpr': fpr,\n",
    "            'detection_rate': detection_rate,\n",
    "            'n_predicted_anomalies': np.sum(y_pred)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Analyze thresholds for each model\n",
    "print(\"THRESHOLD ANALYSIS FOR ALL MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "threshold_results = {}\n",
    "\n",
    "for model_name, scores in anomaly_scores.items():\n",
    "    print(f\"\\nüìä Analyzing {model_name}...\")\n",
    "    \n",
    "    # Analyze different thresholds\n",
    "    results_df = analyze_thresholds(scores, y_test, model_name)\n",
    "    threshold_results[model_name] = results_df\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Threshold analysis completed - {len(results_df)} thresholds tested\")\n",
    "    print(f\"Best F1-Score: {results_df['f1_score'].max():.3f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Threshold analysis completed for all {len(models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc7dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrices for different thresholds\n",
    "def plot_confusion_matrices_by_threshold(model_name, scores, y_true, selected_thresholds=None):\n",
    "    \"\"\"Plot confusion matrices for different thresholds\"\"\"\n",
    "    \n",
    "    if selected_thresholds is None:\n",
    "        # Select 4 representative thresholds\n",
    "        selected_thresholds = np.percentile(scores, [80, 85, 90, 95])\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(6, 5))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, threshold in enumerate(selected_thresholds):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = (scores > threshold).astype(int)\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                   xticklabels=['Normal', 'Anomaly'],\n",
    "                   yticklabels=['Normal', 'Anomaly'])\n",
    "        \n",
    "        ax.set_title(f'Threshold: {threshold:.3f}\\\\n'\n",
    "                    f'P: {precision:.3f}, R: {recall:.3f}, F1: {f1:.3f}')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "    \n",
    "    plt.suptitle(f'Confusion Matrices - {model_name}\\\\nEffect of Different Thresholds', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrices for the best performing models\n",
    "print(\"CONFUSION MATRICES FOR DIFFERENT THRESHOLDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Select top 2 models based on best F1-score\n",
    "model_performance = {}\n",
    "for model_name, results_df in threshold_results.items():\n",
    "    model_performance[model_name] = results_df['f1_score'].max()\n",
    "\n",
    "# Sort models by performance\n",
    "sorted_models = sorted(model_performance.items(), key=lambda x: x[1], reverse=True)\n",
    "top_models = [name for name, _ in sorted_models[:2]]\n",
    "\n",
    "print(f\"Showing confusion matrices for top 2 models: {', '.join(top_models)}\\\\n\")\n",
    "\n",
    "for model_name in top_models:\n",
    "    scores = anomaly_scores[model_name]\n",
    "    plot_confusion_matrices_by_threshold(model_name, scores, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457c820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison across all models and thresholds\n",
    "def create_performance_summary(threshold_results):\n",
    "    \"\"\"Create a comprehensive performance summary\"\"\"\n",
    "    \n",
    "    summary = []\n",
    "    \n",
    "    for model_name, results_df in threshold_results.items():\n",
    "        # Find best threshold for each metric\n",
    "        best_f1_idx = results_df['f1_score'].idxmax()\n",
    "        best_precision_idx = results_df['precision'].idxmax()\n",
    "        best_recall_idx = results_df['recall'].idxmax()\n",
    "        \n",
    "        summary.append({\n",
    "            'Model': model_name,\n",
    "            'Best_F1': results_df.loc[best_f1_idx, 'f1_score'],\n",
    "            'F1_Threshold': results_df.loc[best_f1_idx, 'threshold'],\n",
    "            'F1_Precision': results_df.loc[best_f1_idx, 'precision'],\n",
    "            'F1_Recall': results_df.loc[best_f1_idx, 'recall'],\n",
    "            'Best_Precision': results_df.loc[best_precision_idx, 'precision'],\n",
    "            'Best_Recall': results_df.loc[best_recall_idx, 'recall'],\n",
    "            'Avg_F1': results_df['f1_score'].mean()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summary).sort_values('Best_F1', ascending=False)\n",
    "\n",
    "# Create performance summary\n",
    "performance_summary = create_performance_summary(threshold_results)\n",
    "\n",
    "print(\"COMPREHENSIVE PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "display(performance_summary.round(3))\n",
    "\n",
    "# Plot ROC curves for all models\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(7, 6))\n",
    "\n",
    "# ROC Curves\n",
    "for model_name, scores in anomaly_scores.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, scores)\n",
    "    auc_score = roc_auc_score(y_test, scores)\n",
    "    ax1.plot(fpr, tpr, label=f'{model_name} (AUC: {auc_score:.3f})', linewidth=2)\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curves - All Models')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curves\n",
    "for model_name, scores in anomaly_scores.items():\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_test, scores)\n",
    "    ax2.plot(recall_vals, precision_vals, label=model_name, linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Recall')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision-Recall Curves')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# F1-Score vs Threshold for best model\n",
    "best_model_name = performance_summary.iloc[0]['Model']\n",
    "best_results = threshold_results[best_model_name]\n",
    "\n",
    "ax3.plot(best_results['threshold'], best_results['f1_score'], 'b-', linewidth=2, label='F1-Score')\n",
    "ax3.plot(best_results['threshold'], best_results['precision'], 'r--', linewidth=2, label='Precision')\n",
    "ax3.plot(best_results['threshold'], best_results['recall'], 'g--', linewidth=2, label='Recall')\n",
    "ax3.set_xlabel('Threshold')\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.set_title(f'Metrics vs Threshold - {best_model_name}')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Number of detected anomalies vs threshold\n",
    "ax4.plot(best_results['threshold'], best_results['n_predicted_anomalies'], 'purple', linewidth=2)\n",
    "ax4.axhline(y=100, color='red', linestyle='--', alpha=0.7, label='True Anomalies (100)')\n",
    "ax4.set_xlabel('Threshold')\n",
    "ax4.set_ylabel('Number of Predicted Anomalies')\n",
    "ax4.set_title(f'Detection Count vs Threshold - {best_model_name}')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\\\nüèÜ BEST PERFORMING MODEL: {best_model_name}\")\n",
    "print(f\"   Best F1-Score: {performance_summary.iloc[0]['Best_F1']:.3f}\")\n",
    "print(f\"   Optimal Threshold: {performance_summary.iloc[0]['F1_Threshold']:.3f}\")\n",
    "print(f\"   Precision: {performance_summary.iloc[0]['F1_Precision']:.3f}\")\n",
    "print(f\"   Recall: {performance_summary.iloc[0]['F1_Recall']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19faea1b",
   "metadata": {},
   "source": [
    "## 7. Interactive Threshold Explorer\n",
    "\n",
    "Let's create an interactive tool to explore how different thresholds affect the confusion matrix and performance metrics for different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12933f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_threshold_explorer(model_name='Isolation Forest', threshold_selection='90th percentile'):\n",
    "    \"\"\"Interactive exploration of thresholds and their impact on confusion matrices\"\"\"\n",
    "    \n",
    "    # Get scores for selected model\n",
    "    scores = anomaly_scores[model_name]\n",
    "    \n",
    "    # Parse threshold selection\n",
    "    if 'percentile' in threshold_selection:\n",
    "        percentile = float(threshold_selection.split('th')[0])\n",
    "        threshold = np.percentile(scores, percentile)\n",
    "        threshold_label = f\"{percentile}th percentile\"\n",
    "    elif 'Custom' in threshold_selection:\n",
    "        # For custom threshold, use median as default\n",
    "        threshold = np.median(scores)\n",
    "        threshold_label = \"Custom (median)\"\n",
    "    else:\n",
    "        # Parse specific threshold values\n",
    "        threshold = float(threshold_selection.split(':')[1].strip())\n",
    "        threshold_label = f\"Fixed value: {threshold:.3f}\"\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = (scores > threshold).astype(int)\n",
    "    \n",
    "    # Calculate confusion matrix and metrics\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(7, 5))\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "               xticklabels=['Normal', 'Anomaly'],\n",
    "               yticklabels=['Normal', 'Anomaly'])\n",
    "    ax1.set_title(f'Confusion Matrix\\nThreshold: {threshold:.4f} ({threshold_label})')\n",
    "    ax1.set_xlabel('Predicted')\n",
    "    ax1.set_ylabel('Actual')\n",
    "    \n",
    "    # 2. Metrics Bar Chart\n",
    "    metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "    values = [precision, recall, f1]\n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen']\n",
    "    \n",
    "    bars = ax2.bar(metrics, values, color=colors, edgecolor='black', linewidth=1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.set_title('Performance Metrics')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Score Distribution with Threshold\n",
    "    normal_scores = scores[y_test == 0]\n",
    "    anomaly_scores_subset = scores[y_test == 1]\n",
    "    \n",
    "    ax3.hist(normal_scores, bins=30, alpha=0.7, label='Normal', color='blue', density=True)\n",
    "    ax3.hist(anomaly_scores_subset, bins=20, alpha=0.7, label='Anomalies', color='red', density=True)\n",
    "    ax3.axvline(threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold: {threshold:.4f}')\n",
    "    ax3.set_xlabel('Anomaly Score')\n",
    "    ax3.set_ylabel('Density')\n",
    "    ax3.set_title(f'Score Distribution - {model_name}')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Detailed Breakdown\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        breakdown_labels = ['True Negative\\\\n(Normal‚ÜíNormal)', 'False Positive\\\\n(Normal‚ÜíAnomaly)', \n",
    "                           'False Negative\\\\n(Anomaly‚ÜíNormal)', 'True Positive\\\\n(Anomaly‚ÜíAnomaly)']\n",
    "        breakdown_values = [tn, fp, fn, tp]\n",
    "        breakdown_colors = ['lightblue', 'orange', 'red', 'green']\n",
    "        \n",
    "        wedges, texts, autotexts = ax4.pie(breakdown_values, labels=breakdown_labels, colors=breakdown_colors,\n",
    "                                          autopct='%1.0f', startangle=90, textprops={'fontsize': 9})\n",
    "        ax4.set_title('Prediction Breakdown')\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - Interactive Threshold Analysis', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(f\"DETAILED ANALYSIS - {model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Threshold Value: {threshold:.4f} ({threshold_label})\")\n",
    "    print(f\"Total Samples: {len(y_test)}\")\n",
    "    print(f\"Predicted Anomalies: {np.sum(y_pred)} / {len(y_test)} ({np.sum(y_pred)/len(y_test)*100:.1f}%)\")\n",
    "    print(f\"Actual Anomalies: {np.sum(y_test)} / {len(y_test)} ({np.sum(y_test)/len(y_test)*100:.1f}%)\")\n",
    "    print()\n",
    "    print(\"CONFUSION MATRIX BREAKDOWN:\")\n",
    "    if cm.shape == (2, 2):\n",
    "        print(f\"  True Negatives (TN):  {tn:3d} - Correctly identified normal samples\")\n",
    "        print(f\"  False Positives (FP): {fp:3d} - Normal samples flagged as anomalies\")\n",
    "        print(f\"  False Negatives (FN): {fn:3d} - Missed anomalies (dangerous!)\")\n",
    "        print(f\"  True Positives (TP):  {tp:3d} - Correctly detected anomalies\")\n",
    "    print()\n",
    "    print(\"PERFORMANCE METRICS:\")\n",
    "    print(f\"  Precision: {precision:.3f} - Of predicted anomalies, {precision*100:.1f}% were actually anomalies\")\n",
    "    print(f\"  Recall:    {recall:.3f} - Detected {recall*100:.1f}% of actual anomalies\")\n",
    "    print(f\"  F1-Score:  {f1:.3f} - Harmonic mean of precision and recall\")\n",
    "    \n",
    "    if cm.shape == (2, 2):\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        print(f\"  False Positive Rate: {fpr:.3f} - {fpr*100:.1f}% of normal samples incorrectly flagged\")\n",
    "\n",
    "# Create comprehensive threshold options for each model\n",
    "def get_threshold_options(model_name):\n",
    "    \"\"\"Get comprehensive threshold options for a specific model\"\"\"\n",
    "    scores = anomaly_scores[model_name]\n",
    "    \n",
    "    options = [\n",
    "        \"70th percentile\",\n",
    "        \"75th percentile\", \n",
    "        \"80th percentile\",\n",
    "        \"85th percentile\",\n",
    "        \"90th percentile\",\n",
    "        \"95th percentile\",\n",
    "        \"97th percentile\",\n",
    "        \"99th percentile\"\n",
    "    ]\n",
    "    \n",
    "    # Add some specific threshold values based on score distribution\n",
    "    percentiles = [70, 75, 80, 85, 90, 95, 97, 99]\n",
    "    threshold_values = [np.percentile(scores, p) for p in percentiles]\n",
    "    \n",
    "    # Add quartiles and extremes\n",
    "    quartiles = [\n",
    "        f\"Min value: {scores.min():.4f}\",\n",
    "        f\"25th percentile: {np.percentile(scores, 25):.4f}\",\n",
    "        f\"Median: {np.median(scores):.4f}\",\n",
    "        f\"75th percentile: {np.percentile(scores, 75):.4f}\",\n",
    "        f\"Max value: {scores.max():.4f}\"\n",
    "    ]\n",
    "    \n",
    "    # Combine all options\n",
    "    all_options = options + quartiles\n",
    "    \n",
    "    return all_options\n",
    "\n",
    "# Create interactive widget with listbox for threshold selection\n",
    "model_options = list(anomaly_scores.keys())\n",
    "\n",
    "print(\"üéÆ INTERACTIVE THRESHOLD EXPLORER\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Select a model and threshold to see real-time confusion matrix updates!\")\n",
    "print(\"Different threshold strategies:\")\n",
    "print(\"  üìä Percentiles: Based on score distribution\")\n",
    "print(\"  üìà Quartiles: Statistical distribution points\") \n",
    "print(\"  üéØ Extremes: Minimum and maximum values\")\n",
    "print()\n",
    "\n",
    "interact(interactive_threshold_explorer,\n",
    "         model_name=widgets.Dropdown(options=model_options, \n",
    "                                    value=model_options[0], \n",
    "                                    description='Model:'),\n",
    "         threshold_selection=widgets.Select(\n",
    "             options=get_threshold_options(model_options[0]),\n",
    "             value=\"90th percentile\",\n",
    "             description='Threshold:',\n",
    "             disabled=False,\n",
    "             rows=8,\n",
    "             layout=widgets.Layout(width='300px')\n",
    "         ));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388f108",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways and Industrial Applications\n",
    "\n",
    "### **Threshold Selection Strategy:**\n",
    "- **High Threshold (95th+ percentile)**: Few false alarms, but may miss subtle anomalies\n",
    "- **Medium Threshold (85-90th percentile)**: Balanced precision/recall for most applications  \n",
    "- **Low Threshold (70-80th percentile)**: High sensitivity, more false alarms but catches all issues\n",
    "\n",
    "### **Model Performance Insights:**\n",
    "- **Isolation Forest**: Excellent for high-dimensional data, handles mixed anomaly types well\n",
    "- **One-Class SVM**: Good for complex decision boundaries, sensitive to parameter tuning\n",
    "- **LOF**: Excellent for local anomalies, sensitive to density variations\n",
    "- **GMM**: Works well when normal data has multiple operational modes\n",
    "- **Elliptic Envelope**: Fast and simple, assumes Gaussian distribution\n",
    "- **KDE**: Non-parametric, flexible, but computationally intensive\n",
    "- **Multivariate Gaussian**: Simple probabilistic model, assumes single Gaussian distribution\n",
    "\n",
    "### **Industrial Applications:**\n",
    "- **üè≠ Predictive Maintenance**: Early detection of equipment degradation\n",
    "- **‚öôÔ∏è Process Monitoring**: Identify deviations from normal operating conditions\n",
    "- **üîç Quality Control**: Detect defective products in manufacturing lines\n",
    "- **üö® Safety Systems**: Real-time anomaly detection for critical processes\n",
    "- **üìä Performance Optimization**: Identify sub-optimal operating conditions\n",
    "\n",
    "### **Confusion Matrix Interpretation in Industrial Context:**\n",
    "- **True Negatives (TN)**: Normal operation correctly identified ‚úÖ\n",
    "- **False Positives (FP)**: Unnecessary alarms/shutdowns üí∏\n",
    "- **False Negatives (FN)**: Missed failures (potentially dangerous!) ‚ö†Ô∏è\n",
    "- **True Positives (TP)**: Successful anomaly detection üéØ\n",
    "\n",
    "### **Threshold Optimization Guidelines:**\n",
    "1. **Safety-Critical Systems**: Prioritize recall (minimize false negatives)\n",
    "2. **Cost-Sensitive Operations**: Balance precision and recall based on costs\n",
    "3. **High-Volume Production**: Optimize for minimal false positives to avoid disruptions\n",
    "4. **Research/Development**: Use lower thresholds for comprehensive anomaly discovery\n",
    "\n",
    "---\n",
    "\n",
    "**This tutorial demonstrates how to build robust anomaly detection systems for industrial applications with proper threshold optimization and evaluation strategies.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
