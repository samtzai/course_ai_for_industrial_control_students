{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c88f1831",
   "metadata": {},
   "source": [
    "# 🎯 Overfitting and Underfitting Demonstration\n",
    "\n",
    "## **Neural Networks Learning to Approximate the Sine Function**\n",
    "\n",
    "### **Main Goal:**\n",
    "Learn to identify, understand, and prevent overfitting and underfitting in neural networks by training models to approximate the mathematical function y = sin(x).\n",
    "\n",
    "### **Key Learning Objectives:**\n",
    "- **Understand Model Complexity**: Learn how model complexity affects learning capacity\n",
    "- **Identify Overfitting**: Recognize when models memorize training data but fail to generalize\n",
    "- **Identify Underfitting**: Recognize when models are too simple to capture underlying patterns\n",
    "- **Regularization Techniques**: Apply dropout, L1/L2 regularization, early stopping, and batch normalization\n",
    "- **Performance Evaluation**: Use validation curves and visualization to assess model quality\n",
    "\n",
    "### **Why the Sine Function?**\n",
    "The sine function is perfect for this demonstration because:\n",
    "- **Known Ground Truth**: We know the exact mathematical relationship\n",
    "- **Non-Linear**: Requires neural networks to learn complex patterns\n",
    "- **Smooth Function**: Easy to visualize and understand deviations\n",
    "- **Controllable Noise**: We can add controlled amounts of noise to simulate real-world data\n",
    "\n",
    "### **Interactive Features:**\n",
    "🎮 Model complexity explorer | 📊 Real-time training visualization | ⚡ Regularization parameter tuning | 🔬 Performance comparison dashboard\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997cc4a0",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll use PyTorch for neural networks, NumPy for data manipulation, and Matplotlib for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0dd6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-2 * np.pi, 2 * np.pi, 200)\n",
    "# Updated true function\n",
    "y_true = 3 * np.sin(X)\n",
    "noise = np.random.normal(0, 0.7, size=X.shape)\n",
    "y = y_true + noise\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(X, y_true, label='True function', color='black', linewidth=2)\n",
    "plt.scatter(X, y, label='Noisy data', alpha=0.5)\n",
    "plt.title('Simple True Function: $3 \\sin(x)$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b9168",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Dataset\n",
    "\n",
    "Let's create a dataset based on the sine function with controllable noise levels. This will allow us to demonstrate how different amounts of training data and noise affect overfitting and underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bace030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive dataset for training and testing\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate training data (smaller sample)\n",
    "X_sample = np.linspace(-2*np.pi, 2*np.pi, 50).reshape(-1, 1)\n",
    "y_true_sample = 3 * np.sin(X_sample.flatten())\n",
    "noise_sample = np.random.normal(0, 0.5, size=y_true_sample.shape)\n",
    "y_sample = y_true_sample + noise_sample\n",
    "\n",
    "# Generate dense test data for smooth visualization\n",
    "X_dense = np.linspace(-2*np.pi, 2*np.pi, 200).reshape(-1, 1)\n",
    "y_true_dense = 3 * np.sin(X_dense.flatten())\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(X_dense, y_true_dense, 'k-', linewidth=2, label='True Function')\n",
    "plt.scatter(X_sample, y_sample, alpha=0.7, s=40, color='blue', label='Training Data')\n",
    "plt.title('Training Dataset vs True Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(X, y_true, 'k-', linewidth=2, label='True Function')\n",
    "plt.scatter(X, y, alpha=0.5, s=20, color='orange', label='Noisy Data')\n",
    "plt.title('Full Dataset: $3\\\\sin(x)$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"🎯 DATASET CHARACTERISTICS:\")\n",
    "print(f\"• Training samples: {len(X_sample)}\")\n",
    "print(f\"• Full dataset: {len(X)}\")\n",
    "print(f\"• Function: 3*sin(x)\")\n",
    "print(f\"• Noise level: σ = 0.5-0.7\")\n",
    "print(\"• Challenge: Learn simple sine function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bc933a",
   "metadata": {},
   "source": [
    "## 3. Create Neural Network Models\n",
    "\n",
    "Let's define neural network architectures with different complexities to demonstrate underfitting, good fit, and overfitting scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c49baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics for model performance\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate various performance metrics\"\"\"\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    # R-squared (coefficient of determination)\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse, \n",
    "        'MAE': mae,\n",
    "        'R²': r2\n",
    "    }\n",
    "\n",
    "# Example: Simple baseline predictions\n",
    "# Linear baseline (underfitting example)\n",
    "linear_coeffs = np.polyfit(X.flatten(), y, 1)  # Fit linear model\n",
    "y_pred_linear = np.polyval(linear_coeffs, X)\n",
    "\n",
    "# Neural Network (overfitting example) - Train a very complex NN on limited data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class OverfittingNet(nn.Module):\n",
    "    \"\"\"Very complex network designed to overfit with limited data\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Train overfitting network on limited data (only 30 points)\n",
    "np.random.seed(42)\n",
    "X_limited = np.linspace(-2*np.pi, 2*np.pi, 30).reshape(-1, 1)  # Very limited data\n",
    "y_true_limited = 3 * np.sin(X_limited.flatten())\n",
    "noise_limited = np.random.normal(0, 0.3, size=y_true_limited.shape)\n",
    "y_limited = y_true_limited + noise_limited\n",
    "\n",
    "# Convert to tensors\n",
    "X_limited_tensor = torch.FloatTensor(X_limited).to(device)\n",
    "y_limited_tensor = torch.FloatTensor(y_limited.reshape(-1, 1)).to(device)\n",
    "X_tensor = torch.FloatTensor(X.reshape(-1, 1)).to(device)\n",
    "\n",
    "# Train the overfitting network\n",
    "overfitting_model = OverfittingNet().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(overfitting_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training overfitting neural network...\")\n",
    "for epoch in range(1000):  # Train for many epochs to ensure overfitting\n",
    "    optimizer.zero_grad()\n",
    "    pred = overfitting_model(X_limited_tensor)\n",
    "    loss = criterion(pred, y_limited_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Get predictions on full range\n",
    "overfitting_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_nn = overfitting_model(X_tensor).cpu().numpy().flatten()\n",
    "\n",
    "print(\"✅ Overfitting neural network trained!\")\n",
    "\n",
    "# Calculate metrics\n",
    "linear_metrics = calculate_metrics(y_true, y_pred_linear)\n",
    "nn_metrics = calculate_metrics(y_true, y_pred_nn)\n",
    "\n",
    "print(\"📊 BASELINE MODEL COMPARISON:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Linear Model (Underfitting):\")\n",
    "for metric, value in linear_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nNeural Network (Overfitting):\")\n",
    "for metric, value in nn_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Visualize baseline models\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(X, y_true, 'k-', linewidth=2, label='True Function')\n",
    "plt.scatter(X, y, alpha=0.5, s=20, color='lightgray', label='Noisy Data')\n",
    "plt.plot(X, y_pred_linear, 'b--', linewidth=2, label='Linear Fit')\n",
    "plt.title('Linear Model - Underfitting')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(X, y_true, 'k-', linewidth=2, label='True Function')\n",
    "plt.scatter(X, y, alpha=0.5, s=20, color='lightgray', label='Full Noisy Data')\n",
    "plt.scatter(X_limited, y_limited, alpha=0.8, s=60, color='red', label='Limited Training Data', marker='x')\n",
    "plt.plot(X, y_pred_nn, 'r--', linewidth=2, label='Neural Network')\n",
    "plt.title('Neural Network - Overfitting')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(X, y_true, 'k-', linewidth=3, label='True Function', alpha=0.9)\n",
    "plt.plot(X, y_pred_linear, 'b--', linewidth=2, label='Linear (Underfit)', alpha=0.8)\n",
    "plt.plot(X, y_pred_nn, 'r--', linewidth=2, label='Neural Net (Overfit)', alpha=0.8)\n",
    "plt.scatter(X_limited, y_limited, alpha=0.9, s=40, color='red', label='Training Data', marker='o')\n",
    "plt.title('Comparison: Under vs Over-fitting')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a8c6d",
   "metadata": {},
   "source": [
    "## 4. Train Models with Different Complexities\n",
    "\n",
    "Now let's train each model and track their performance to observe overfitting and underfitting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=300, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model and track training/validation loss\n",
    "    \n",
    "    Parameters:\n",
    "    - model: PyTorch model to train\n",
    "    - X_train, y_train: Training data\n",
    "    - X_val, y_val: Validation data\n",
    "    - epochs: Number of training epochs\n",
    "    - lr: Learning rate\n",
    "    \n",
    "    Returns:\n",
    "    - model: Trained model\n",
    "    - train_losses: List of training losses\n",
    "    - val_losses: List of validation losses\n",
    "    \"\"\"\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val.reshape(-1, 1)).to(device)\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Track losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        # Training step\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model(X_train_tensor)\n",
    "        train_loss = criterion(train_pred, y_train_tensor)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val_tensor)\n",
    "            val_loss = criterion(val_pred, y_val_tensor)\n",
    "        model.train()\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        # Print progress every 50 epochs\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss.item():.6f}, Val Loss: {val_loss.item():.6f}\")\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Define neural network models with different complexities\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"Simple network - may underfit\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class OptimalNet(nn.Module):\n",
    "    \"\"\"Optimal network - good balance\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class ComplexNet(nn.Module):\n",
    "    \"\"\"Complex network - may overfit\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Generate data for training\n",
    "np.random.seed(42)\n",
    "X_data = np.linspace(-2*np.pi, 2*np.pi, 100).reshape(-1, 1)\n",
    "y_true_data = 3 * np.sin(X_data.flatten())\n",
    "noise = np.random.normal(0, 0.5, size=y_true_data.shape)\n",
    "y_data = y_true_data + noise\n",
    "\n",
    "# Split data into train/validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"TRAINING ALL MODELS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Train models\n",
    "simple_model, simple_train_losses, simple_val_losses = train_model(\n",
    "    SimpleNet(), X_train, y_train, X_val, y_val, epochs=300\n",
    ")\n",
    "\n",
    "optimal_model, optimal_train_losses, optimal_val_losses = train_model(\n",
    "    OptimalNet(), X_train, y_train, X_val, y_val, epochs=300\n",
    ")\n",
    "\n",
    "complex_model, complex_train_losses, complex_val_losses = train_model(\n",
    "    ComplexNet(), X_train, y_train, X_val, y_val, epochs=300\n",
    ")\n",
    "\n",
    "# Store training histories for visualization\n",
    "training_histories = {\n",
    "    'Simple Model': {'train_losses': simple_train_losses, 'val_losses': simple_val_losses},\n",
    "    'Optimal Model': {'train_losses': optimal_train_losses, 'val_losses': optimal_val_losses},\n",
    "    'Complex Model': {'train_losses': complex_train_losses, 'val_losses': complex_val_losses}\n",
    "}\n",
    "\n",
    "print(\"✅ All models trained successfully!\")\n",
    "print(f\"Training data points: {len(X_train)}\")\n",
    "print(f\"Validation data points: {len(X_val)}\")\n",
    "\n",
    "# Define neural network models with different complexities\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"Simple network - may underfit\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class OptimalNet(nn.Module):\n",
    "    \"\"\"Optimal network - good balance\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class ComplexNet(nn.Module):\n",
    "    \"\"\"Complex network - may overfit\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Prepare training data from the samples we created earlier\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"TRAINING NEURAL NETWORK MODELS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create and train models\n",
    "models = {\n",
    "    'Simple (Underfitting)': SimpleNet(),\n",
    "    'Optimal': OptimalNet(), \n",
    "    'Complex (Overfitting)': ComplexNet()\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "training_histories = {}\n",
    "\n",
    "# Train each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n🔄 Training {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    trained_model, train_losses, val_losses = train_model(\n",
    "        model, X_train, y_train, X_val, y_val, epochs=200, lr=0.01\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    trained_models[name] = trained_model\n",
    "    training_histories[name] = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ {name} training completed!\")\n",
    "\n",
    "print(\"\\n🏁 All models trained successfully!\")\n",
    "print(\"\\nFinal Training Losses:\")\n",
    "for name, history in training_histories.items():\n",
    "    final_train_loss = history['train_losses'][-1]\n",
    "    final_val_loss = history['val_losses'][-1]\n",
    "    overfitting_gap = final_val_loss - final_train_loss\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Train Loss: {final_train_loss:.6f}\")\n",
    "    print(f\"    Val Loss:   {final_val_loss:.6f}\")\n",
    "    print(f\"    Gap:        {overfitting_gap:.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7a6bd0",
   "metadata": {},
   "source": [
    "## 5. Implement Regularization Techniques\n",
    "\n",
    "Let's explore various regularization methods to prevent overfitting: dropout, L1/L2 regularization, early stopping, and batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c2c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple regularization demonstration\n",
    "class RegularizedNet(nn.Module):\n",
    "    \"\"\"Network with dropout for regularization\"\"\"\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "print(\"🔧 REGULARIZATION DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Train a regularized model for comparison\n",
    "print(\"Training regularized model with dropout...\")\n",
    "regularized_model = RegularizedNet(dropout_rate=0.5)\n",
    "reg_model, reg_train_losses, reg_val_losses = train_model(\n",
    "    regularized_model, X_train, y_train, X_val, y_val, epochs=200, lr=0.01\n",
    ")\n",
    "\n",
    "# Compare final losses\n",
    "final_reg_train = reg_train_losses[-1]\n",
    "final_reg_val = reg_val_losses[-1]\n",
    "reg_gap = final_reg_val - final_reg_train\n",
    "\n",
    "print(f\"\\n📊 REGULARIZED MODEL RESULTS:\")\n",
    "print(f\"  Train Loss: {final_reg_train:.6f}\")\n",
    "print(f\"  Val Loss:   {final_reg_val:.6f}\")\n",
    "print(f\"  Gap:        {reg_gap:.6f}\")\n",
    "\n",
    "print(\"\\n💡 KEY INSIGHTS:\")\n",
    "print(\"• Dropout randomly sets neurons to zero during training\")\n",
    "print(\"• This prevents the model from memorizing specific patterns\")\n",
    "print(\"• Results in better generalization to new data\")\n",
    "print(\"• Lower overfitting gap indicates better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd6eeb2",
   "metadata": {},
   "source": [
    "## 6. Visualize Training and Validation Loss\n",
    "\n",
    "Let's plot training and validation loss curves to clearly see overfitting and underfitting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbd3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learning curves for all models\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Training curves comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "for name, history in training_histories.items():\n",
    "    epochs = range(len(history['train_losses']))\n",
    "    plt.plot(epochs, history['train_losses'], '-', label=f'{name} Train', linewidth=2)\n",
    "    plt.plot(epochs, history['val_losses'], '--', label=f'{name} Val', linewidth=2)\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 2: Final loss comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "model_names = list(training_histories.keys())\n",
    "train_final = [training_histories[name]['train_losses'][-1] for name in model_names]\n",
    "val_final = [training_histories[name]['val_losses'][-1] for name in model_names]\n",
    "\n",
    "x_pos = range(len(model_names))\n",
    "plt.bar([x - 0.2 for x in x_pos], train_final, 0.4, label='Train Loss', alpha=0.7)\n",
    "plt.bar([x + 0.2 for x in x_pos], val_final, 0.4, label='Val Loss', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Final Loss')\n",
    "plt.title('Final Loss Comparison')\n",
    "plt.xticks(x_pos, [name.split()[0] for name in model_names], rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Overfitting gap\n",
    "plt.subplot(1, 3, 3)\n",
    "gaps = [val_final[i] - train_final[i] for i in range(len(model_names))]\n",
    "colors = ['blue' if gap < 0.01 else 'orange' if gap < 0.05 else 'red' for gap in gaps]\n",
    "\n",
    "plt.bar(x_pos, gaps, color=colors, alpha=0.7)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Overfitting Gap (Val - Train)')\n",
    "plt.title('Overfitting Analysis')\n",
    "plt.xticks(x_pos, [name.split()[0] for name in model_names], rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show key insights about overfitting/underfitting\n",
    "print(\"📈 TRAINING CURVE ANALYSIS:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"🔍 Underfitting (Simple Model):\")\n",
    "print(\"   • High training AND validation loss\")\n",
    "print(\"   • Both curves plateau at high values\")\n",
    "print(\"   • Model lacks capacity to learn patterns\")\n",
    "\n",
    "print(\"\\n🔍 Good Fit (Optimal Model):\")\n",
    "print(\"   • Low training and validation loss\")\n",
    "print(\"   • Small gap between train/val curves\")\n",
    "print(\"   • Good generalization\")\n",
    "\n",
    "print(\"\\n🔍 Overfitting (Complex Model):\")\n",
    "print(\"   • Very low training loss\")\n",
    "print(\"   • Higher validation loss\")\n",
    "print(\"   • Large gap indicates poor generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6897ec7",
   "metadata": {},
   "source": [
    "## 7. Compare Model Performance\n",
    "\n",
    "Let's visualize how well each model learned to approximate the sine function by comparing their predictions against the true function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ef722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Ensure we have consistent data\n",
    "np.random.seed(42)\n",
    "X_data = np.linspace(-2*np.pi, 2*np.pi, 100).reshape(-1, 1)\n",
    "y_true_data = 3 * np.sin(X_data.flatten())\n",
    "noise = np.random.normal(0, 0.5, size=y_true_data.shape)\n",
    "y_data = y_true_data + noise\n",
    "\n",
    "# Split data into train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define simple neural network models\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class OptimalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class ComplexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_simple_model(model, epochs=300):\n",
    "    \"\"\"Train a model and return training history\"\"\"\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val.reshape(-1, 1)).to(device)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model(X_train_tensor)\n",
    "        train_loss = criterion(train_pred, y_train_tensor)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val_tensor)\n",
    "            val_loss = criterion(val_pred, y_val_tensor)\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Train models\n",
    "print(\"Training models...\")\n",
    "simple_model, simple_train_loss, simple_val_loss = train_simple_model(SimpleNet())\n",
    "optimal_model, optimal_train_loss, optimal_val_loss = train_simple_model(OptimalNet())\n",
    "complex_model, complex_train_loss, complex_val_loss = train_simple_model(ComplexNet())\n",
    "\n",
    "print(\"✅ All models trained!\")\n",
    "\n",
    "# Evaluate models on test data\n",
    "X_test = np.linspace(-2*np.pi, 2*np.pi, 200).reshape(-1, 1)\n",
    "y_test_true = 3 * np.sin(X_test.flatten())\n",
    "\n",
    "# Get predictions\n",
    "models = {\n",
    "    'Simple (Underfitting)': simple_model,\n",
    "    'Optimal': optimal_model, \n",
    "    'Complex (Overfitting)': complex_model\n",
    "}\n",
    "\n",
    "predictions = {}\n",
    "for name, model in models.items():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "        pred = model(X_test_tensor).cpu().numpy().flatten()\n",
    "        predictions[name] = pred\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7, 5))\n",
    "\n",
    "# Plot 1: Training curves\n",
    "ax1 = axes[0, 0]\n",
    "epochs = range(len(simple_train_loss))\n",
    "ax1.plot(epochs, simple_train_loss, 'b-', label='Simple Train', alpha=0.7)\n",
    "ax1.plot(epochs, simple_val_loss, 'b--', label='Simple Val', alpha=0.7)\n",
    "ax1.plot(epochs, optimal_train_loss, 'g-', label='Optimal Train', alpha=0.7)\n",
    "ax1.plot(epochs, optimal_val_loss, 'g--', label='Optimal Val', alpha=0.7)\n",
    "ax1.plot(epochs, complex_train_loss, 'r-', label='Complex Train', alpha=0.7)\n",
    "ax1.plot(epochs, complex_val_loss, 'r--', label='Complex Val', alpha=0.7)\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Plots 2-4: Model predictions\n",
    "colors = ['blue', 'green', 'red']\n",
    "for i, (name, pred) in enumerate(predictions.items()):\n",
    "    ax = axes[0, 1] if i == 0 else axes[1, i-1]\n",
    "    \n",
    "    # Plot true function and data\n",
    "    ax.plot(X_test, y_test_true, 'k-', linewidth=2, label='True Function', alpha=0.8)\n",
    "    ax.scatter(X_train, y_train, alpha=0.5, s=20, color='lightgray', label='Training Data')\n",
    "    \n",
    "    # Plot prediction\n",
    "    ax.plot(X_test, pred, color=colors[i], linewidth=2, linestyle='--', label=f'{name}')\n",
    "    \n",
    "    # Calculate and display MSE\n",
    "    mse = np.mean((pred - y_test_true)**2)\n",
    "    r2 = r2_score(y_test_true, pred)\n",
    "    \n",
    "    ax.text(0.02, 0.98, f'MSE: {mse:.3f}\\nR²: {r2:.3f}', \n",
    "            transform=ax.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "            fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'{name} Predictions')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-2*np.pi, 2*np.pi)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance summary\n",
    "print(\"\\n📊 MODEL PERFORMANCE SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "for name, pred in predictions.items():\n",
    "    mse = np.mean((pred - y_test_true)**2)\n",
    "    r2 = r2_score(y_test_true, pred)\n",
    "    final_train = models[name] \n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  R²:  {r2:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"🔍 Simple Model: May underfit - cannot capture complexity\")\n",
    "print(\"🔍 Optimal Model: Good balance between bias and variance\") \n",
    "print(\"🔍 Complex Model: May overfit - high variance, memorizes noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b0856",
   "metadata": {},
   "source": [
    "## 8. Interactive Model Explorer\n",
    "\n",
    "Let's create an interactive tool to explore how different hyperparameters affect overfitting and underfitting in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for interactive widgets\n",
    "try:\n",
    "    from ipywidgets import interact, widgets\n",
    "    from IPython.display import display\n",
    "    from sklearn.metrics import r2_score\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ ipywidgets not available. Interactive features will be disabled.\")\n",
    "    try:\n",
    "        from sklearn.metrics import r2_score\n",
    "        WIDGETS_AVAILABLE = False\n",
    "    except ImportError:\n",
    "        print(\"⚠️ sklearn not available. Some features will be disabled.\")\n",
    "        def r2_score(y_true, y_pred):\n",
    "            return 0.0  # Fallback function\n",
    "        WIDGETS_AVAILABLE = False\n",
    "\n",
    "def generate_sine_data(n_samples=200, noise_level=0.1, random_state=None):\n",
    "    \"\"\"Generate synthetic sine data for training\"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    X = np.linspace(-2*np.pi, 2*np.pi, n_samples).reshape(-1, 1)\n",
    "    y_true = 3 * np.sin(X.flatten())\n",
    "    noise = np.random.normal(0, noise_level, size=y_true.shape)\n",
    "    y = y_true + noise\n",
    "    return X, y, y_true\n",
    "\n",
    "class InteractiveModelExplorer:\n",
    "    \"\"\"Interactive tool for exploring model complexity and regularization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.current_model = None\n",
    "        self.training_history = None\n",
    "    \n",
    "    def create_and_train_model(self, hidden_size, num_layers, dropout_rate, l2_reg, \n",
    "                              learning_rate, epochs, dataset_size, noise_level):\n",
    "        \"\"\"Create and train a model with specified parameters\"\"\"\n",
    "        \n",
    "        # Generate dataset with specified parameters - use different random seed each time\n",
    "        import time\n",
    "        random_seed = int(time.time()) % 10000  # Different seed each time\n",
    "        X_data, y_data, _ = generate_sine_data(\n",
    "            n_samples=dataset_size, \n",
    "            noise_level=noise_level,\n",
    "            random_state=random_seed\n",
    "        )\n",
    "        \n",
    "        # Split data\n",
    "        X_train_inter, X_val_inter, y_train_inter, y_val_inter = train_test_split(\n",
    "            X_data, y_data, test_size=0.2, random_state=random_seed\n",
    "        )\n",
    "        \n",
    "        # Create model architecture - FIX: Define class properly with parameters\n",
    "        class DynamicNet(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_rate):\n",
    "                super(DynamicNet, self).__init__()\n",
    "                \n",
    "                layers = []\n",
    "                current_size = input_size\n",
    "                \n",
    "                # First layer\n",
    "                layers.append(nn.Linear(current_size, hidden_size))\n",
    "                layers.append(nn.ReLU())\n",
    "                if dropout_rate > 0:\n",
    "                    layers.append(nn.Dropout(dropout_rate))\n",
    "                current_size = hidden_size\n",
    "                \n",
    "                # Hidden layers\n",
    "                for _ in range(max(0, num_layers - 2)):  # Ensure we don't get negative layers\n",
    "                    layers.append(nn.Linear(current_size, hidden_size))\n",
    "                    layers.append(nn.ReLU())\n",
    "                    if dropout_rate > 0:\n",
    "                        layers.append(nn.Dropout(dropout_rate))\n",
    "                \n",
    "                # Output layer\n",
    "                layers.append(nn.Linear(current_size, output_size))\n",
    "                \n",
    "                self.network = nn.Sequential(*layers)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                return self.network(x)\n",
    "        \n",
    "        # Create and train model - FIX: Pass parameters explicitly\n",
    "        model = DynamicNet(\n",
    "            input_size=1, \n",
    "            hidden_size=hidden_size, \n",
    "            output_size=1, \n",
    "            num_layers=num_layers, \n",
    "            dropout_rate=dropout_rate\n",
    "        ).to(device)\n",
    "        \n",
    "        # Training setup\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_reg)\n",
    "        \n",
    "        # Convert to tensors - FIX: Proper tensor shapes\n",
    "        X_train_tensor = torch.FloatTensor(X_train_inter).to(device)\n",
    "        y_train_tensor = torch.FloatTensor(y_train_inter.reshape(-1, 1)).to(device)  # FIX: Reshape y\n",
    "        X_val_tensor = torch.FloatTensor(X_val_inter).to(device)\n",
    "        y_val_tensor = torch.FloatTensor(y_val_inter.reshape(-1, 1)).to(device)  # FIX: Reshape y\n",
    "        \n",
    "        # Training loop\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            # Training step\n",
    "            optimizer.zero_grad()\n",
    "            train_pred = model(X_train_tensor)\n",
    "            train_loss = criterion(train_pred, y_train_tensor)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Validation step\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = model(X_val_tensor)\n",
    "                val_loss = criterion(val_pred, y_val_tensor)\n",
    "            model.train()\n",
    "            \n",
    "            train_losses.append(train_loss.item())\n",
    "            val_losses.append(val_loss.item())\n",
    "        \n",
    "        # Store results\n",
    "        self.current_model = model\n",
    "        self.training_history = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'X_train': X_train_inter,\n",
    "            'y_train': y_train_inter,\n",
    "            'X_val': X_val_inter,\n",
    "            'y_val': y_val_inter\n",
    "        }\n",
    "        \n",
    "        return model, train_losses, val_losses, X_train_inter, y_train_inter\n",
    "    \n",
    "    def plot_results(self, model, train_losses, val_losses, X_train_data, y_train_data):\n",
    "        \"\"\"Plot training curves and model predictions\"\"\"\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(7, 5))\n",
    "        \n",
    "        # 1. Training curves\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "        ax1.plot(epochs, val_losses, 'r--', label='Validation Loss', linewidth=2)\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss (MSE)')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_yscale('log')\n",
    "        \n",
    "        # Add overfitting detection\n",
    "        final_train_loss = train_losses[-1]\n",
    "        final_val_loss = val_losses[-1]\n",
    "        overfitting_gap = final_val_loss - final_train_loss\n",
    "        \n",
    "        if overfitting_gap > 0.05:\n",
    "            ax1.text(0.6, 0.9, '⚠️ OVERFITTING', transform=ax1.transAxes,\n",
    "                    bbox=dict(boxstyle='round', facecolor='red', alpha=0.3),\n",
    "                    fontsize=12, fontweight='bold')\n",
    "        elif final_train_loss > 0.1:\n",
    "            ax1.text(0.6, 0.9, '⚠️ UNDERFITTING', transform=ax1.transAxes,\n",
    "                    bbox=dict(boxstyle='round', facecolor='orange', alpha=0.3),\n",
    "                    fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            ax1.text(0.6, 0.9, '✅ GOOD FIT', transform=ax1.transAxes,\n",
    "                    bbox=dict(boxstyle='round', facecolor='green', alpha=0.3),\n",
    "                    fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # 2. Model predictions\n",
    "        X_test_range = np.linspace(-2*np.pi, 2*np.pi, 200).reshape(-1, 1)\n",
    "        y_test_true = 3 * np.sin(X_test_range.flatten())\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.FloatTensor(X_test_range).to(device)\n",
    "            y_pred = model(X_test_tensor).cpu().numpy().flatten()  # FIX: Flatten for plotting\n",
    "        \n",
    "        ax2.scatter(X_train_data, y_train_data, alpha=0.6, s=30, \n",
    "                   color='lightblue', label='Training Data')\n",
    "        ax2.plot(X_test_range, y_test_true, 'k-', linewidth=3, \n",
    "                label='True Function', alpha=0.8)\n",
    "        ax2.plot(X_test_range, y_pred, 'r--', linewidth=2, \n",
    "                label='Model Prediction')\n",
    "        \n",
    "        ax2.set_xlabel('x')\n",
    "        ax2.set_ylabel('y')\n",
    "        ax2.set_title('Model Prediction vs True Function')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_xlim(-2*np.pi, 2*np.pi)\n",
    "        ax2.set_ylim(-6, 6)\n",
    "        \n",
    "        # 3. Loss evolution details\n",
    "        if len(epochs) > 50:\n",
    "            ax3.plot(epochs[-50:], train_losses[-50:], 'b-', \n",
    "                    label='Training Loss (last 50 epochs)', linewidth=2)\n",
    "            ax3.plot(epochs[-50:], val_losses[-50:], 'r--', \n",
    "                    label='Validation Loss (last 50 epochs)', linewidth=2)\n",
    "        else:\n",
    "            ax3.plot(epochs, train_losses, 'b-', \n",
    "                    label='Training Loss', linewidth=2)\n",
    "            ax3.plot(epochs, val_losses, 'r--', \n",
    "                    label='Validation Loss', linewidth=2)\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Loss (MSE)')\n",
    "        ax3.set_title('Loss Evolution (Final Epochs)')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Performance metrics\n",
    "        mse_test = np.mean((y_pred - y_test_true)**2)\n",
    "        r2_test = r2_score(y_test_true, y_pred)\n",
    "        \n",
    "        metrics_text = f\"\"\"\n",
    "        Performance Metrics:\n",
    "        \n",
    "        Final Training Loss: {final_train_loss:.6f}\n",
    "        Final Validation Loss: {final_val_loss:.6f}\n",
    "        Overfitting Gap: {overfitting_gap:.6f}\n",
    "        \n",
    "        Test MSE: {mse_test:.6f}\n",
    "        Test R²: {r2_test:.4f}\n",
    "        \n",
    "        Model Parameters: {sum(p.numel() for p in model.parameters()):,}\n",
    "        \"\"\"\n",
    "        \n",
    "        ax4.text(0.1, 0.9, metrics_text, transform=ax4.transAxes,\n",
    "                fontsize=11, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "        ax4.set_xlim(0, 1)\n",
    "        ax4.set_ylim(0, 1)\n",
    "        ax4.axis('off')\n",
    "        ax4.set_title('Performance Summary')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'final_train_loss': final_train_loss,\n",
    "            'final_val_loss': final_val_loss,\n",
    "            'overfitting_gap': overfitting_gap,\n",
    "            'test_mse': mse_test,\n",
    "            'test_r2': r2_test\n",
    "        }\n",
    "\n",
    "# Create explorer instance\n",
    "explorer = InteractiveModelExplorer()\n",
    "\n",
    "def interactive_model_training(hidden_size=50, num_layers=2, dropout_rate=0.0, \n",
    "                             l2_reg=0.0001, learning_rate=0.01, epochs=200,\n",
    "                             dataset_size=200, noise_level=0.1):\n",
    "    \"\"\"Interactive function for model training and visualization\"\"\"\n",
    "    \n",
    "    print(f\"🔄 Training model with parameters:\")\n",
    "    print(f\"   Hidden Size: {hidden_size}\")\n",
    "    print(f\"   Layers: {num_layers}\")\n",
    "    print(f\"   Dropout: {dropout_rate}\")\n",
    "    print(f\"   L2 Regularization: {l2_reg}\")\n",
    "    print(f\"   Learning Rate: {learning_rate}\")\n",
    "    print(f\"   Epochs: {epochs}\")\n",
    "    print(f\"   Dataset Size: {dataset_size}\")\n",
    "    print(f\"   Noise Level: {noise_level}\")\n",
    "    print()\n",
    "    \n",
    "    # Train model\n",
    "    model, train_losses, val_losses, X_train_data, y_train_data = explorer.create_and_train_model(\n",
    "        hidden_size, num_layers, dropout_rate, l2_reg, learning_rate, epochs, dataset_size, noise_level\n",
    "    )\n",
    "    \n",
    "    # Plot results\n",
    "    metrics = explorer.plot_results(model, train_losses, val_losses, X_train_data, y_train_data)\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"INTERPRETATION:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if metrics['overfitting_gap'] > 0.05:\n",
    "        print(\"🚨 OVERFITTING DETECTED!\")\n",
    "        print(\"   Try: Reduce model complexity, increase dropout, add L2 regularization\")\n",
    "    elif metrics['final_train_loss'] > 0.1:\n",
    "        print(\"🚨 UNDERFITTING DETECTED!\")\n",
    "        print(\"   Try: Increase model complexity, reduce regularization, train longer\")\n",
    "    else:\n",
    "        print(\"✅ GOOD BALANCE!\")\n",
    "        print(\"   Model shows good generalization capability\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"🎮 INTERACTIVE MODEL EXPLORER\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Experiment with different parameters to see their effects on overfitting and underfitting!\")\n",
    "print()\n",
    "\n",
    "# Create interactive widget only if widgets are available\n",
    "if WIDGETS_AVAILABLE:\n",
    "    try:\n",
    "        interact(interactive_model_training,\n",
    "                 hidden_size=widgets.IntSlider(value=50, min=5, max=200, step=5, \n",
    "                                              description='Hidden Size:'),\n",
    "                 num_layers=widgets.IntSlider(value=2, min=1, max=6, step=1, \n",
    "                                             description='# Layers:'),\n",
    "                 dropout_rate=widgets.FloatSlider(value=0.0, min=0.0, max=0.8, step=0.01, \n",
    "                                                 description='Dropout Rate:'),\n",
    "                 l2_reg=widgets.FloatLogSlider(value=0.0001, base=10, min=-6, max=-1, step=1, \n",
    "                                              description='L2 Reg:'),\n",
    "                 learning_rate=widgets.FloatLogSlider(value=0.01, base=10, min=-6, max=-1, step=1, \n",
    "                                                    description='Learning Rate:'),\n",
    "                 epochs=widgets.IntSlider(value=200, min=50, max=2000, step=100, \n",
    "                                         description='Epochs:'),\n",
    "                 dataset_size=widgets.IntSlider(value=200, min=50, max=1000, step=50, \n",
    "                                               description='Dataset Size:'),\n",
    "                 noise_level=widgets.FloatSlider(value=0.1, min=0.0, max=2.0, step=0.05, \n",
    "                                                description='Noise Level:'))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating interactive widget: {e}\")\n",
    "        print(\"📝 You can still run the function manually:\")\n",
    "        print(\"interactive_model_training(hidden_size=50, num_layers=2, dropout_rate=0.2)\")\n",
    "else:\n",
    "    print(\"📝 To use interactive features, install ipywidgets:\")\n",
    "    print(\"   pip install ipywidgets\")\n",
    "    print(\"   jupyter nbextension enable --py widgetsnbextension\")\n",
    "    print()\n",
    "    print(\"📝 You can still run the function manually:\")\n",
    "    print(\"interactive_model_training(hidden_size=50, num_layers=2, dropout_rate=0.2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed076061",
   "metadata": {},
   "source": [
    "## 9. 🎯 Design Recipe: First Overfit, Then Reduce Overfitting\n",
    "\n",
    "### **The Golden Rule of Deep Learning Model Development**\n",
    "\n",
    "> **\"First get a model that can overfit, then reduce overfitting\"**\n",
    "> \n",
    "> *This approach ensures your model has sufficient capacity to learn before you constrain it.*\n",
    "\n",
    "### **Why This Recipe Works:**\n",
    "\n",
    "1. **🎯 Capacity Verification**: If your model can't overfit, it lacks the capacity to learn the underlying patterns\n",
    "2. **🔧 Systematic Approach**: Start with maximum learning potential, then systematically constrain\n",
    "3. **🚀 Faster Development**: Avoids the trap of underparameterized models that will never work\n",
    "4. **📊 Clear Diagnostics**: Makes it obvious whether you have a capacity or generalization problem\n",
    "\n",
    "### **The Step-by-Step Process:**\n",
    "\n",
    "#### **Phase 1: Build an Overfitting Model**\n",
    "- Use a complex architecture with many parameters\n",
    "- No regularization initially  \n",
    "- Train until you see clear overfitting\n",
    "- This proves your model CAN learn the patterns\n",
    "\n",
    "#### **Phase 2: Systematically Reduce Overfitting**\n",
    "- Add regularization techniques one by one\n",
    "- Monitor the training/validation gap\n",
    "- Find the sweet spot between capacity and generalization\n",
    "\n",
    "---\n",
    "\n",
    "Let's demonstrate this recipe in action! 👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab405b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 DESIGN RECIPE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Following the golden rule: 'First get a model that can overfit, then reduce overfitting'\")\n",
    "print()\n",
    "\n",
    "# Generate training data (intentionally small to make overfitting easier)\n",
    "np.random.seed(42)\n",
    "X_recipe = np.linspace(-2*np.pi, 2*np.pi, 50).reshape(-1, 1)  # Small dataset\n",
    "y_true_recipe = 3 * np.sin(X_recipe.flatten())\n",
    "noise_recipe = np.random.normal(0, 0.3, size=y_true_recipe.shape)\n",
    "y_recipe = y_true_recipe + noise_recipe\n",
    "\n",
    "# Split into train/val (intentionally small)\n",
    "X_train_recipe, X_val_recipe, y_train_recipe, y_val_recipe = train_test_split(\n",
    "    X_recipe, y_recipe, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"📊 DATASET FOR RECIPE:\")\n",
    "print(f\"   Training samples: {len(X_train_recipe)}\")\n",
    "print(f\"   Validation samples: {len(X_val_recipe)}\")\n",
    "print(\"   Small dataset → Perfect for demonstrating overfitting\")\n",
    "print()\n",
    "\n",
    "# PHASE 1: Build a model that CAN overfit\n",
    "print(\"🚀 PHASE 1: BUILD AN OVERFITTING MODEL\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Goal: Create a model with enough capacity to memorize the training data\")\n",
    "print()\n",
    "\n",
    "class OverfittingCapableNet(nn.Module):\n",
    "    \"\"\"Intentionally over-parameterized network to demonstrate overfitting\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 256),      # Large hidden layers\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Train the overfitting model\n",
    "overfitting_capable = OverfittingCapableNet().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(overfitting_capable.parameters(), lr=0.001)  # Standard learning rate\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_recipe).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train_recipe.reshape(-1, 1)).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val_recipe).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val_recipe.reshape(-1, 1)).to(device)\n",
    "\n",
    "# Training loop - track losses\n",
    "train_losses_phase1 = []\n",
    "val_losses_phase1 = []\n",
    "\n",
    "print(\"Training overfitting-capable model...\")\n",
    "for epoch in range(1000):  # Train for many epochs\n",
    "    # Training\n",
    "    overfitting_capable.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_pred = overfitting_capable(X_train_tensor)\n",
    "    train_loss = criterion(train_pred, y_train_tensor)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    overfitting_capable.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = overfitting_capable(X_val_tensor)\n",
    "        val_loss = criterion(val_pred, y_val_tensor)\n",
    "    \n",
    "    train_losses_phase1.append(train_loss.item())\n",
    "    val_losses_phase1.append(val_loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        gap = val_loss.item() - train_loss.item()\n",
    "        print(f\"   Epoch {epoch+1}: Train={train_loss.item():.6f}, Val={val_loss.item():.6f}, Gap={gap:.6f}\")\n",
    "\n",
    "# Check if we achieved overfitting\n",
    "final_train_loss = train_losses_phase1[-1]\n",
    "final_val_loss = val_losses_phase1[-1]\n",
    "overfitting_gap = final_val_loss - final_train_loss\n",
    "\n",
    "print(f\"\\n✅ PHASE 1 RESULTS:\")\n",
    "print(f\"   Final Train Loss: {final_train_loss:.6f}\")\n",
    "print(f\"   Final Val Loss: {final_val_loss:.6f}\")\n",
    "print(f\"   Overfitting Gap: {overfitting_gap:.6f}\")\n",
    "\n",
    "if overfitting_gap > 0.01:\n",
    "    print(\"   ✅ SUCCESS: Model can overfit! (Large gap between train/val)\")\n",
    "    print(\"   📋 This proves the model has sufficient capacity to learn\")\n",
    "else:\n",
    "    print(\"   ⚠️ WARNING: Model might not be overfitting enough\")\n",
    "    print(\"   📋 Consider: More parameters, longer training, or smaller dataset\")\n",
    "\n",
    "print(f\"\\n📈 Model Parameters: {sum(p.numel() for p in overfitting_capable.parameters()):,}\")\n",
    "print(\"🎯 Next: Apply regularization to reduce overfitting while maintaining performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e220957",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🔧 PHASE 2: SYSTEMATICALLY REDUCE OVERFITTING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Goal: Apply regularization techniques one by one to find the optimal balance\")\n",
    "print()\n",
    "\n",
    "# Strategy: Test different regularization approaches\n",
    "regularization_strategies = {\n",
    "    \"No Regularization\": {\"dropout\": 0.0, \"weight_decay\": 0.0, \"early_stop\": False},\n",
    "    \"Light Dropout\": {\"dropout\": 0.2, \"weight_decay\": 0.0, \"early_stop\": False},\n",
    "    \"Heavy Dropout\": {\"dropout\": 0.5, \"weight_decay\": 0.0, \"early_stop\": False},\n",
    "    \"L2 Regularization\": {\"dropout\": 0.0, \"weight_decay\": 0.01, \"early_stop\": False},\n",
    "    \"Combined (Dropout + L2)\": {\"dropout\": 0.3, \"weight_decay\": 0.001, \"early_stop\": False},\n",
    "    \"Early Stopping\": {\"dropout\": 0.0, \"weight_decay\": 0.0, \"early_stop\": True}\n",
    "}\n",
    "\n",
    "class RegularizedNet(nn.Module):\n",
    "    \"\"\"Network with configurable regularization\"\"\"\n",
    "    def __init__(self, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        # Same architecture as overfitting model, but with dropout\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_with_regularization(dropout_rate, weight_decay, early_stop, max_epochs=800):\n",
    "    \"\"\"Train model with specified regularization parameters\"\"\"\n",
    "    model = RegularizedNet(dropout_rate).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 50  # For early stopping\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model(X_train_tensor)\n",
    "        train_loss = criterion(train_pred, y_train_tensor)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val_tensor)\n",
    "            val_loss = criterion(val_pred, y_val_tensor)\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stop:\n",
    "            if val_loss.item() < best_val_loss:\n",
    "                best_val_loss = val_loss.item()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"      Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Test each regularization strategy\n",
    "results = {}\n",
    "print(\"Testing different regularization strategies...\")\n",
    "print()\n",
    "\n",
    "for strategy_name, params in regularization_strategies.items():\n",
    "    print(f\"🔄 Testing: {strategy_name}\")\n",
    "    \n",
    "    model, train_losses, val_losses = train_with_regularization(\n",
    "        dropout_rate=params[\"dropout\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        early_stop=params[\"early_stop\"]\n",
    "    )\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    final_train = train_losses[-1]\n",
    "    final_val = val_losses[-1]\n",
    "    gap = final_val - final_train\n",
    "    epochs_trained = len(train_losses)\n",
    "    \n",
    "    results[strategy_name] = {\n",
    "        \"model\": model,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"final_train_loss\": final_train,\n",
    "        \"final_val_loss\": final_val,\n",
    "        \"overfitting_gap\": gap,\n",
    "        \"epochs_trained\": epochs_trained\n",
    "    }\n",
    "    \n",
    "    print(f\"   Train: {final_train:.4f}, Val: {final_val:.4f}, Gap: {gap:.4f}, Epochs: {epochs_trained}\")\n",
    "\n",
    "print(\"\\n📊 REGULARIZATION RESULTS SUMMARY:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Strategy':<25} {'Train Loss':<12} {'Val Loss':<10} {'Gap':<8} {'Epochs':<8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"{name:<25} {result['final_train_loss']:<12.4f} {result['final_val_loss']:<10.4f} \"\n",
    "          f\"{result['overfitting_gap']:<8.4f} {result['epochs_trained']:<8}\")\n",
    "\n",
    "# Find the best strategy (lowest validation loss with reasonable gap)\n",
    "best_strategy = min(results.keys(), \n",
    "                   key=lambda x: results[x]['final_val_loss'] + 0.1 * max(0, results[x]['overfitting_gap']))\n",
    "\n",
    "print(f\"\\n🏆 RECOMMENDED STRATEGY: {best_strategy}\")\n",
    "best_result = results[best_strategy]\n",
    "print(f\"   ✅ Val Loss: {best_result['final_val_loss']:.4f}\")\n",
    "print(f\"   ✅ Overfitting Gap: {best_result['overfitting_gap']:.4f}\")\n",
    "print(f\"   ✅ Good balance between performance and generalization\")\n",
    "\n",
    "print(f\"\\n💡 KEY INSIGHT:\")\n",
    "print(f\"   Started with overfitting gap of {overfitting_gap:.4f}\")\n",
    "print(f\"   Reduced to {best_result['overfitting_gap']:.4f} with {best_strategy}\")\n",
    "print(f\"   Improvement: {((overfitting_gap - best_result['overfitting_gap']) / overfitting_gap * 100):.1f}% reduction in overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the design recipe results\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Training curves comparison\n",
    "plt.subplot(2, 3, 1)\n",
    "# First show the original overfitting model\n",
    "epochs_phase1 = range(len(train_losses_phase1))\n",
    "plt.plot(epochs_phase1, train_losses_phase1, 'r-', alpha=0.7, linewidth=2, label='Phase 1: Overfitting Model (Train)')\n",
    "plt.plot(epochs_phase1, val_losses_phase1, 'r--', alpha=0.7, linewidth=2, label='Phase 1: Overfitting Model (Val)')\n",
    "\n",
    "# Show the best regularized model\n",
    "best_train = results[best_strategy]['train_losses']\n",
    "best_val = results[best_strategy]['val_losses']\n",
    "epochs_best = range(len(best_train))\n",
    "plt.plot(epochs_best, best_train, 'g-', alpha=0.8, linewidth=2, label=f'Phase 2: {best_strategy} (Train)')\n",
    "plt.plot(epochs_best, best_val, 'g--', alpha=0.8, linewidth=2, label=f'Phase 2: {best_strategy} (Val)')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Design Recipe: Before vs After Regularization')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 2: Overfitting gap comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "strategies = list(results.keys())\n",
    "gaps = [results[strategy]['overfitting_gap'] for strategy in strategies]\n",
    "colors = ['red' if gap > 0.05 else 'orange' if gap > 0.02 else 'green' for gap in gaps]\n",
    "\n",
    "bars = plt.bar(range(len(strategies)), gaps, color=colors, alpha=0.7)\n",
    "plt.xlabel('Regularization Strategy')\n",
    "plt.ylabel('Overfitting Gap (Val - Train)')\n",
    "plt.title('Overfitting Gap by Strategy')\n",
    "plt.xticks(range(len(strategies)), [s.replace(' ', '\\n') for s in strategies], rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.02, color='orange', linestyle='--', alpha=0.5, label='Good Gap Threshold')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: Validation loss comparison\n",
    "plt.subplot(2, 3, 3)\n",
    "val_losses_final = [results[strategy]['final_val_loss'] for strategy in strategies]\n",
    "colors_val = ['green' if loss == min(val_losses_final) else 'lightblue' for loss in val_losses_final]\n",
    "\n",
    "bars = plt.bar(range(len(strategies)), val_losses_final, color=colors_val, alpha=0.7)\n",
    "plt.xlabel('Regularization Strategy')\n",
    "plt.ylabel('Final Validation Loss')\n",
    "plt.title('Validation Performance by Strategy')\n",
    "plt.xticks(range(len(strategies)), [s.replace(' ', '\\n') for s in strategies], rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Model predictions comparison\n",
    "plt.subplot(2, 3, 4)\n",
    "X_test_recipe = np.linspace(-2*np.pi, 2*np.pi, 200).reshape(-1, 1)\n",
    "y_test_true_recipe = 3 * np.sin(X_test_recipe.flatten())\n",
    "\n",
    "# Original overfitting model prediction\n",
    "overfitting_capable.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.FloatTensor(X_test_recipe).to(device)\n",
    "    y_pred_overfitting = overfitting_capable(X_test_tensor).cpu().numpy().flatten()\n",
    "\n",
    "# Best regularized model prediction\n",
    "best_model = results[best_strategy]['model']\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_best = best_model(X_test_tensor).cpu().numpy().flatten()\n",
    "\n",
    "plt.plot(X_test_recipe, y_test_true_recipe, 'k-', linewidth=3, label='True Function', alpha=0.8)\n",
    "plt.scatter(X_train_recipe, y_train_recipe, alpha=0.7, s=60, color='blue', label='Training Data', zorder=5)\n",
    "plt.plot(X_test_recipe, y_pred_overfitting, 'r--', linewidth=2, alpha=0.7, label='Phase 1: Overfitting')\n",
    "plt.plot(X_test_recipe, y_pred_best, 'g--', linewidth=2, alpha=0.8, label=f'Phase 2: {best_strategy}')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Predictions: Before vs After Regularization')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-2*np.pi, 2*np.pi)\n",
    "\n",
    "# Plot 5: Design recipe process flow\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.text(0.5, 0.9, '🎯 DESIGN RECIPE PROCESS', ha='center', va='center', \n",
    "         fontsize=16, fontweight='bold', transform=plt.gca().transAxes)\n",
    "\n",
    "process_steps = [\n",
    "    \"1. 🚀 Build Overfitting Model\",\n",
    "    \"   • Large architecture\",\n",
    "    \"   • No regularization\", \n",
    "    \"   • Train until overfitting\",\n",
    "    \"\",\n",
    "    \"2. ✅ Verify Overfitting\",\n",
    "    \"   • Large train/val gap\",\n",
    "    \"   • Proves sufficient capacity\",\n",
    "    \"\",\n",
    "    \"3. 🔧 Apply Regularization\",\n",
    "    \"   • Test different techniques\",\n",
    "    \"   • Monitor train/val gap\",\n",
    "    \"   • Find optimal balance\",\n",
    "    \"\",\n",
    "    \"4. 🏆 Select Best Model\",\n",
    "    f\"   • Chosen: {best_strategy}\",\n",
    "    f\"   • Gap reduced by {((overfitting_gap - best_result['overfitting_gap']) / overfitting_gap * 100):.1f}%\"\n",
    "]\n",
    "\n",
    "y_pos = 0.75\n",
    "for step in process_steps:\n",
    "    if step.startswith((\"1.\", \"2.\", \"3.\", \"4.\")):\n",
    "        plt.text(0.1, y_pos, step, ha='left', va='center', fontsize=12, fontweight='bold',\n",
    "                color='darkblue', transform=plt.gca().transAxes)\n",
    "    elif step.startswith(\"   •\"):\n",
    "        plt.text(0.15, y_pos, step, ha='left', va='center', fontsize=10,\n",
    "                color='darkgreen', transform=plt.gca().transAxes)\n",
    "    else:\n",
    "        plt.text(0.15, y_pos, step, ha='left', va='center', fontsize=10,\n",
    "                transform=plt.gca().transAxes)\n",
    "    y_pos -= 0.05\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot 6: Success metrics\n",
    "plt.subplot(2, 3, 6)\n",
    "metrics_data = {\n",
    "    'Original\\nOverfitting': [overfitting_gap, final_val_loss],\n",
    "    f'Best Strategy\\n({best_strategy})': [best_result['overfitting_gap'], best_result['final_val_loss']]\n",
    "}\n",
    "\n",
    "x_pos = np.arange(len(metrics_data))\n",
    "width = 0.35\n",
    "\n",
    "overfitting_gaps = [metrics_data[key][0] for key in metrics_data.keys()]\n",
    "val_losses = [metrics_data[key][1] for key in metrics_data.keys()]\n",
    "\n",
    "bars1 = plt.bar(x_pos - width/2, overfitting_gaps, width, label='Overfitting Gap', alpha=0.7, color='orange')\n",
    "bars2 = plt.bar(x_pos + width/2, val_losses, width, label='Validation Loss', alpha=0.7, color='lightblue')\n",
    "\n",
    "plt.xlabel('Model Version')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title('Recipe Success: Before vs After')\n",
    "plt.xticks(x_pos, metrics_data.keys())\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎯 DESIGN RECIPE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ PHASE 1 SUCCESS: Built a model that can overfit\")\n",
    "print(f\"   • Overfitting gap: {overfitting_gap:.4f}\")\n",
    "print(\"   • This proves the model has sufficient learning capacity\")\n",
    "print()\n",
    "print(\"✅ PHASE 2 SUCCESS: Systematically reduced overfitting\")\n",
    "print(f\"   • Best strategy: {best_strategy}\")\n",
    "print(f\"   • New overfitting gap: {best_result['overfitting_gap']:.4f}\")\n",
    "print(f\"   • Improvement: {((overfitting_gap - best_result['overfitting_gap']) / overfitting_gap * 100):.1f}% reduction\")\n",
    "print()\n",
    "print(\"🏆 FINAL RESULT: Well-generalized model with good performance!\")\n",
    "print(f\"   • Maintained learning capacity while improving generalization\")\n",
    "print(f\"   • Clear systematic approach avoids guesswork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7941c1c",
   "metadata": {},
   "source": [
    "### 🛠️ Practical Guidelines for Applying the Design Recipe\n",
    "\n",
    "#### **Phase 1 Checklist: Building an Overfitting Model**\n",
    "\n",
    "✅ **Model Architecture:**\n",
    "- Start with more layers/neurons than you think you need\n",
    "- Use standard activations (ReLU, etc.)\n",
    "- No regularization initially (no dropout, weight decay, batch norm)\n",
    "\n",
    "✅ **Training Setup:**\n",
    "- Use a reasonable learning rate (0.001-0.01)\n",
    "- Train for many epochs (until clear overfitting appears)\n",
    "- Monitor both training and validation loss\n",
    "\n",
    "✅ **Success Criteria:**\n",
    "- Training loss much lower than validation loss\n",
    "- Clear divergence between train/val curves\n",
    "- Gap > 0.02-0.05 depending on problem complexity\n",
    "\n",
    "❌ **Common Mistakes:**\n",
    "- Starting with regularization too early\n",
    "- Using too small architectures\n",
    "- Stopping training before overfitting appears\n",
    "\n",
    "---\n",
    "\n",
    "#### **Phase 2 Checklist: Reducing Overfitting Systematically**\n",
    "\n",
    "✅ **Regularization Techniques to Try (in order):**\n",
    "1. **Dropout** (0.1 → 0.3 → 0.5): Simple and effective\n",
    "2. **L2 Regularization** (1e-4 → 1e-3 → 1e-2): Penalizes large weights\n",
    "3. **Early Stopping**: Stop when validation loss stops improving\n",
    "4. **Batch Normalization**: Stabilizes training\n",
    "5. **Data Augmentation**: If applicable to your domain\n",
    "\n",
    "✅ **Systematic Testing:**\n",
    "- Try one technique at a time\n",
    "- Start with mild regularization, increase gradually\n",
    "- Keep track of train/val gap for each experiment\n",
    "- Test combinations of successful techniques\n",
    "\n",
    "✅ **Success Criteria:**\n",
    "- Validation loss improves\n",
    "- Train/val gap reduces to reasonable level (< 0.02-0.05)\n",
    "- Model still learns (training loss decreases)\n",
    "\n",
    "❌ **Common Mistakes:**\n",
    "- Applying too much regularization at once\n",
    "- Not testing techniques systematically\n",
    "- Focusing only on training loss\n",
    "- Giving up too early\n",
    "\n",
    "---\n",
    "\n",
    "#### **🎯 When to Use This Recipe:**\n",
    "\n",
    "**✅ Perfect for:**\n",
    "- New deep learning projects\n",
    "- Complex architectures (CNNs, RNNs, Transformers)\n",
    "- Limited training data scenarios\n",
    "- Industrial applications where reliability is crucial\n",
    "\n",
    "**⚠️ Consider alternatives for:**\n",
    "- Very simple problems (linear regression, etc.)\n",
    "- When you have infinite data\n",
    "- Extremely time-sensitive applications\n",
    "\n",
    "---\n",
    "\n",
    "#### **🏭 Industrial Control Applications:**\n",
    "\n",
    "**Process Control:**\n",
    "- Phase 1: Complex LSTM with many layers for sensor fusion\n",
    "- Phase 2: Add dropout to prevent memorizing sensor noise\n",
    "\n",
    "**Predictive Maintenance:**\n",
    "- Phase 1: Deep CNN for vibration pattern recognition  \n",
    "- Phase 2: Early stopping to prevent overfitting to specific machines\n",
    "\n",
    "**Quality Control:**\n",
    "- Phase 1: Large neural network for defect detection\n",
    "- Phase 2: L2 regularization to ensure generalization across product batches\n",
    "\n",
    "---\n",
    "\n",
    "**💡 Remember: This recipe ensures you never waste time with underpowered models. If your model can't overfit, it definitely can't generalize!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36de6d8",
   "metadata": {},
   "source": [
    "## 🎯 Key Takeaways and Best Practices\n",
    "\n",
    "### **🏆 THE GOLDEN RULE: Design Recipe for Deep Learning**\n",
    "\n",
    "> **\"First get a model that can overfit, then reduce overfitting\"**\n",
    "\n",
    "This fundamental principle ensures:\n",
    "- ✅ Your model has sufficient capacity to learn\n",
    "- ✅ You focus on the right problem (generalization vs capacity)\n",
    "- ✅ Systematic approach leads to better results\n",
    "- ✅ Clear diagnostics guide your decisions\n",
    "\n",
    "---\n",
    "\n",
    "### **Understanding Overfitting and Underfitting:**\n",
    "\n",
    "#### **🔴 UNDERFITTING (High Bias, Low Variance):**\n",
    "- **Symptoms**: High training and validation loss, both curves plateau at high values\n",
    "- **Causes**: Model too simple, insufficient training, high regularization\n",
    "- **Solutions**: \n",
    "  - Increase model complexity (more layers/neurons)\n",
    "  - Reduce regularization strength\n",
    "  - Train for more epochs\n",
    "  - Use more sophisticated architectures\n",
    "\n",
    "#### **🔴 OVERFITTING (Low Bias, High Variance):**\n",
    "- **Symptoms**: Low training loss, high validation loss, large gap between them\n",
    "- **Causes**: Model too complex, insufficient data, no regularization\n",
    "- **Solutions**:\n",
    "  - Add dropout layers\n",
    "  - Apply L1/L2 regularization\n",
    "  - Use early stopping\n",
    "  - Collect more training data\n",
    "  - Reduce model complexity\n",
    "\n",
    "#### **🟢 GOOD FIT (Balanced Bias-Variance):**\n",
    "- **Symptoms**: Both losses converge to low values, small gap between them\n",
    "- **Characteristics**: Model generalizes well to unseen data\n",
    "\n",
    "### **Regularization Techniques Summary:**\n",
    "\n",
    "| **Technique** | **How it Works** | **When to Use** | **PyTorch Implementation** |\n",
    "|---------------|------------------|-----------------|---------------------------|\n",
    "| **Dropout** | Randomly sets neurons to zero during training | High variance, complex models | `nn.Dropout(p=0.3)` |\n",
    "| **L2 Regularization** | Penalizes large weights | Prevent weight explosion | `weight_decay` in optimizer |\n",
    "| **L1 Regularization** | Promotes sparse weights | Feature selection needed | Custom loss term |\n",
    "| **Early Stopping** | Stops training when validation loss increases | Prevent overtraining | Monitor validation loss |\n",
    "| **Batch Normalization** | Normalizes layer inputs | Stabilize training | `nn.BatchNorm1d()` |\n",
    "\n",
    "### **Model Development Workflow:**\n",
    "\n",
    "1. **🚀 Phase 1: Build Overfitting Capability**\n",
    "   - Create complex architecture\n",
    "   - No regularization initially\n",
    "   - Train until clear overfitting\n",
    "   - Verify sufficient capacity\n",
    "\n",
    "2. **🔧 Phase 2: Systematic Regularization**\n",
    "   - Apply techniques one by one\n",
    "   - Monitor train/validation gap\n",
    "   - Find optimal balance\n",
    "   - Test combinations\n",
    "\n",
    "3. **✅ Validation and Selection**\n",
    "   - Choose based on validation performance\n",
    "   - Consider train/val gap\n",
    "   - Test on hold-out set\n",
    "\n",
    "### **Industrial Applications:**\n",
    "\n",
    "This knowledge is crucial for:\n",
    "- **Process Control**: Models that predict equipment behavior\n",
    "- **Quality Prediction**: Ensuring models generalize to new production batches  \n",
    "- **Predictive Maintenance**: Balancing sensitivity and false alarms\n",
    "- **Optimization**: Models that work across different operating conditions\n",
    "\n",
    "### **Experiment Suggestions:**\n",
    "\n",
    "Try the interactive explorer with these scenarios:\n",
    "1. **Small dataset (50 samples) + Complex model** → Observe overfitting\n",
    "2. **Large dataset (1000 samples) + Simple model** → Observe underfitting  \n",
    "3. **Medium dataset + High dropout (0.7)** → See regularization effect\n",
    "4. **High noise (0.4) + Various complexities** → Study noise robustness\n",
    "\n",
    "---\n",
    "\n",
    "**This tutorial demonstrates the fundamental trade-offs in machine learning and provides a systematic approach (the design recipe) to build well-generalized models for industrial applications. The sine function y = 3*sin(x) provides a clear demonstration of overfitting and underfitting concepts while the design recipe ensures you always start with sufficient model capacity.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
