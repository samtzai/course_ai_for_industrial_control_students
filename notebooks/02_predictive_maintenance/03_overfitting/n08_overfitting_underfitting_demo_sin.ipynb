{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c88f1831",
   "metadata": {},
   "source": [
    "# üéØ Overfitting and Underfitting Demonstration\n",
    "\n",
    "## **Neural Networks Learning to Approximate the Sine Function**\n",
    "\n",
    "### **Main Goal:**\n",
    "Learn to identify, understand, and prevent overfitting and underfitting in neural networks by training models to approximate the mathematical function y = sin(x).\n",
    "\n",
    "### **Key Learning Objectives:**\n",
    "- **Understand Model Complexity**: Learn how model complexity affects learning capacity\n",
    "- **Identify Overfitting**: Recognize when models memorize training data but fail to generalize\n",
    "- **Identify Underfitting**: Recognize when models are too simple to capture underlying patterns\n",
    "- **Regularization Techniques**: Apply dropout, L1/L2 regularization, early stopping, and batch normalization\n",
    "- **Performance Evaluation**: Use validation curves and visualization to assess model quality\n",
    "\n",
    "### **Why the Sine Function?**\n",
    "The sine function is perfect for this demonstration because:\n",
    "- **Known Ground Truth**: We know the exact mathematical relationship\n",
    "- **Non-Linear**: Requires neural networks to learn complex patterns\n",
    "- **Smooth Function**: Easy to visualize and understand deviations\n",
    "- **Controllable Noise**: We can add controlled amounts of noise to simulate real-world data\n",
    "\n",
    "### **Interactive Features:**\n",
    "üéÆ Model complexity explorer | üìä Real-time training visualization | ‚ö° Regularization parameter tuning | üî¨ Performance comparison dashboard\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997cc4a0",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll use PyTorch for neural networks, NumPy for data manipulation, and Matplotlib for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0dd6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-2 * np.pi, 2 * np.pi, 200)\n",
    "# Updated true function\n",
    "y_true = 3 * np.sin(X)\n",
    "noise = np.random.normal(0, 0.7, size=X.shape)\n",
    "y = y_true + noise\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(X, y_true, label='True function', color='black', linewidth=2)\n",
    "plt.scatter(X, y, label='Noisy data', alpha=0.5)\n",
    "plt.title('Simple True Function: $3 \\sin(x)$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b9168",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Dataset\n",
    "\n",
    "Let's create a dataset based on the sine function with controllable noise levels. This will allow us to demonstrate how different amounts of training data and noise affect overfitting and underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bace030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive dataset for training and testing\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate training data (smaller sample)\n",
    "X_sample = np.linspace(-2*np.pi, 2*np.pi, 50).reshape(-1, 1)\n",
    "y_true_sample = 3 * np.sin(X_sample.flatten())\n",
    "noise_sample = np.random.normal(0, 0.5, size=y_true_sample.shape)\n",
    "y_sample = y_true_sample + noise_sample\n",
    "\n",
    "# Generate dense test data for smooth visualization\n",
    "X_dense = np.linspace(-2*np.pi, 2*np.pi, 200).reshape(-1, 1)\n",
    "y_true_dense = 3 * np.sin(X_dense.flatten())\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(X_dense, y_true_dense, 'k-', linewidth=2, label='True Function')\n",
    "plt.scatter(X_sample, y_sample, alpha=0.7, s=40, color='blue', label='Training Data')\n",
    "plt.title('Training Dataset vs True Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(X, y_true, 'k-', linewidth=2, label='True Function')\n",
    "plt.scatter(X, y, alpha=0.5, s=20, color='orange', label='Noisy Data')\n",
    "plt.title('Full Dataset: $3\\\\sin(x)$')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ DATASET CHARACTERISTICS:\")\n",
    "print(f\"‚Ä¢ Training samples: {len(X_sample)}\")\n",
    "print(f\"‚Ä¢ Full dataset: {len(X)}\")\n",
    "print(f\"‚Ä¢ Function: 3*sin(x)\")\n",
    "print(f\"‚Ä¢ Noise level: œÉ = 0.5-0.7\")\n",
    "print(\"‚Ä¢ Challenge: Learn simple sine function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bc933a",
   "metadata": {},
   "source": [
    "## 3. Create Neural Network Models\n",
    "\n",
    "Let's define neural network architectures with different complexities to demonstrate underfitting, good fit, and overfitting scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c49baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics for model performance\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate various performance metrics\"\"\"\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    # R-squared (coefficient of determination)\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse, \n",
    "        'MAE': mae,\n",
    "        'R¬≤': r2\n",
    "    }\n",
    "\n",
    "# Example: Simple baseline predictions\n",
    "# Linear baseline (underfitting example)\n",
    "linear_coeffs = np.polyfit(X.flatten(), y, 1)  # Fit linear model\n",
    "y_pred_linear = np.polyval(linear_coeffs, X)\n",
    "\n",
    "# Neural Network (overfitting example) - Train a very complex NN on limited data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class OverfittingNet(nn.Module):\n",
    "    \"\"\"Very complex network designed to overfit with limited data\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Train overfitting network on limited data (only 30 points)\n",
    "np.random.seed(42)\n",
    "X_limited = np.linspace(-2*np.pi, 2*np.pi, 30).reshape(-1, 1)  # Very limited data\n",
    "y_true_limited = 3 * np.sin(X_limited.flatten())\n",
    "noise_limited = np.random.normal(0, 0.3, size=y_true_limited.shape)\n",
    "y_limited = y_true_limited + noise_limited\n",
    "\n",
    "# Convert to tensors\n",
    "X_limited_tensor = torch.FloatTensor(X_limited).to(device)\n",
    "y_limited_tensor = torch.FloatTensor(y_limited.reshape(-1, 1)).to(device)\n",
    "X_tensor = torch.FloatTensor(X.reshape(-1, 1)).to(device)\n",
    "\n",
    "# Train the overfitting network\n",
    "overfitting_model = OverfittingNet().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(overfitting_model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training overfitting neural network...\")\n",
    "for epoch in range(1000):  # Train for many epochs to ensure overfitting\n",
    "    optimizer.zero_grad()\n",
    "    pred = overfitting_model(X_limited_tensor)\n",
    "    loss = criterion(pred, y_limited_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Get predictions on full range\n",
    "overfitting_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_nn = overfitting_model(X_tensor).cpu().numpy().flatten()\n",
    "\n",
    "print(\"‚úÖ Overfitting neural network trained!\")\n",
    "\n",
    "# Calculate metrics\n",
    "linear_metrics = calculate_metrics(y_true, y_pred_linear)\n",
    "nn_metrics = calculate_metrics(y_true, y_pred_nn)\n",
    "\n",
    "print(\"üìä BASELINE MODEL COMPARISON:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Linear Model (Underfitting):\")\n",
    "for metric, value in linear_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(f\"\\nNeural Network (Overfitting):\")\n",
    "for metric, value in nn_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Visualize baseline models\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(X, y_true, 'k-', linewidth=2, label='True Function')\n",
    "plt.scatter(X, y, alpha=0.5, s=20, color='lightgray', label='Noisy Data')\n",
    "plt.plot(X, y_pred_linear, 'b--', linewidth=2, label='Linear Fit')\n",
    "plt.title('Linear Model - Underfitting')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(X, y_true, 'k-', linewidth=2, label='True Function')\n",
    "plt.scatter(X, y, alpha=0.5, s=20, color='lightgray', label='Full Noisy Data')\n",
    "plt.scatter(X_limited, y_limited, alpha=0.8, s=60, color='red', label='Limited Training Data', marker='x')\n",
    "plt.plot(X, y_pred_nn, 'r--', linewidth=2, label='Neural Network')\n",
    "plt.title('Neural Network - Overfitting')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(X, y_true, 'k-', linewidth=3, label='True Function', alpha=0.9)\n",
    "plt.plot(X, y_pred_linear, 'b--', linewidth=2, label='Linear (Underfit)', alpha=0.8)\n",
    "plt.plot(X, y_pred_nn, 'r--', linewidth=2, label='Neural Net (Overfit)', alpha=0.8)\n",
    "plt.scatter(X_limited, y_limited, alpha=0.9, s=40, color='red', label='Training Data', marker='o')\n",
    "plt.title('Comparison: Under vs Over-fitting')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a8c6d",
   "metadata": {},
   "source": [
    "## 4. Train Models with Different Complexities\n",
    "\n",
    "Now let's train each model and track their performance to observe overfitting and underfitting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, epochs=300, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model and track training/validation loss\n",
    "    \n",
    "    Parameters:\n",
    "    - model: PyTorch model to train\n",
    "    - X_train, y_train: Training data\n",
    "    - X_val, y_val: Validation data\n",
    "    - epochs: Number of training epochs\n",
    "    - lr: Learning rate\n",
    "    \n",
    "    Returns:\n",
    "    - model: Trained model\n",
    "    - train_losses: List of training losses\n",
    "    - val_losses: List of validation losses\n",
    "    \"\"\"\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val.reshape(-1, 1)).to(device)\n",
    "    \n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Track losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        # Training step\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model(X_train_tensor)\n",
    "        train_loss = criterion(train_pred, y_train_tensor)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val_tensor)\n",
    "            val_loss = criterion(val_pred, y_val_tensor)\n",
    "        model.train()\n",
    "        \n",
    "        # Store losses\n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        # Print progress every 50 epochs\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss.item():.6f}, Val Loss: {val_loss.item():.6f}\")\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Define neural network models with different complexities\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"Simple network - may underfit\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class OptimalNet(nn.Module):\n",
    "    \"\"\"Optimal network - good balance\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class ComplexNet(nn.Module):\n",
    "    \"\"\"Complex network - may overfit\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Generate data for training\n",
    "np.random.seed(42)\n",
    "X_data = np.linspace(-2*np.pi, 2*np.pi, 100).reshape(-1, 1)\n",
    "y_true_data = 3 * np.sin(X_data.flatten())\n",
    "noise = np.random.normal(0, 0.5, size=y_true_data.shape)\n",
    "y_data = y_true_data + noise\n",
    "\n",
    "# Split data into train/validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"TRAINING ALL MODELS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Train models\n",
    "simple_model, simple_train_losses, simple_val_losses = train_model(\n",
    "    SimpleNet(), X_train, y_train, X_val, y_val, epochs=300\n",
    ")\n",
    "\n",
    "optimal_model, optimal_train_losses, optimal_val_losses = train_model(\n",
    "    OptimalNet(), X_train, y_train, X_val, y_val, epochs=300\n",
    ")\n",
    "\n",
    "complex_model, complex_train_losses, complex_val_losses = train_model(\n",
    "    ComplexNet(), X_train, y_train, X_val, y_val, epochs=300\n",
    ")\n",
    "\n",
    "# Store training histories for visualization\n",
    "training_histories = {\n",
    "    'Simple Model': {'train_losses': simple_train_losses, 'val_losses': simple_val_losses},\n",
    "    'Optimal Model': {'train_losses': optimal_train_losses, 'val_losses': optimal_val_losses},\n",
    "    'Complex Model': {'train_losses': complex_train_losses, 'val_losses': complex_val_losses}\n",
    "}\n",
    "\n",
    "print(\"‚úÖ All models trained successfully!\")\n",
    "print(f\"Training data points: {len(X_train)}\")\n",
    "print(f\"Validation data points: {len(X_val)}\")\n",
    "\n",
    "# Define neural network models with different complexities\n",
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"Simple network - may underfit\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class OptimalNet(nn.Module):\n",
    "    \"\"\"Optimal network - good balance\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class ComplexNet(nn.Module):\n",
    "    \"\"\"Complex network - may overfit\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Prepare training data from the samples we created earlier\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"TRAINING NEURAL NETWORK MODELS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create and train models\n",
    "models = {\n",
    "    'Simple (Underfitting)': SimpleNet(),\n",
    "    'Optimal': OptimalNet(), \n",
    "    'Complex (Overfitting)': ComplexNet()\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "training_histories = {}\n",
    "\n",
    "# Train each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîÑ Training {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    trained_model, train_losses, val_losses = train_model(\n",
    "        model, X_train, y_train, X_val, y_val, epochs=200, lr=0.01\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    trained_models[name] = trained_model\n",
    "    training_histories[name] = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {name} training completed!\")\n",
    "\n",
    "print(\"\\nüèÅ All models trained successfully!\")\n",
    "print(\"\\nFinal Training Losses:\")\n",
    "for name, history in training_histories.items():\n",
    "    final_train_loss = history['train_losses'][-1]\n",
    "    final_val_loss = history['val_losses'][-1]\n",
    "    overfitting_gap = final_val_loss - final_train_loss\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Train Loss: {final_train_loss:.6f}\")\n",
    "    print(f\"    Val Loss:   {final_val_loss:.6f}\")\n",
    "    print(f\"    Gap:        {overfitting_gap:.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7a6bd0",
   "metadata": {},
   "source": [
    "## 5. Implement Regularization Techniques\n",
    "\n",
    "Let's explore various regularization methods to prevent overfitting: dropout, L1/L2 regularization, early stopping, and batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c2c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple regularization demonstration\n",
    "class RegularizedNet(nn.Module):\n",
    "    \"\"\"Network with dropout for regularization\"\"\"\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "print(\"üîß REGULARIZATION DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Train a regularized model for comparison\n",
    "print(\"Training regularized model with dropout...\")\n",
    "regularized_model = RegularizedNet(dropout_rate=0.5)\n",
    "reg_model, reg_train_losses, reg_val_losses = train_model(\n",
    "    regularized_model, X_train, y_train, X_val, y_val, epochs=200, lr=0.01\n",
    ")\n",
    "\n",
    "# Compare final losses\n",
    "final_reg_train = reg_train_losses[-1]\n",
    "final_reg_val = reg_val_losses[-1]\n",
    "reg_gap = final_reg_val - final_reg_train\n",
    "\n",
    "print(f\"\\nüìä REGULARIZED MODEL RESULTS:\")\n",
    "print(f\"  Train Loss: {final_reg_train:.6f}\")\n",
    "print(f\"  Val Loss:   {final_reg_val:.6f}\")\n",
    "print(f\"  Gap:        {reg_gap:.6f}\")\n",
    "\n",
    "print(\"\\nüí° KEY INSIGHTS:\")\n",
    "print(\"‚Ä¢ Dropout randomly sets neurons to zero during training\")\n",
    "print(\"‚Ä¢ This prevents the model from memorizing specific patterns\")\n",
    "print(\"‚Ä¢ Results in better generalization to new data\")\n",
    "print(\"‚Ä¢ Lower overfitting gap indicates better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd6eeb2",
   "metadata": {},
   "source": [
    "## 6. Visualize Training and Validation Loss\n",
    "\n",
    "Let's plot training and validation loss curves to clearly see overfitting and underfitting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbd3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learning curves for all models\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Training curves comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "for name, history in training_histories.items():\n",
    "    epochs = range(len(history['train_losses']))\n",
    "    plt.plot(epochs, history['train_losses'], '-', label=f'{name} Train', linewidth=2)\n",
    "    plt.plot(epochs, history['val_losses'], '--', label=f'{name} Val', linewidth=2)\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 2: Final loss comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "model_names = list(training_histories.keys())\n",
    "train_final = [training_histories[name]['train_losses'][-1] for name in model_names]\n",
    "val_final = [training_histories[name]['val_losses'][-1] for name in model_names]\n",
    "\n",
    "x_pos = range(len(model_names))\n",
    "plt.bar([x - 0.2 for x in x_pos], train_final, 0.4, label='Train Loss', alpha=0.7)\n",
    "plt.bar([x + 0.2 for x in x_pos], val_final, 0.4, label='Val Loss', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Final Loss')\n",
    "plt.title('Final Loss Comparison')\n",
    "plt.xticks(x_pos, [name.split()[0] for name in model_names], rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Overfitting gap\n",
    "plt.subplot(1, 3, 3)\n",
    "gaps = [val_final[i] - train_final[i] for i in range(len(model_names))]\n",
    "colors = ['blue' if gap < 0.01 else 'orange' if gap < 0.05 else 'red' for gap in gaps]\n",
    "\n",
    "plt.bar(x_pos, gaps, color=colors, alpha=0.7)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Overfitting Gap (Val - Train)')\n",
    "plt.title('Overfitting Analysis')\n",
    "plt.xticks(x_pos, [name.split()[0] for name in model_names], rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show key insights about overfitting/underfitting\n",
    "print(\"üìà TRAINING CURVE ANALYSIS:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"üîç Underfitting (Simple Model):\")\n",
    "print(\"   ‚Ä¢ High training AND validation loss\")\n",
    "print(\"   ‚Ä¢ Both curves plateau at high values\")\n",
    "print(\"   ‚Ä¢ Model lacks capacity to learn patterns\")\n",
    "\n",
    "print(\"\\nüîç Good Fit (Optimal Model):\")\n",
    "print(\"   ‚Ä¢ Low training and validation loss\")\n",
    "print(\"   ‚Ä¢ Small gap between train/val curves\")\n",
    "print(\"   ‚Ä¢ Good generalization\")\n",
    "\n",
    "print(\"\\nüîç Overfitting (Complex Model):\")\n",
    "print(\"   ‚Ä¢ Very low training loss\")\n",
    "print(\"   ‚Ä¢ Higher validation loss\")\n",
    "print(\"   ‚Ä¢ Large gap indicates poor generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6897ec7",
   "metadata": {},
   "source": [
    "## 7. Compare Model Performance\n",
    "\n",
    "Let's visualize how well each model learned to approximate the sine function by comparing their predictions against the true function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ef722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Ensure we have consistent data\n",
    "np.random.seed(42)\n",
    "X_data = np.linspace(-2*np.pi, 2*np.pi, 100).reshape(-1, 1)\n",
    "y_true_data = 3 * np.sin(X_data.flatten())\n",
    "noise = np.random.normal(0, 0.5, size=y_true_data.shape)\n",
    "y_data = y_true_data + noise\n",
    "\n",
    "# Split data into train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define simple neural network models\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class OptimalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class ComplexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_simple_model(model, epochs=300):\n",
    "    \"\"\"Train a model and return training history\"\"\"\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train.reshape(-1, 1)).to(device)\n",
    "    X_val_tensor = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_tensor = torch.FloatTensor(y_val.reshape(-1, 1)).to(device)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model(X_train_tensor)\n",
    "        train_loss = criterion(train_pred, y_train_tensor)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val_tensor)\n",
    "            val_loss = criterion(val_pred, y_val_tensor)\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Train models\n",
    "print(\"Training models...\")\n",
    "simple_model, simple_train_loss, simple_val_loss = train_simple_model(SimpleNet())\n",
    "optimal_model, optimal_train_loss, optimal_val_loss = train_simple_model(OptimalNet())\n",
    "complex_model, complex_train_loss, complex_val_loss = train_simple_model(ComplexNet())\n",
    "\n",
    "print(\"‚úÖ All models trained!\")\n",
    "\n",
    "# Evaluate models on test data\n",
    "X_test = np.linspace(-2*np.pi, 2*np.pi, 200).reshape(-1, 1)\n",
    "y_test_true = 3 * np.sin(X_test.flatten())\n",
    "\n",
    "# Get predictions\n",
    "models = {\n",
    "    'Simple (Underfitting)': simple_model,\n",
    "    'Optimal': optimal_model, \n",
    "    'Complex (Overfitting)': complex_model\n",
    "}\n",
    "\n",
    "predictions = {}\n",
    "for name, model in models.items():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "        pred = model(X_test_tensor).cpu().numpy().flatten()\n",
    "        predictions[name] = pred\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7, 5))\n",
    "\n",
    "# Plot 1: Training curves\n",
    "ax1 = axes[0, 0]\n",
    "epochs = range(len(simple_train_loss))\n",
    "ax1.plot(epochs, simple_train_loss, 'b-', label='Simple Train', alpha=0.7)\n",
    "ax1.plot(epochs, simple_val_loss, 'b--', label='Simple Val', alpha=0.7)\n",
    "ax1.plot(epochs, optimal_train_loss, 'g-', label='Optimal Train', alpha=0.7)\n",
    "ax1.plot(epochs, optimal_val_loss, 'g--', label='Optimal Val', alpha=0.7)\n",
    "ax1.plot(epochs, complex_train_loss, 'r-', label='Complex Train', alpha=0.7)\n",
    "ax1.plot(epochs, complex_val_loss, 'r--', label='Complex Val', alpha=0.7)\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Plots 2-4: Model predictions\n",
    "colors = ['blue', 'green', 'red']\n",
    "for i, (name, pred) in enumerate(predictions.items()):\n",
    "    ax = axes[0, 1] if i == 0 else axes[1, i-1]\n",
    "    \n",
    "    # Plot true function and data\n",
    "    ax.plot(X_test, y_test_true, 'k-', linewidth=2, label='True Function', alpha=0.8)\n",
    "    ax.scatter(X_train, y_train, alpha=0.5, s=20, color='lightgray', label='Training Data')\n",
    "    \n",
    "    # Plot prediction\n",
    "    ax.plot(X_test, pred, color=colors[i], linewidth=2, linestyle='--', label=f'{name}')\n",
    "    \n",
    "    # Calculate and display MSE\n",
    "    mse = np.mean((pred - y_test_true)**2)\n",
    "    r2 = r2_score(y_test_true, pred)\n",
    "    \n",
    "    ax.text(0.02, 0.98, f'MSE: {mse:.3f}\\nR¬≤: {r2:.3f}', \n",
    "            transform=ax.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "            fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f'{name} Predictions')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-2*np.pi, 2*np.pi)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance summary\n",
    "print(\"\\nüìä MODEL PERFORMANCE SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "for name, pred in predictions.items():\n",
    "    mse = np.mean((pred - y_test_true)**2)\n",
    "    r2 = r2_score(y_test_true, pred)\n",
    "    final_train = models[name] \n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  R¬≤:  {r2:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"üîç Simple Model: May underfit - cannot capture complexity\")\n",
    "print(\"üîç Optimal Model: Good balance between bias and variance\") \n",
    "print(\"üîç Complex Model: May overfit - high variance, memorizes noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b0856",
   "metadata": {},
   "source": [
    "## 8. Interactive Model Explorer\n",
    "\n",
    "Let's create an interactive tool to explore how different hyperparameters affect overfitting and underfitting in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for interactive widgets\n",
    "try:\n",
    "    from ipywidgets import interact, widgets\n",
    "    from IPython.display import display\n",
    "    from sklearn.metrics import r2_score\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è ipywidgets not available. Interactive features will be disabled.\")\n",
    "    try:\n",
    "        from sklearn.metrics import r2_score\n",
    "        WIDGETS_AVAILABLE = False\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è sklearn not available. Some features will be disabled.\")\n",
    "        def r2_score(y_true, y_pred):\n",
    "            return 0.0  # Fallback function\n",
    "        WIDGETS_AVAILABLE = False\n",
    "\n",
    "def generate_sine_data(n_samples=200, noise_level=0.1, random_state=None):\n",
    "    \"\"\"Generate synthetic sine data for training\"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    X = np.linspace(-2*np.pi, 2*np.pi, n_samples).reshape(-1, 1)\n",
    "    y_true = 3 * np.sin(X.flatten())\n",
    "    noise = np.random.normal(0, noise_level, size=y_true.shape)\n",
    "    y = y_true + noise\n",
    "    return X, y, y_true\n",
    "\n",
    "class InteractiveModelExplorer:\n",
    "    \"\"\"Interactive tool for exploring model complexity and regularization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.current_model = None\n",
    "        self.training_history = None\n",
    "    \n",
    "    def create_and_train_model(self, hidden_size, num_layers, dropout_rate, l2_reg, \n",
    "                              learning_rate, epochs, dataset_size, noise_level):\n",
    "        \"\"\"Create and train a model with specified parameters\"\"\"\n",
    "        \n",
    "        # Generate dataset with specified parameters - use different random seed each time\n",
    "        import time\n",
    "        random_seed = int(time.time()) % 10000  # Different seed each time\n",
    "        X_data, y_data, _ = generate_sine_data(\n",
    "            n_samples=dataset_size, \n",
    "            noise_level=noise_level,\n",
    "            random_state=random_seed\n",
    "        )\n",
    "        \n",
    "        # Split data\n",
    "        X_train_inter, X_val_inter, y_train_inter, y_val_inter = train_test_split(\n",
    "            X_data, y_data, test_size=0.2, random_state=random_seed\n",
    "        )\n",
    "        \n",
    "        # Create model architecture - FIX: Define class properly with parameters\n",
    "        class DynamicNet(nn.Module):\n",
    "            def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_rate):\n",
    "                super(DynamicNet, self).__init__()\n",
    "                \n",
    "                layers = []\n",
    "                current_size = input_size\n",
    "                \n",
    "                # First layer\n",
    "                layers.append(nn.Linear(current_size, hidden_size))\n",
    "                layers.append(nn.ReLU())\n",
    "                if dropout_rate > 0:\n",
    "                    layers.append(nn.Dropout(dropout_rate))\n",
    "                current_size = hidden_size\n",
    "                \n",
    "                # Hidden layers\n",
    "                for _ in range(max(0, num_layers - 2)):  # Ensure we don't get negative layers\n",
    "                    layers.append(nn.Linear(current_size, hidden_size))\n",
    "                    layers.append(nn.ReLU())\n",
    "                    if dropout_rate > 0:\n",
    "                        layers.append(nn.Dropout(dropout_rate))\n",
    "                \n",
    "                # Output layer\n",
    "                layers.append(nn.Linear(current_size, output_size))\n",
    "                \n",
    "                self.network = nn.Sequential(*layers)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                return self.network(x)\n",
    "        \n",
    "        # Create and train model - FIX: Pass parameters explicitly\n",
    "        model = DynamicNet(\n",
    "            input_size=1, \n",
    "            hidden_size=hidden_size, \n",
    "            output_size=1, \n",
    "            num_layers=num_layers, \n",
    "            dropout_rate=dropout_rate\n",
    "        ).to(device)\n",
    "        \n",
    "        # Training setup\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_reg)\n",
    "        \n",
    "        # Convert to tensors - FIX: Proper tensor shapes\n",
    "        X_train_tensor = torch.FloatTensor(X_train_inter).to(device)\n",
    "        y_train_tensor = torch.FloatTensor(y_train_inter.reshape(-1, 1)).to(device)  # FIX: Reshape y\n",
    "        X_val_tensor = torch.FloatTensor(X_val_inter).to(device)\n",
    "        y_val_tensor = torch.FloatTensor(y_val_inter.reshape(-1, 1)).to(device)  # FIX: Reshape y\n",
    "        \n",
    "        # Training loop\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            # Training step\n",
    "            optimizer.zero_grad()\n",
    "            train_pred = model(X_train_tensor)\n",
    "            train_loss = criterion(train_pred, y_train_tensor)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Validation step\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_pred = model(X_val_tensor)\n",
    "                val_loss = criterion(val_pred, y_val_tensor)\n",
    "            model.train()\n",
    "            \n",
    "            train_losses.append(train_loss.item())\n",
    "            val_losses.append(val_loss.item())\n",
    "        \n",
    "        # Store results\n",
    "        self.current_model = model\n",
    "        self.training_history = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'X_train': X_train_inter,\n",
    "            'y_train': y_train_inter,\n",
    "            'X_val': X_val_inter,\n",
    "            'y_val': y_val_inter\n",
    "        }\n",
    "        \n",
    "        return model, train_losses, val_losses, X_train_inter, y_train_inter\n",
    "    \n",
    "    def plot_results(self, model, train_losses, val_losses, X_train_data, y_train_data):\n",
    "        \"\"\"Plot training curves and model predictions\"\"\"\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(7, 5))\n",
    "        \n",
    "        # 1. Training curves\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        ax1.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n",
    "        ax1.plot(epochs, val_losses, 'r--', label='Validation Loss', linewidth=2)\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss (MSE)')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_yscale('log')\n",
    "        \n",
    "        # Add overfitting detection\n",
    "        final_train_loss = train_losses[-1]\n",
    "        final_val_loss = val_losses[-1]\n",
    "        overfitting_gap = final_val_loss - final_train_loss\n",
    "        \n",
    "        if overfitting_gap > 0.05:\n",
    "            ax1.text(0.6, 0.9, '‚ö†Ô∏è OVERFITTING', transform=ax1.transAxes,\n",
    "                    bbox=dict(boxstyle='round', facecolor='red', alpha=0.3),\n",
    "                    fontsize=12, fontweight='bold')\n",
    "        elif final_train_loss > 0.1:\n",
    "            ax1.text(0.6, 0.9, '‚ö†Ô∏è UNDERFITTING', transform=ax1.transAxes,\n",
    "                    bbox=dict(boxstyle='round', facecolor='orange', alpha=0.3),\n",
    "                    fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            ax1.text(0.6, 0.9, '‚úÖ GOOD FIT', transform=ax1.transAxes,\n",
    "                    bbox=dict(boxstyle='round', facecolor='green', alpha=0.3),\n",
    "                    fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # 2. Model predictions\n",
    "        X_test_range = np.linspace(-2*np.pi, 2*np.pi, 200).reshape(-1, 1)\n",
    "        y_test_true = 3 * np.sin(X_test_range.flatten())\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.FloatTensor(X_test_range).to(device)\n",
    "            y_pred = model(X_test_tensor).cpu().numpy().flatten()  # FIX: Flatten for plotting\n",
    "        \n",
    "        ax2.scatter(X_train_data, y_train_data, alpha=0.6, s=30, \n",
    "                   color='lightblue', label='Training Data')\n",
    "        ax2.plot(X_test_range, y_test_true, 'k-', linewidth=3, \n",
    "                label='True Function', alpha=0.8)\n",
    "        ax2.plot(X_test_range, y_pred, 'r--', linewidth=2, \n",
    "                label='Model Prediction')\n",
    "        \n",
    "        ax2.set_xlabel('x')\n",
    "        ax2.set_ylabel('y')\n",
    "        ax2.set_title('Model Prediction vs True Function')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_xlim(-2*np.pi, 2*np.pi)\n",
    "        ax2.set_ylim(-6, 6)\n",
    "        \n",
    "        # 3. Loss evolution details\n",
    "        if len(epochs) > 50:\n",
    "            ax3.plot(epochs[-50:], train_losses[-50:], 'b-', \n",
    "                    label='Training Loss (last 50 epochs)', linewidth=2)\n",
    "            ax3.plot(epochs[-50:], val_losses[-50:], 'r--', \n",
    "                    label='Validation Loss (last 50 epochs)', linewidth=2)\n",
    "        else:\n",
    "            ax3.plot(epochs, train_losses, 'b-', \n",
    "                    label='Training Loss', linewidth=2)\n",
    "            ax3.plot(epochs, val_losses, 'r--', \n",
    "                    label='Validation Loss', linewidth=2)\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Loss (MSE)')\n",
    "        ax3.set_title('Loss Evolution (Final Epochs)')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Performance metrics\n",
    "        mse_test = np.mean((y_pred - y_test_true)**2)\n",
    "        r2_test = r2_score(y_test_true, y_pred)\n",
    "        \n",
    "        metrics_text = f\"\"\"\n",
    "        Performance Metrics:\n",
    "        \n",
    "        Final Training Loss: {final_train_loss:.6f}\n",
    "        Final Validation Loss: {final_val_loss:.6f}\n",
    "        Overfitting Gap: {overfitting_gap:.6f}\n",
    "        \n",
    "        Test MSE: {mse_test:.6f}\n",
    "        Test R¬≤: {r2_test:.4f}\n",
    "        \n",
    "        Model Parameters: {sum(p.numel() for p in model.parameters()):,}\n",
    "        \"\"\"\n",
    "        \n",
    "        ax4.text(0.1, 0.9, metrics_text, transform=ax4.transAxes,\n",
    "                fontsize=11, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "        ax4.set_xlim(0, 1)\n",
    "        ax4.set_ylim(0, 1)\n",
    "        ax4.axis('off')\n",
    "        ax4.set_title('Performance Summary')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'final_train_loss': final_train_loss,\n",
    "            'final_val_loss': final_val_loss,\n",
    "            'overfitting_gap': overfitting_gap,\n",
    "            'test_mse': mse_test,\n",
    "            'test_r2': r2_test\n",
    "        }\n",
    "\n",
    "# Create explorer instance\n",
    "explorer = InteractiveModelExplorer()\n",
    "\n",
    "def interactive_model_training(hidden_size=50, num_layers=2, dropout_rate=0.0, \n",
    "                             l2_reg=0.0001, learning_rate=0.01, epochs=200,\n",
    "                             dataset_size=200, noise_level=0.1):\n",
    "    \"\"\"Interactive function for model training and visualization\"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Training model with parameters:\")\n",
    "    print(f\"   Hidden Size: {hidden_size}\")\n",
    "    print(f\"   Layers: {num_layers}\")\n",
    "    print(f\"   Dropout: {dropout_rate}\")\n",
    "    print(f\"   L2 Regularization: {l2_reg}\")\n",
    "    print(f\"   Learning Rate: {learning_rate}\")\n",
    "    print(f\"   Epochs: {epochs}\")\n",
    "    print(f\"   Dataset Size: {dataset_size}\")\n",
    "    print(f\"   Noise Level: {noise_level}\")\n",
    "    print()\n",
    "    \n",
    "    # Train model\n",
    "    model, train_losses, val_losses, X_train_data, y_train_data = explorer.create_and_train_model(\n",
    "        hidden_size, num_layers, dropout_rate, l2_reg, learning_rate, epochs, dataset_size, noise_level\n",
    "    )\n",
    "    \n",
    "    # Plot results\n",
    "    metrics = explorer.plot_results(model, train_losses, val_losses, X_train_data, y_train_data)\n",
    "    \n",
    "    # Interpretation\n",
    "    print(\"INTERPRETATION:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    if metrics['overfitting_gap'] > 0.05:\n",
    "        print(\"üö® OVERFITTING DETECTED!\")\n",
    "        print(\"   Try: Reduce model complexity, increase dropout, add L2 regularization\")\n",
    "    elif metrics['final_train_loss'] > 0.1:\n",
    "        print(\"üö® UNDERFITTING DETECTED!\")\n",
    "        print(\"   Try: Increase model complexity, reduce regularization, train longer\")\n",
    "    else:\n",
    "        print(\"‚úÖ GOOD BALANCE!\")\n",
    "        print(\"   Model shows good generalization capability\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"üéÆ INTERACTIVE MODEL EXPLORER\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Experiment with different parameters to see their effects on overfitting and underfitting!\")\n",
    "print()\n",
    "\n",
    "# Create interactive widget only if widgets are available\n",
    "if WIDGETS_AVAILABLE:\n",
    "    try:\n",
    "        interact(interactive_model_training,\n",
    "                 hidden_size=widgets.IntSlider(value=50, min=5, max=200, step=5, \n",
    "                                              description='Hidden Size:'),\n",
    "                 num_layers=widgets.IntSlider(value=2, min=1, max=6, step=1, \n",
    "                                             description='# Layers:'),\n",
    "                 dropout_rate=widgets.FloatSlider(value=0.0, min=0.0, max=0.8, step=0.01, \n",
    "                                                 description='Dropout Rate:'),\n",
    "                 l2_reg=widgets.FloatLogSlider(value=0.0001, base=10, min=-6, max=-1, step=1, \n",
    "                                              description='L2 Reg:'),\n",
    "                 learning_rate=widgets.FloatLogSlider(value=0.01, base=10, min=-6, max=-1, step=1, \n",
    "                                                    description='Learning Rate:'),\n",
    "                 epochs=widgets.IntSlider(value=200, min=50, max=2000, step=100, \n",
    "                                         description='Epochs:'),\n",
    "                 dataset_size=widgets.IntSlider(value=200, min=50, max=1000, step=50, \n",
    "                                               description='Dataset Size:'),\n",
    "                 noise_level=widgets.FloatSlider(value=0.1, min=0.0, max=2.0, step=0.05, \n",
    "                                                description='Noise Level:'))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating interactive widget: {e}\")\n",
    "        print(\"üìù You can still run the function manually:\")\n",
    "        print(\"interactive_model_training(hidden_size=50, num_layers=2, dropout_rate=0.2)\")\n",
    "else:\n",
    "    print(\"üìù To use interactive features, install ipywidgets:\")\n",
    "    print(\"   pip install ipywidgets\")\n",
    "    print(\"   jupyter nbextension enable --py widgetsnbextension\")\n",
    "    print()\n",
    "    print(\"üìù You can still run the function manually:\")\n",
    "    print(\"interactive_model_training(hidden_size=50, num_layers=2, dropout_rate=0.2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed076061",
   "metadata": {},
   "source": [
    "## 9. üéØ Design Recipe: First Overfit, Then Reduce Overfitting\n",
    "\n",
    "### **The Golden Rule of Deep Learning Model Development**\n",
    "\n",
    "> **\"First get a model that can overfit, then reduce overfitting\"**\n",
    "> \n",
    "> *This approach ensures your model has sufficient capacity to learn before you constrain it.*\n",
    "\n",
    "### **Why This Recipe Works:**\n",
    "\n",
    "1. **üéØ Capacity Verification**: If your model can't overfit, it lacks the capacity to learn the underlying patterns\n",
    "2. **üîß Systematic Approach**: Start with maximum learning potential, then systematically constrain\n",
    "3. **üöÄ Faster Development**: Avoids the trap of underparameterized models that will never work\n",
    "4. **üìä Clear Diagnostics**: Makes it obvious whether you have a capacity or generalization problem\n",
    "\n",
    "### **The Step-by-Step Process:**\n",
    "\n",
    "#### **Phase 1: Build an Overfitting Model**\n",
    "- Use a complex architecture with many parameters\n",
    "- No regularization initially  \n",
    "- Train until you see clear overfitting\n",
    "- This proves your model CAN learn the patterns\n",
    "\n",
    "#### **Phase 2: Systematically Reduce Overfitting**\n",
    "- Add regularization techniques one by one\n",
    "- Monitor the training/validation gap\n",
    "- Find the sweet spot between capacity and generalization\n",
    "\n",
    "---\n",
    "\n",
    "Let's demonstrate this recipe in action! üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab405b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ DESIGN RECIPE DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Following the golden rule: 'First get a model that can overfit, then reduce overfitting'\")\n",
    "print()\n",
    "\n",
    "# Generate training data (intentionally small to make overfitting easier)\n",
    "np.random.seed(42)\n",
    "X_recipe = np.linspace(-2*np.pi, 2*np.pi, 50).reshape(-1, 1)  # Small dataset\n",
    "y_true_recipe = 3 * np.sin(X_recipe.flatten())\n",
    "noise_recipe = np.random.normal(0, 0.3, size=y_true_recipe.shape)\n",
    "y_recipe = y_true_recipe + noise_recipe\n",
    "\n",
    "# Split into train/val (intentionally small)\n",
    "X_train_recipe, X_val_recipe, y_train_recipe, y_val_recipe = train_test_split(\n",
    "    X_recipe, y_recipe, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä DATASET FOR RECIPE:\")\n",
    "print(f\"   Training samples: {len(X_train_recipe)}\")\n",
    "print(f\"   Validation samples: {len(X_val_recipe)}\")\n",
    "print(\"   Small dataset ‚Üí Perfect for demonstrating overfitting\")\n",
    "print()\n",
    "\n",
    "# PHASE 1: Build a model that CAN overfit\n",
    "print(\"üöÄ PHASE 1: BUILD AN OVERFITTING MODEL\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Goal: Create a model with enough capacity to memorize the training data\")\n",
    "print()\n",
    "\n",
    "class OverfittingCapableNet(nn.Module):\n",
    "    \"\"\"Intentionally over-parameterized network to demonstrate overfitting\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 256),      # Large hidden layers\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Train the overfitting model\n",
    "overfitting_capable = OverfittingCapableNet().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(overfitting_capable.parameters(), lr=0.001)  # Standard learning rate\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_recipe).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train_recipe.reshape(-1, 1)).to(device)\n",
    "X_val_tensor = torch.FloatTensor(X_val_recipe).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_val_recipe.reshape(-1, 1)).to(device)\n",
    "\n",
    "# Training loop - track losses\n",
    "train_losses_phase1 = []\n",
    "val_losses_phase1 = []\n",
    "\n",
    "print(\"Training overfitting-capable model...\")\n",
    "for epoch in range(1000):  # Train for many epochs\n",
    "    # Training\n",
    "    overfitting_capable.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_pred = overfitting_capable(X_train_tensor)\n",
    "    train_loss = criterion(train_pred, y_train_tensor)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    overfitting_capable.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = overfitting_capable(X_val_tensor)\n",
    "        val_loss = criterion(val_pred, y_val_tensor)\n",
    "    \n",
    "    train_losses_phase1.append(train_loss.item())\n",
    "    val_losses_phase1.append(val_loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        gap = val_loss.item() - train_loss.item()\n",
    "        print(f\"   Epoch {epoch+1}: Train={train_loss.item():.6f}, Val={val_loss.item():.6f}, Gap={gap:.6f}\")\n",
    "\n",
    "# Check if we achieved overfitting\n",
    "final_train_loss = train_losses_phase1[-1]\n",
    "final_val_loss = val_losses_phase1[-1]\n",
    "overfitting_gap = final_val_loss - final_train_loss\n",
    "\n",
    "print(f\"\\n‚úÖ PHASE 1 RESULTS:\")\n",
    "print(f\"   Final Train Loss: {final_train_loss:.6f}\")\n",
    "print(f\"   Final Val Loss: {final_val_loss:.6f}\")\n",
    "print(f\"   Overfitting Gap: {overfitting_gap:.6f}\")\n",
    "\n",
    "if overfitting_gap > 0.01:\n",
    "    print(\"   ‚úÖ SUCCESS: Model can overfit! (Large gap between train/val)\")\n",
    "    print(\"   üìã This proves the model has sufficient capacity to learn\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è WARNING: Model might not be overfitting enough\")\n",
    "    print(\"   üìã Consider: More parameters, longer training, or smaller dataset\")\n",
    "\n",
    "print(f\"\\nüìà Model Parameters: {sum(p.numel() for p in overfitting_capable.parameters()):,}\")\n",
    "print(\"üéØ Next: Apply regularization to reduce overfitting while maintaining performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e220957",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîß PHASE 2: SYSTEMATICALLY REDUCE OVERFITTING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Goal: Apply regularization techniques one by one to find the optimal balance\")\n",
    "print()\n",
    "\n",
    "# Strategy: Test different regularization approaches\n",
    "regularization_strategies = {\n",
    "    \"No Regularization\": {\"dropout\": 0.0, \"weight_decay\": 0.0, \"early_stop\": False},\n",
    "    \"Light Dropout\": {\"dropout\": 0.2, \"weight_decay\": 0.0, \"early_stop\": False},\n",
    "    \"Heavy Dropout\": {\"dropout\": 0.5, \"weight_decay\": 0.0, \"early_stop\": False},\n",
    "    \"L2 Regularization\": {\"dropout\": 0.0, \"weight_decay\": 0.01, \"early_stop\": False},\n",
    "    \"Combined (Dropout + L2)\": {\"dropout\": 0.3, \"weight_decay\": 0.001, \"early_stop\": False},\n",
    "    \"Early Stopping\": {\"dropout\": 0.0, \"weight_decay\": 0.0, \"early_stop\": True}\n",
    "}\n",
    "\n",
    "class RegularizedNet(nn.Module):\n",
    "    \"\"\"Network with configurable regularization\"\"\"\n",
    "    def __init__(self, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        # Same architecture as overfitting model, but with dropout\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(1, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_with_regularization(dropout_rate, weight_decay, early_stop, max_epochs=800):\n",
    "    \"\"\"Train model with specified regularization parameters\"\"\"\n",
    "    model = RegularizedNet(dropout_rate).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 50  # For early stopping\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model(X_train_tensor)\n",
    "        train_loss = criterion(train_pred, y_train_tensor)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val_tensor)\n",
    "            val_loss = criterion(val_pred, y_val_tensor)\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stop:\n",
    "            if val_loss.item() < best_val_loss:\n",
    "                best_val_loss = val_loss.item()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"      Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Test each regularization strategy\n",
    "results = {}\n",
    "print(\"Testing different regularization strategies...\")\n",
    "print()\n",
    "\n",
    "for strategy_name, params in regularization_strategies.items():\n",
    "    print(f\"üîÑ Testing: {strategy_name}\")\n",
    "    \n",
    "    model, train_losses, val_losses = train_with_regularization(\n",
    "        dropout_rate=params[\"dropout\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        early_stop=params[\"early_stop\"]\n",
    "    )\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    final_train = train_losses[-1]\n",
    "    final_val = val_losses[-1]\n",
    "    gap = final_val - final_train\n",
    "    epochs_trained = len(train_losses)\n",
    "    \n",
    "    results[strategy_name] = {\n",
    "        \"model\": model,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"final_train_loss\": final_train,\n",
    "        \"final_val_loss\": final_val,\n",
    "        \"overfitting_gap\": gap,\n",
    "        \"epochs_trained\": epochs_trained\n",
    "    }\n",
    "    \n",
    "    print(f\"   Train: {final_train:.4f}, Val: {final_val:.4f}, Gap: {gap:.4f}, Epochs: {epochs_trained}\")\n",
    "\n",
    "print(\"\\nüìä REGULARIZATION RESULTS SUMMARY:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Strategy':<25} {'Train Loss':<12} {'Val Loss':<10} {'Gap':<8} {'Epochs':<8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, result in results.items():\n",
    "    print(f\"{name:<25} {result['final_train_loss']:<12.4f} {result['final_val_loss']:<10.4f} \"\n",
    "          f\"{result['overfitting_gap']:<8.4f} {result['epochs_trained']:<8}\")\n",
    "\n",
    "# Find the best strategy (lowest validation loss with reasonable gap)\n",
    "best_strategy = min(results.keys(), \n",
    "                   key=lambda x: results[x]['final_val_loss'] + 0.1 * max(0, results[x]['overfitting_gap']))\n",
    "\n",
    "print(f\"\\nüèÜ RECOMMENDED STRATEGY: {best_strategy}\")\n",
    "best_result = results[best_strategy]\n",
    "print(f\"   ‚úÖ Val Loss: {best_result['final_val_loss']:.4f}\")\n",
    "print(f\"   ‚úÖ Overfitting Gap: {best_result['overfitting_gap']:.4f}\")\n",
    "print(f\"   ‚úÖ Good balance between performance and generalization\")\n",
    "\n",
    "print(f\"\\nüí° KEY INSIGHT:\")\n",
    "print(f\"   Started with overfitting gap of {overfitting_gap:.4f}\")\n",
    "print(f\"   Reduced to {best_result['overfitting_gap']:.4f} with {best_strategy}\")\n",
    "print(f\"   Improvement: {((overfitting_gap - best_result['overfitting_gap']) / overfitting_gap * 100):.1f}% reduction in overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the design recipe results\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Training curves comparison\n",
    "plt.subplot(2, 3, 1)\n",
    "# First show the original overfitting model\n",
    "epochs_phase1 = range(len(train_losses_phase1))\n",
    "plt.plot(epochs_phase1, train_losses_phase1, 'r-', alpha=0.7, linewidth=2, label='Phase 1: Overfitting Model (Train)')\n",
    "plt.plot(epochs_phase1, val_losses_phase1, 'r--', alpha=0.7, linewidth=2, label='Phase 1: Overfitting Model (Val)')\n",
    "\n",
    "# Show the best regularized model\n",
    "best_train = results[best_strategy]['train_losses']\n",
    "best_val = results[best_strategy]['val_losses']\n",
    "epochs_best = range(len(best_train))\n",
    "plt.plot(epochs_best, best_train, 'g-', alpha=0.8, linewidth=2, label=f'Phase 2: {best_strategy} (Train)')\n",
    "plt.plot(epochs_best, best_val, 'g--', alpha=0.8, linewidth=2, label=f'Phase 2: {best_strategy} (Val)')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Design Recipe: Before vs After Regularization')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 2: Overfitting gap comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "strategies = list(results.keys())\n",
    "gaps = [results[strategy]['overfitting_gap'] for strategy in strategies]\n",
    "colors = ['red' if gap > 0.05 else 'orange' if gap > 0.02 else 'green' for gap in gaps]\n",
    "\n",
    "bars = plt.bar(range(len(strategies)), gaps, color=colors, alpha=0.7)\n",
    "plt.xlabel('Regularization Strategy')\n",
    "plt.ylabel('Overfitting Gap (Val - Train)')\n",
    "plt.title('Overfitting Gap by Strategy')\n",
    "plt.xticks(range(len(strategies)), [s.replace(' ', '\\n') for s in strategies], rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0.02, color='orange', linestyle='--', alpha=0.5, label='Good Gap Threshold')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: Validation loss comparison\n",
    "plt.subplot(2, 3, 3)\n",
    "val_losses_final = [results[strategy]['final_val_loss'] for strategy in strategies]\n",
    "colors_val = ['green' if loss == min(val_losses_final) else 'lightblue' for loss in val_losses_final]\n",
    "\n",
    "bars = plt.bar(range(len(strategies)), val_losses_final, color=colors_val, alpha=0.7)\n",
    "plt.xlabel('Regularization Strategy')\n",
    "plt.ylabel('Final Validation Loss')\n",
    "plt.title('Validation Performance by Strategy')\n",
    "plt.xticks(range(len(strategies)), [s.replace(' ', '\\n') for s in strategies], rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Model predictions comparison\n",
    "plt.subplot(2, 3, 4)\n",
    "X_test_recipe = np.linspace(-2*np.pi, 2*np.pi, 200).reshape(-1, 1)\n",
    "y_test_true_recipe = 3 * np.sin(X_test_recipe.flatten())\n",
    "\n",
    "# Original overfitting model prediction\n",
    "overfitting_capable.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.FloatTensor(X_test_recipe).to(device)\n",
    "    y_pred_overfitting = overfitting_capable(X_test_tensor).cpu().numpy().flatten()\n",
    "\n",
    "# Best regularized model prediction\n",
    "best_model = results[best_strategy]['model']\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_best = best_model(X_test_tensor).cpu().numpy().flatten()\n",
    "\n",
    "plt.plot(X_test_recipe, y_test_true_recipe, 'k-', linewidth=3, label='True Function', alpha=0.8)\n",
    "plt.scatter(X_train_recipe, y_train_recipe, alpha=0.7, s=60, color='blue', label='Training Data', zorder=5)\n",
    "plt.plot(X_test_recipe, y_pred_overfitting, 'r--', linewidth=2, alpha=0.7, label='Phase 1: Overfitting')\n",
    "plt.plot(X_test_recipe, y_pred_best, 'g--', linewidth=2, alpha=0.8, label=f'Phase 2: {best_strategy}')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Predictions: Before vs After Regularization')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-2*np.pi, 2*np.pi)\n",
    "\n",
    "# Plot 5: Design recipe process flow\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.text(0.5, 0.9, 'üéØ DESIGN RECIPE PROCESS', ha='center', va='center', \n",
    "         fontsize=16, fontweight='bold', transform=plt.gca().transAxes)\n",
    "\n",
    "process_steps = [\n",
    "    \"1. üöÄ Build Overfitting Model\",\n",
    "    \"   ‚Ä¢ Large architecture\",\n",
    "    \"   ‚Ä¢ No regularization\", \n",
    "    \"   ‚Ä¢ Train until overfitting\",\n",
    "    \"\",\n",
    "    \"2. ‚úÖ Verify Overfitting\",\n",
    "    \"   ‚Ä¢ Large train/val gap\",\n",
    "    \"   ‚Ä¢ Proves sufficient capacity\",\n",
    "    \"\",\n",
    "    \"3. üîß Apply Regularization\",\n",
    "    \"   ‚Ä¢ Test different techniques\",\n",
    "    \"   ‚Ä¢ Monitor train/val gap\",\n",
    "    \"   ‚Ä¢ Find optimal balance\",\n",
    "    \"\",\n",
    "    \"4. üèÜ Select Best Model\",\n",
    "    f\"   ‚Ä¢ Chosen: {best_strategy}\",\n",
    "    f\"   ‚Ä¢ Gap reduced by {((overfitting_gap - best_result['overfitting_gap']) / overfitting_gap * 100):.1f}%\"\n",
    "]\n",
    "\n",
    "y_pos = 0.75\n",
    "for step in process_steps:\n",
    "    if step.startswith((\"1.\", \"2.\", \"3.\", \"4.\")):\n",
    "        plt.text(0.1, y_pos, step, ha='left', va='center', fontsize=12, fontweight='bold',\n",
    "                color='darkblue', transform=plt.gca().transAxes)\n",
    "    elif step.startswith(\"   ‚Ä¢\"):\n",
    "        plt.text(0.15, y_pos, step, ha='left', va='center', fontsize=10,\n",
    "                color='darkgreen', transform=plt.gca().transAxes)\n",
    "    else:\n",
    "        plt.text(0.15, y_pos, step, ha='left', va='center', fontsize=10,\n",
    "                transform=plt.gca().transAxes)\n",
    "    y_pos -= 0.05\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.axis('off')\n",
    "\n",
    "# Plot 6: Success metrics\n",
    "plt.subplot(2, 3, 6)\n",
    "metrics_data = {\n",
    "    'Original\\nOverfitting': [overfitting_gap, final_val_loss],\n",
    "    f'Best Strategy\\n({best_strategy})': [best_result['overfitting_gap'], best_result['final_val_loss']]\n",
    "}\n",
    "\n",
    "x_pos = np.arange(len(metrics_data))\n",
    "width = 0.35\n",
    "\n",
    "overfitting_gaps = [metrics_data[key][0] for key in metrics_data.keys()]\n",
    "val_losses = [metrics_data[key][1] for key in metrics_data.keys()]\n",
    "\n",
    "bars1 = plt.bar(x_pos - width/2, overfitting_gaps, width, label='Overfitting Gap', alpha=0.7, color='orange')\n",
    "bars2 = plt.bar(x_pos + width/2, val_losses, width, label='Validation Loss', alpha=0.7, color='lightblue')\n",
    "\n",
    "plt.xlabel('Model Version')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title('Recipe Success: Before vs After')\n",
    "plt.xticks(x_pos, metrics_data.keys())\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ DESIGN RECIPE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ PHASE 1 SUCCESS: Built a model that can overfit\")\n",
    "print(f\"   ‚Ä¢ Overfitting gap: {overfitting_gap:.4f}\")\n",
    "print(\"   ‚Ä¢ This proves the model has sufficient learning capacity\")\n",
    "print()\n",
    "print(\"‚úÖ PHASE 2 SUCCESS: Systematically reduced overfitting\")\n",
    "print(f\"   ‚Ä¢ Best strategy: {best_strategy}\")\n",
    "print(f\"   ‚Ä¢ New overfitting gap: {best_result['overfitting_gap']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Improvement: {((overfitting_gap - best_result['overfitting_gap']) / overfitting_gap * 100):.1f}% reduction\")\n",
    "print()\n",
    "print(\"üèÜ FINAL RESULT: Well-generalized model with good performance!\")\n",
    "print(f\"   ‚Ä¢ Maintained learning capacity while improving generalization\")\n",
    "print(f\"   ‚Ä¢ Clear systematic approach avoids guesswork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7941c1c",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Practical Guidelines for Applying the Design Recipe\n",
    "\n",
    "#### **Phase 1 Checklist: Building an Overfitting Model**\n",
    "\n",
    "‚úÖ **Model Architecture:**\n",
    "- Start with more layers/neurons than you think you need\n",
    "- Use standard activations (ReLU, etc.)\n",
    "- No regularization initially (no dropout, weight decay, batch norm)\n",
    "\n",
    "‚úÖ **Training Setup:**\n",
    "- Use a reasonable learning rate (0.001-0.01)\n",
    "- Train for many epochs (until clear overfitting appears)\n",
    "- Monitor both training and validation loss\n",
    "\n",
    "‚úÖ **Success Criteria:**\n",
    "- Training loss much lower than validation loss\n",
    "- Clear divergence between train/val curves\n",
    "- Gap > 0.02-0.05 depending on problem complexity\n",
    "\n",
    "‚ùå **Common Mistakes:**\n",
    "- Starting with regularization too early\n",
    "- Using too small architectures\n",
    "- Stopping training before overfitting appears\n",
    "\n",
    "---\n",
    "\n",
    "#### **Phase 2 Checklist: Reducing Overfitting Systematically**\n",
    "\n",
    "‚úÖ **Regularization Techniques to Try (in order):**\n",
    "1. **Dropout** (0.1 ‚Üí 0.3 ‚Üí 0.5): Simple and effective\n",
    "2. **L2 Regularization** (1e-4 ‚Üí 1e-3 ‚Üí 1e-2): Penalizes large weights\n",
    "3. **Early Stopping**: Stop when validation loss stops improving\n",
    "4. **Batch Normalization**: Stabilizes training\n",
    "5. **Data Augmentation**: If applicable to your domain\n",
    "\n",
    "‚úÖ **Systematic Testing:**\n",
    "- Try one technique at a time\n",
    "- Start with mild regularization, increase gradually\n",
    "- Keep track of train/val gap for each experiment\n",
    "- Test combinations of successful techniques\n",
    "\n",
    "‚úÖ **Success Criteria:**\n",
    "- Validation loss improves\n",
    "- Train/val gap reduces to reasonable level (< 0.02-0.05)\n",
    "- Model still learns (training loss decreases)\n",
    "\n",
    "‚ùå **Common Mistakes:**\n",
    "- Applying too much regularization at once\n",
    "- Not testing techniques systematically\n",
    "- Focusing only on training loss\n",
    "- Giving up too early\n",
    "\n",
    "---\n",
    "\n",
    "#### **üéØ When to Use This Recipe:**\n",
    "\n",
    "**‚úÖ Perfect for:**\n",
    "- New deep learning projects\n",
    "- Complex architectures (CNNs, RNNs, Transformers)\n",
    "- Limited training data scenarios\n",
    "- Industrial applications where reliability is crucial\n",
    "\n",
    "**‚ö†Ô∏è Consider alternatives for:**\n",
    "- Very simple problems (linear regression, etc.)\n",
    "- When you have infinite data\n",
    "- Extremely time-sensitive applications\n",
    "\n",
    "---\n",
    "\n",
    "#### **üè≠ Industrial Control Applications:**\n",
    "\n",
    "**Process Control:**\n",
    "- Phase 1: Complex LSTM with many layers for sensor fusion\n",
    "- Phase 2: Add dropout to prevent memorizing sensor noise\n",
    "\n",
    "**Predictive Maintenance:**\n",
    "- Phase 1: Deep CNN for vibration pattern recognition  \n",
    "- Phase 2: Early stopping to prevent overfitting to specific machines\n",
    "\n",
    "**Quality Control:**\n",
    "- Phase 1: Large neural network for defect detection\n",
    "- Phase 2: L2 regularization to ensure generalization across product batches\n",
    "\n",
    "---\n",
    "\n",
    "**üí° Remember: This recipe ensures you never waste time with underpowered models. If your model can't overfit, it definitely can't generalize!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36de6d8",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways and Best Practices\n",
    "\n",
    "### **üèÜ THE GOLDEN RULE: Design Recipe for Deep Learning**\n",
    "\n",
    "> **\"First get a model that can overfit, then reduce overfitting\"**\n",
    "\n",
    "This fundamental principle ensures:\n",
    "- ‚úÖ Your model has sufficient capacity to learn\n",
    "- ‚úÖ You focus on the right problem (generalization vs capacity)\n",
    "- ‚úÖ Systematic approach leads to better results\n",
    "- ‚úÖ Clear diagnostics guide your decisions\n",
    "\n",
    "---\n",
    "\n",
    "### **Understanding Overfitting and Underfitting:**\n",
    "\n",
    "#### **üî¥ UNDERFITTING (High Bias, Low Variance):**\n",
    "- **Symptoms**: High training and validation loss, both curves plateau at high values\n",
    "- **Causes**: Model too simple, insufficient training, high regularization\n",
    "- **Solutions**: \n",
    "  - Increase model complexity (more layers/neurons)\n",
    "  - Reduce regularization strength\n",
    "  - Train for more epochs\n",
    "  - Use more sophisticated architectures\n",
    "\n",
    "#### **üî¥ OVERFITTING (Low Bias, High Variance):**\n",
    "- **Symptoms**: Low training loss, high validation loss, large gap between them\n",
    "- **Causes**: Model too complex, insufficient data, no regularization\n",
    "- **Solutions**:\n",
    "  - Add dropout layers\n",
    "  - Apply L1/L2 regularization\n",
    "  - Use early stopping\n",
    "  - Collect more training data\n",
    "  - Reduce model complexity\n",
    "\n",
    "#### **üü¢ GOOD FIT (Balanced Bias-Variance):**\n",
    "- **Symptoms**: Both losses converge to low values, small gap between them\n",
    "- **Characteristics**: Model generalizes well to unseen data\n",
    "\n",
    "### **Regularization Techniques Summary:**\n",
    "\n",
    "| **Technique** | **How it Works** | **When to Use** | **PyTorch Implementation** |\n",
    "|---------------|------------------|-----------------|---------------------------|\n",
    "| **Dropout** | Randomly sets neurons to zero during training | High variance, complex models | `nn.Dropout(p=0.3)` |\n",
    "| **L2 Regularization** | Penalizes large weights | Prevent weight explosion | `weight_decay` in optimizer |\n",
    "| **L1 Regularization** | Promotes sparse weights | Feature selection needed | Custom loss term |\n",
    "| **Early Stopping** | Stops training when validation loss increases | Prevent overtraining | Monitor validation loss |\n",
    "| **Batch Normalization** | Normalizes layer inputs | Stabilize training | `nn.BatchNorm1d()` |\n",
    "\n",
    "### **Model Development Workflow:**\n",
    "\n",
    "1. **üöÄ Phase 1: Build Overfitting Capability**\n",
    "   - Create complex architecture\n",
    "   - No regularization initially\n",
    "   - Train until clear overfitting\n",
    "   - Verify sufficient capacity\n",
    "\n",
    "2. **üîß Phase 2: Systematic Regularization**\n",
    "   - Apply techniques one by one\n",
    "   - Monitor train/validation gap\n",
    "   - Find optimal balance\n",
    "   - Test combinations\n",
    "\n",
    "3. **‚úÖ Validation and Selection**\n",
    "   - Choose based on validation performance\n",
    "   - Consider train/val gap\n",
    "   - Test on hold-out set\n",
    "\n",
    "### **Industrial Applications:**\n",
    "\n",
    "This knowledge is crucial for:\n",
    "- **Process Control**: Models that predict equipment behavior\n",
    "- **Quality Prediction**: Ensuring models generalize to new production batches  \n",
    "- **Predictive Maintenance**: Balancing sensitivity and false alarms\n",
    "- **Optimization**: Models that work across different operating conditions\n",
    "\n",
    "### **Experiment Suggestions:**\n",
    "\n",
    "Try the interactive explorer with these scenarios:\n",
    "1. **Small dataset (50 samples) + Complex model** ‚Üí Observe overfitting\n",
    "2. **Large dataset (1000 samples) + Simple model** ‚Üí Observe underfitting  \n",
    "3. **Medium dataset + High dropout (0.7)** ‚Üí See regularization effect\n",
    "4. **High noise (0.4) + Various complexities** ‚Üí Study noise robustness\n",
    "\n",
    "---\n",
    "\n",
    "**This tutorial demonstrates the fundamental trade-offs in machine learning and provides a systematic approach (the design recipe) to build well-generalized models for industrial applications. The sine function y = 3*sin(x) provides a clear demonstration of overfitting and underfitting concepts while the design recipe ensures you always start with sufficient model capacity.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
