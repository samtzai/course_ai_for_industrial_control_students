{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966cb31e",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) Demonstration\n",
    "\n",
    "This notebook demonstrates the fundamentals of Principal Component Analysis (PCA), a dimensionality reduction technique commonly used in machine learning and data analysis.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what PCA is and why it's useful\n",
    "- Learn how PCA works mathematically\n",
    "- Apply PCA to real datasets\n",
    "- Visualize the results of dimensionality reduction\n",
    "- Understand how to choose the number of components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c797d4",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b65009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, load_wine, make_blobs\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b4810",
   "metadata": {},
   "source": [
    "## 2. What is PCA?\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a statistical technique used for:\n",
    "- **Dimensionality reduction**: Reducing the number of features while preserving most of the information\n",
    "- **Data visualization**: Projecting high-dimensional data to 2D or 3D for visualization\n",
    "- **Noise reduction**: Removing less important variations in the data\n",
    "- **Feature extraction**: Creating new features that capture the most variance in the data\n",
    "\n",
    "### Key Concepts:\n",
    "- **Principal Components**: New orthogonal axes that capture maximum variance\n",
    "- **Explained Variance**: How much of the original data's variance each component explains\n",
    "- **Loadings**: How much each original feature contributes to each principal component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7eebb7",
   "metadata": {},
   "source": [
    "## 3. Simple 2D Example: Understanding PCA Geometrically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc95e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic 2D data with correlation\n",
    "np.random.seed(42)\n",
    "n_samples = 300\n",
    "\n",
    "# Create correlated data\n",
    "x1 = np.random.normal(0, 1, n_samples)\n",
    "x2 = x1 + np.random.normal(0, 0.5, n_samples)  # x2 is correlated with x1\n",
    "\n",
    "# Combine into dataset\n",
    "data_2d = np.column_stack([x1, x2])\n",
    "\n",
    "print(f\"Original data shape: {data_2d.shape}\")\n",
    "print(f\"Data correlation coefficient: {np.corrcoef(x1, x2)[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d8b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to 2D data\n",
    "pca_2d = PCA(n_components=2)\n",
    "data_2d_pca = pca_2d.fit_transform(data_2d)\n",
    "\n",
    "# Plot original data and principal components\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3))\n",
    "\n",
    "# Original data\n",
    "ax1.scatter(data_2d[:, 0], data_2d[:, 1], alpha=0.6, color='blue')\n",
    "ax1.set_xlabel('X1')\n",
    "ax1.set_ylabel('X2')\n",
    "ax1.set_title('Original 2D Data')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add principal component vectors\n",
    "mean_x, mean_y = np.mean(data_2d, axis=0)\n",
    "for i in range(2):\n",
    "    # Principal component vectors (scaled for visualization)\n",
    "    pc_vector = pca_2d.components_[i] * np.sqrt(pca_2d.explained_variance_[i]) * 3\n",
    "    ax1.arrow(mean_x, mean_y, pc_vector[0], pc_vector[1], \n",
    "              head_width=0.1, head_length=0.1, fc=f'C{i}', ec=f'C{i}', linewidth=2)\n",
    "    ax1.text(mean_x + pc_vector[0]*1.1, mean_y + pc_vector[1]*1.1, \n",
    "             f'PC{i+1}', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Transformed data (in PC space)\n",
    "ax2.scatter(data_2d_pca[:, 0], data_2d_pca[:, 1], alpha=0.6, color='red')\n",
    "ax2.set_xlabel('First Principal Component')\n",
    "ax2.set_ylabel('Second Principal Component')\n",
    "ax2.set_title('Data in Principal Component Space')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print explained variance\n",
    "print(f\"Explained variance ratio: {pca_2d.explained_variance_ratio_}\")\n",
    "print(f\"First PC explains {pca_2d.explained_variance_ratio_[0]:.1%} of variance\")\n",
    "print(f\"Second PC explains {pca_2d.explained_variance_ratio_[1]:.1%} of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f407cbd",
   "metadata": {},
   "source": [
    "## 4. Real Dataset Example: Iris Dataset\n",
    "\n",
    "Let's apply PCA to the famous Iris dataset to see how it works with real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(f\"Iris dataset shape: {X_iris.shape}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Target classes: {target_names}\")\n",
    "\n",
    "# Create DataFrame for easier handling\n",
    "iris_df = pd.DataFrame(X_iris, columns=feature_names)\n",
    "iris_df['species'] = [target_names[i] for i in y_iris]\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(iris_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a81d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features (important for PCA!)\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "\n",
    "print(\"Why standardization is important:\")\n",
    "print(f\"Original feature means: {X_iris.mean(axis=0)}\")\n",
    "print(f\"Original feature std: {X_iris.std(axis=0)}\")\n",
    "print(f\"Scaled feature means: {X_iris_scaled.mean(axis=0)}\")\n",
    "print(f\"Scaled feature std: {X_iris_scaled.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdffca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to Iris dataset\n",
    "pca_iris = PCA(n_components=4)  # Keep all components initially\n",
    "X_iris_pca = pca_iris.fit_transform(X_iris_scaled)\n",
    "\n",
    "# Create DataFrame with PCA results\n",
    "pca_df = pd.DataFrame(X_iris_pca, columns=[f'PC{i+1}' for i in range(4)])\n",
    "pca_df['species'] = [target_names[i] for i in y_iris]\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(f\"Explained variance ratio: {pca_iris.explained_variance_ratio_}\")\n",
    "print(f\"Cumulative explained variance: {pca_iris.explained_variance_ratio_.cumsum()}\")\n",
    "print(f\"\\nFirst 2 PCs explain {pca_iris.explained_variance_ratio_[:2].sum():.1%} of variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a92580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7, 6))\n",
    "\n",
    "# 1. Explained Variance\n",
    "ax1 = axes[0, 0]\n",
    "components = range(1, 5)\n",
    "ax1.bar(components, pca_iris.explained_variance_ratio_, alpha=0.7, color='skyblue')\n",
    "ax1.plot(components, pca_iris.explained_variance_ratio_.cumsum(), \n",
    "         'ro-', color='red', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "ax1.set_title('Explained Variance by Principal Component')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(['Cumulative', 'Individual'], loc='center right')\n",
    "\n",
    "# 2. PC1 vs PC2 scatter plot\n",
    "ax2 = axes[0, 1]\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, species in enumerate(target_names):\n",
    "    mask = pca_df['species'] == species\n",
    "    ax2.scatter(pca_df.loc[mask, 'PC1'], pca_df.loc[mask, 'PC2'], \n",
    "                c=colors[i], label=species, alpha=0.7, s=50)\n",
    "ax2.set_xlabel(f'PC1 ({pca_iris.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax2.set_ylabel(f'PC2 ({pca_iris.explained_variance_ratio_[1]:.1%} variance)')\n",
    "ax2.set_title('Iris Dataset: First Two Principal Components')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. PC1 vs PC3 scatter plot\n",
    "ax3 = axes[1, 0]\n",
    "for i, species in enumerate(target_names):\n",
    "    mask = pca_df['species'] == species\n",
    "    ax3.scatter(pca_df.loc[mask, 'PC1'], pca_df.loc[mask, 'PC3'], \n",
    "                c=colors[i], label=species, alpha=0.7, s=50)\n",
    "ax3.set_xlabel(f'PC1 ({pca_iris.explained_variance_ratio_[0]:.1%} variance)')\n",
    "ax3.set_ylabel(f'PC3 ({pca_iris.explained_variance_ratio_[2]:.1%} variance)')\n",
    "ax3.set_title('Iris Dataset: PC1 vs PC3')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Feature loadings (how original features contribute to PCs)\n",
    "ax4 = axes[1, 1]\n",
    "loadings = pca_iris.components_.T * np.sqrt(pca_iris.explained_variance_)\n",
    "for i, feature in enumerate(feature_names):\n",
    "    ax4.arrow(0, 0, loadings[i, 0], loadings[i, 1], \n",
    "              head_width=0.05, head_length=0.05, fc='black', ec='black')\n",
    "    ax4.text(loadings[i, 0]*1.15, loadings[i, 1]*1.15, feature, \n",
    "             fontsize=10, ha='center', va='center')\n",
    "ax4.set_xlim(-1, 1)\n",
    "ax4.set_ylim(-1, 1)\n",
    "ax4.set_xlabel('PC1 Loading')\n",
    "ax4.set_ylabel('PC2 Loading')\n",
    "ax4.set_title('Feature Loadings on First Two PCs')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "ax4.axvline(x=0, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd7e46c",
   "metadata": {},
   "source": [
    "## 5. PCA Components Analysis\n",
    "\n",
    "Let's examine what each principal component represents in terms of the original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48acd8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of principal components\n",
    "components_df = pd.DataFrame(\n",
    "    pca_iris.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(4)],\n",
    "    index=feature_names\n",
    ")\n",
    "\n",
    "print(\"Principal Components (Loadings):\")\n",
    "print(components_df.round(3))\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"PC1: Overall size/scale of the flower (all features have similar positive loadings)\")\n",
    "print(\"PC2: Sepal vs Petal contrast (sepal features vs petal features)\")\n",
    "print(\"PC3: Length vs Width contrast (length features vs width features)\")\n",
    "print(\"PC4: Specific shape variations (mixed loadings)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c0db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize component loadings as heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(components_df.T, annot=True, cmap='RdBu_r', center=0, \n",
    "            cbar_kws={'label': 'Loading Value'})\n",
    "plt.title('Principal Component Loadings Heatmap')\n",
    "plt.xlabel('Original Features')\n",
    "plt.ylabel('Principal Components')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show explained variance for each component\n",
    "variance_df = pd.DataFrame({\n",
    "    'Component': [f'PC{i+1}' for i in range(4)],\n",
    "    'Explained Variance Ratio': pca_iris.explained_variance_ratio_,\n",
    "    'Cumulative Variance': pca_iris.explained_variance_ratio_.cumsum()\n",
    "})\n",
    "\n",
    "print(\"\\nVariance Explanation:\")\n",
    "print(variance_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2dca09",
   "metadata": {},
   "source": [
    "## 6. Choosing the Number of Components\n",
    "\n",
    "A key decision in PCA is how many components to keep. Let's explore different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0541ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Cumulative explained variance threshold\n",
    "cumvar = pca_iris.explained_variance_ratio_.cumsum()\n",
    "n_components_90 = np.argmax(cumvar >= 0.9) + 1\n",
    "n_components_95 = np.argmax(cumvar >= 0.95) + 1\n",
    "\n",
    "print(f\"Components needed for 90% variance: {n_components_90}\")\n",
    "print(f\"Components needed for 95% variance: {n_components_95}\")\n",
    "\n",
    "# Method 2: Kaiser criterion (eigenvalue > 1)\n",
    "eigenvalues = pca_iris.explained_variance_\n",
    "n_components_kaiser = np.sum(eigenvalues > 1)\n",
    "print(f\"Components with eigenvalue > 1: {n_components_kaiser}\")\n",
    "\n",
    "# Method 3: Scree plot (elbow method)\n",
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, 5), pca_iris.explained_variance_ratio_, 'bo-', linewidth=2, markersize=8)\n",
    "plt.axhline(y=0.05, color='r', linestyle='--', alpha=0.7, label='5% threshold')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 5), cumvar, 'ro-', linewidth=2, markersize=8)\n",
    "plt.axhline(y=0.9, color='g', linestyle='--', alpha=0.7, label='90% threshold')\n",
    "plt.axhline(y=0.95, color='b', linestyle='--', alpha=0.7, label='95% threshold')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Variance Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad7353b",
   "metadata": {},
   "source": [
    "## 7. PCA for Data Compression and Reconstruction\n",
    "\n",
    "Let's see how PCA can be used to compress data and reconstruct it with reduced dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA with different numbers of components\n",
    "n_components_list = [1, 2, 3, 4]\n",
    "reconstruction_errors = []\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(7, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, n_comp in enumerate(n_components_list):\n",
    "    # Fit PCA with n_comp components\n",
    "    pca_temp = PCA(n_components=n_comp)\n",
    "    X_transformed = pca_temp.fit_transform(X_iris_scaled)\n",
    "    X_reconstructed = pca_temp.inverse_transform(X_transformed)\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    reconstruction_error = np.mean((X_iris_scaled - X_reconstructed) ** 2)\n",
    "    reconstruction_errors.append(reconstruction_error)\n",
    "    \n",
    "    # Plot original vs reconstructed for first two features\n",
    "    ax = axes[i]\n",
    "    ax.scatter(X_iris_scaled[:, 0], X_iris_scaled[:, 1], \n",
    "               alpha=0.6, label='Original', s=50)\n",
    "    ax.scatter(X_reconstructed[:, 0], X_reconstructed[:, 1], \n",
    "               alpha=0.6, label='Reconstructed', s=30)\n",
    "    \n",
    "    ax.set_xlabel('Sepal Length (scaled)')\n",
    "    ax.set_ylabel('Sepal Width (scaled)')\n",
    "    ax.set_title(f'{n_comp} PC(s): {pca_temp.explained_variance_ratio_.sum():.1%} variance\\n'\n",
    "                f'Reconstruction Error: {reconstruction_error:.4f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show reconstruction errors\n",
    "error_df = pd.DataFrame({\n",
    "    'Components': n_components_list,\n",
    "    'Explained Variance': [PCA(n_components=n).fit(X_iris_scaled).explained_variance_ratio_.sum() \n",
    "                          for n in n_components_list],\n",
    "    'Reconstruction Error': reconstruction_errors\n",
    "})\n",
    "\n",
    "print(\"\\nReconstruction Quality vs Number of Components:\")\n",
    "print(error_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e64b2f",
   "metadata": {},
   "source": [
    "## 8. 3D Visualization Example\n",
    "\n",
    "Let's create a 3D visualization to better understand how PCA works in higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6889ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D visualization using Wine dataset (more features)\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "wine_feature_names = wine.feature_names\n",
    "wine_target_names = wine.target_names\n",
    "\n",
    "print(f\"Wine dataset shape: {X_wine.shape}\")\n",
    "print(f\"Number of features: {len(wine_feature_names)}\")\n",
    "print(f\"Target classes: {wine_target_names}\")\n",
    "\n",
    "# Standardize and apply PCA\n",
    "X_wine_scaled = StandardScaler().fit_transform(X_wine)\n",
    "pca_wine = PCA(n_components=3)\n",
    "X_wine_pca = pca_wine.fit_transform(X_wine_scaled)\n",
    "\n",
    "print(f\"\\nFirst 3 PCs explain {pca_wine.explained_variance_ratio_.sum():.1%} of variance\")\n",
    "print(f\"Individual explained variance: {pca_wine.explained_variance_ratio_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51c56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D scatter plot\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# 3D plot\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, wine_class in enumerate(wine_target_names):\n",
    "    mask = y_wine == i\n",
    "    ax1.scatter(X_wine_pca[mask, 0], X_wine_pca[mask, 1], X_wine_pca[mask, 2],\n",
    "                c=colors[i], label=wine_class, alpha=0.7, s=50)\n",
    "\n",
    "ax1.set_xlabel(f'PC1 ({pca_wine.explained_variance_ratio_[0]:.1%})')\n",
    "ax1.set_ylabel(f'PC2 ({pca_wine.explained_variance_ratio_[1]:.1%})')\n",
    "ax1.set_zlabel(f'PC3 ({pca_wine.explained_variance_ratio_[2]:.1%})')\n",
    "ax1.set_title('Wine Dataset: First 3 Principal Components')\n",
    "ax1.legend()\n",
    "\n",
    "# 2D projection\n",
    "ax2 = fig.add_subplot(122)\n",
    "for i, wine_class in enumerate(wine_target_names):\n",
    "    mask = y_wine == i\n",
    "    ax2.scatter(X_wine_pca[mask, 0], X_wine_pca[mask, 1],\n",
    "                c=colors[i], label=wine_class, alpha=0.7, s=50)\n",
    "\n",
    "ax2.set_xlabel(f'PC1 ({pca_wine.explained_variance_ratio_[0]:.1%})')\n",
    "ax2.set_ylabel(f'PC2 ({pca_wine.explained_variance_ratio_[1]:.1%})')\n",
    "ax2.set_title('Wine Dataset: First 2 Principal Components')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf844c",
   "metadata": {},
   "source": [
    "## 9. Practical Tips and Best Practices\n",
    "\n",
    "Let's summarize key points and best practices for using PCA effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979334cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the importance of scaling\n",
    "print(\"=== Importance of Feature Scaling ===\")\n",
    "\n",
    "# PCA without scaling\n",
    "pca_unscaled = PCA(n_components=2)\n",
    "X_iris_pca_unscaled = pca_unscaled.fit_transform(X_iris)\n",
    "\n",
    "# PCA with scaling\n",
    "pca_scaled = PCA(n_components=2)\n",
    "X_iris_pca_scaled = pca_scaled.fit_transform(X_iris_scaled)\n",
    "\n",
    "# Compare results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3))\n",
    "\n",
    "# Without scaling\n",
    "for i, species in enumerate(target_names):\n",
    "    mask = y_iris == i\n",
    "    ax1.scatter(X_iris_pca_unscaled[mask, 0], X_iris_pca_unscaled[mask, 1], \n",
    "                c=colors[i], label=species, alpha=0.7, s=50)\n",
    "ax1.set_title(f'PCA without Scaling\\nPC1: {pca_unscaled.explained_variance_ratio_[0]:.1%}, '\n",
    "              f'PC2: {pca_unscaled.explained_variance_ratio_[1]:.1%}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# With scaling\n",
    "for i, species in enumerate(target_names):\n",
    "    mask = y_iris == i\n",
    "    ax2.scatter(X_iris_pca_scaled[mask, 0], X_iris_pca_scaled[mask, 1], \n",
    "                c=colors[i], label=species, alpha=0.7, s=50)\n",
    "ax2.set_title(f'PCA with Scaling\\nPC1: {pca_scaled.explained_variance_ratio_[0]:.1%}, '\n",
    "              f'PC2: {pca_scaled.explained_variance_ratio_[1]:.1%}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Without scaling - explained variance: {pca_unscaled.explained_variance_ratio_}\")\n",
    "print(f\"With scaling - explained variance: {pca_scaled.explained_variance_ratio_}\")\n",
    "print(f\"\\nDifference in first PC: {abs(pca_unscaled.explained_variance_ratio_[0] - pca_scaled.explained_variance_ratio_[0]):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b97fb",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Takeaways\n",
    "\n",
    "### What we learned:\n",
    "\n",
    "1. **PCA Purpose**: Dimensionality reduction while preserving maximum variance\n",
    "2. **Preprocessing**: Always standardize features before applying PCA\n",
    "3. **Component Selection**: Use explained variance, scree plots, or domain knowledge\n",
    "4. **Interpretation**: Principal components are linear combinations of original features\n",
    "5. **Applications**: Data visualization, noise reduction, feature extraction\n",
    "\n",
    "### Best Practices:\n",
    "- âœ… Standardize features (mean=0, std=1)\n",
    "- âœ… Check explained variance ratios\n",
    "- âœ… Use scree plots to choose components\n",
    "- âœ… Interpret component loadings\n",
    "- âœ… Consider domain knowledge\n",
    "- âŒ Don't apply PCA to categorical variables directly\n",
    "- âŒ Don't ignore the interpretability trade-off\n",
    "\n",
    "### When to use PCA:\n",
    "- High-dimensional data visualization\n",
    "- Preprocessing for other ML algorithms\n",
    "- Data compression\n",
    "- Noise reduction\n",
    "- Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demonstration: PCA pipeline function\n",
    "def pca_analysis_pipeline(X, y=None, target_names=None, feature_names=None, \n",
    "                         n_components=2, plot=True):\n",
    "    \"\"\"\n",
    "    Complete PCA analysis pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    - X: feature matrix\n",
    "    - y: target vector (optional)\n",
    "    - target_names: names of target classes (optional)\n",
    "    - feature_names: names of features (optional)\n",
    "    - n_components: number of components to keep\n",
    "    - plot: whether to create visualizations\n",
    "    \n",
    "    Returns:\n",
    "    - pca: fitted PCA object\n",
    "    - X_transformed: transformed data\n",
    "    - results: dictionary with analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Step 2: Apply PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_transformed = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Step 3: Analyze results\n",
    "    results = {\n",
    "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
    "        'cumulative_variance': pca.explained_variance_ratio_.cumsum(),\n",
    "        'components': pca.components_,\n",
    "        'n_features_original': X.shape[1],\n",
    "        'n_components_kept': n_components,\n",
    "        'compression_ratio': n_components / X.shape[1]\n",
    "    }\n",
    "    \n",
    "    # Step 4: Print summary\n",
    "    print(f\"PCA Analysis Summary:\")\n",
    "    print(f\"Original dimensions: {X.shape}\")\n",
    "    print(f\"Reduced dimensions: {X_transformed.shape}\")\n",
    "    print(f\"Compression ratio: {results['compression_ratio']:.2f}\")\n",
    "    print(f\"Total variance explained: {results['cumulative_variance'][-1]:.1%}\")\n",
    "    \n",
    "    # Step 5: Visualize (if requested and applicable)\n",
    "    if plot and n_components >= 2:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.bar(range(1, n_components + 1), pca.explained_variance_ratio_)\n",
    "        plt.xlabel('Principal Component')\n",
    "        plt.ylabel('Explained Variance Ratio')\n",
    "        plt.title('Explained Variance by Component')\n",
    "        \n",
    "        if y is not None and n_components >= 2:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            unique_labels = np.unique(y)\n",
    "            colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))\n",
    "            \n",
    "            for i, label in enumerate(unique_labels):\n",
    "                mask = y == label\n",
    "                label_name = target_names[label] if target_names is not None else f'Class {label}'\n",
    "                plt.scatter(X_transformed[mask, 0], X_transformed[mask, 1],\n",
    "                           c=[colors[i]], label=label_name, alpha=0.7)\n",
    "            \n",
    "            plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "            plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "            plt.title('First Two Principal Components')\n",
    "            plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return pca, X_transformed, results\n",
    "\n",
    "# Test the pipeline with Iris dataset\n",
    "print(\"Testing PCA Pipeline with Iris Dataset:\")\n",
    "pca_iris_final, X_iris_final, results_iris = pca_analysis_pipeline(\n",
    "    X_iris, y_iris, target_names, feature_names, n_components=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7fa420",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Practice Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:\n",
    "\n",
    "1. **Load a different dataset** (e.g., digits, breast_cancer) and apply PCA\n",
    "2. **Experiment with different numbers of components** and observe the trade-offs\n",
    "3. **Compare PCA results with and without standardization** on datasets with different scales\n",
    "4. **Use PCA as preprocessing** for a classification algorithm and compare performance\n",
    "5. **Create a function** to automatically determine the optimal number of components based on explained variance threshold\n",
    "\n",
    "## ðŸ“š Further Reading\n",
    "\n",
    "- Scikit-learn PCA documentation\n",
    "- \"Pattern Recognition and Machine Learning\" by Christopher Bishop\n",
    "- \"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman\n",
    "- Online courses on dimensionality reduction techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
