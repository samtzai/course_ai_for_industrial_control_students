{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df210000",
   "metadata": {},
   "source": [
    "# üéØ Principal Component Analysis (PCA) in 2D - Visual Demonstration\n",
    "\n",
    "## **Understanding PCA Through Interactive 2D Visualizations**\n",
    "\n",
    "### **Main Goal:**\n",
    "Learn how Principal Component Analysis (PCA) works by visualizing it step-by-step with 2D data, making the mathematical concepts intuitive and easy to understand.\n",
    "\n",
    "### **Key Learning Objectives:**\n",
    "- **Understand PCA Fundamentals**: Learn what principal components represent geometrically\n",
    "- **Step-by-Step Implementation**: Build PCA from scratch to understand the mathematics\n",
    "- **Visual Intuition**: See how PCA finds directions of maximum variance in data\n",
    "- **Dimensionality Reduction**: Understand how PCA reduces dimensions while preserving information\n",
    "- **Interactive Exploration**: Experiment with different data patterns and PCA parameters\n",
    "- **Real-World Applications**: Apply PCA to actual datasets and interpret results\n",
    "\n",
    "### **Why Start with 2D?**\n",
    "2D data is perfect for learning PCA because:\n",
    "- **Visual Clarity**: Easy to visualize data points and principal components\n",
    "- **Mathematical Simplicity**: All calculations can be followed step-by-step\n",
    "- **Geometric Intuition**: Principal components are simply directions (arrows) in the plane\n",
    "- **Foundation Building**: Concepts learned here extend directly to higher dimensions\n",
    "\n",
    "### **Interactive Features:**\n",
    "üéÆ Data pattern explorer | üìä Live PCA computation | ‚ö° Parameter adjustment | üî¨ Comparison dashboard\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9f6a73",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll use NumPy for computations, Matplotlib for visualization, Scikit-learn for validation, and IPython widgets for interactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26de9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris, make_blobs\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, VBox, HBox\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Enable interactive plots\n",
    "%matplotlib widget\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Ready to explore PCA in 2D!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7148d6",
   "metadata": {},
   "source": [
    "## 2. Generate 2D Synthetic Datasets\n",
    "\n",
    "Let's create various types of 2D datasets to understand how PCA behaves with different data patterns and correlation structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee902d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_correlated_data(n_samples=300, correlation=0.8, noise=0.1):\n",
    "    \"\"\"\n",
    "    Generate 2D correlated Gaussian data\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples: Number of data points\n",
    "    - correlation: Correlation coefficient between x and y\n",
    "    - noise: Amount of noise to add\n",
    "    \n",
    "    Returns:\n",
    "    - X: 2D array of data points\n",
    "    \"\"\"\n",
    "    # Create covariance matrix\n",
    "    cov_matrix = np.array([[1, correlation], \n",
    "                          [correlation, 1]])\n",
    "    \n",
    "    # Generate data\n",
    "    data = np.random.multivariate_normal([0, 0], cov_matrix, n_samples)\n",
    "    \n",
    "    # Add noise\n",
    "    noise_data = np.random.normal(0, noise, data.shape)\n",
    "    \n",
    "    return data + noise_data\n",
    "\n",
    "def generate_circular_data(n_samples=300, radius_range=(1, 3), noise=0.2):\n",
    "    \"\"\"\n",
    "    Generate data points arranged in a circular pattern\n",
    "    \"\"\"\n",
    "    angles = np.random.uniform(0, 2*np.pi, n_samples)\n",
    "    radii = np.random.uniform(radius_range[0], radius_range[1], n_samples)\n",
    "    \n",
    "    x = radii * np.cos(angles) + np.random.normal(0, noise, n_samples)\n",
    "    y = radii * np.sin(angles) + np.random.normal(0, noise, n_samples)\n",
    "    \n",
    "    return np.column_stack([x, y])\n",
    "\n",
    "def generate_diagonal_data(n_samples=300, slope=2, spread=0.5):\n",
    "    \"\"\"\n",
    "    Generate data along a diagonal line with some spread\n",
    "    \"\"\"\n",
    "    x = np.random.uniform(-2, 2, n_samples)\n",
    "    y = slope * x + np.random.normal(0, spread, n_samples)\n",
    "    \n",
    "    return np.column_stack([x, y])\n",
    "\n",
    "def generate_uncorrelated_data(n_samples=300, x_std=1, y_std=2):\n",
    "    \"\"\"\n",
    "    Generate uncorrelated data with different variances in x and y\n",
    "    \"\"\"\n",
    "    x = np.random.normal(0, x_std, n_samples)\n",
    "    y = np.random.normal(0, y_std, n_samples)\n",
    "    \n",
    "    return np.column_stack([x, y])\n",
    "\n",
    "# Generate different types of datasets\n",
    "print(\"GENERATING 2D DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "datasets = {\n",
    "    'Highly Correlated': generate_correlated_data(correlation=0.9),\n",
    "    'Moderately Correlated': generate_correlated_data(correlation=0.6),\n",
    "    'Diagonal Pattern': generate_diagonal_data(),\n",
    "    'Circular Pattern': generate_circular_data(),\n",
    "    'Uncorrelated (Different Variances)': generate_uncorrelated_data(),\n",
    "    'Weakly Correlated': generate_correlated_data(correlation=0.3)\n",
    "}\n",
    "\n",
    "print(f\"Generated {len(datasets)} different 2D datasets\")\n",
    "for name, data in datasets.items():\n",
    "    print(f\"  {name}: {data.shape[0]} points\")\n",
    "\n",
    "# Visualize all datasets\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(datasets)))\n",
    "\n",
    "for i, (name, data) in enumerate(datasets.items()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot data points\n",
    "    ax.scatter(data[:, 0], data[:, 1], alpha=0.6, s=30, color=colors[i])\n",
    "    \n",
    "    # Calculate correlation coefficient\n",
    "    correlation = np.corrcoef(data[:, 0], data[:, 1])[0, 1]\n",
    "    \n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title(f'{name}\\nCorrelation: {correlation:.3f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY OBSERVATIONS:\")\n",
    "print(\"‚úÖ Different datasets show various correlation patterns\")\n",
    "print(\"üìä Correlation coefficient quantifies linear relationship\")\n",
    "print(\"üéØ PCA will find different principal components for each pattern\")\n",
    "print(\"üîç Notice how data spreads differently in each case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197cf1af",
   "metadata": {},
   "source": [
    "## 3. Implement PCA from Scratch\n",
    "\n",
    "Let's build PCA step-by-step to understand exactly what happens mathematically. We'll implement each step clearly so you can see the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95009a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA2D:\n",
    "    \"\"\"\n",
    "    Principal Component Analysis implemented from scratch for 2D data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mean_ = None\n",
    "        self.components_ = None\n",
    "        self.explained_variance_ratio_ = None\n",
    "        self.eigenvalues_ = None\n",
    "        self.eigenvectors_ = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit PCA on 2D data\n",
    "        \n",
    "        Steps:\n",
    "        1. Center the data (subtract mean)\n",
    "        2. Compute covariance matrix\n",
    "        3. Find eigenvalues and eigenvectors\n",
    "        4. Sort by eigenvalues (descending)\n",
    "        5. Store principal components\n",
    "        \"\"\"\n",
    "        # Step 1: Center the data\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean_\n",
    "        \n",
    "        print(\"STEP-BY-STEP PCA COMPUTATION\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Step 1: Center the data\")\n",
    "        print(f\"  Original data mean: [{self.mean_[0]:.3f}, {self.mean_[1]:.3f}]\")\n",
    "        print(f\"  Centered data mean: [{np.mean(X_centered, axis=0)[0]:.3f}, {np.mean(X_centered, axis=0)[1]:.3f}]\")\n",
    "        \n",
    "        # Step 2: Compute covariance matrix\n",
    "        n_samples = X.shape[0]\n",
    "        cov_matrix = np.cov(X_centered.T)\n",
    "        \n",
    "        print(f\"\\nStep 2: Compute covariance matrix\")\n",
    "        print(f\"  Covariance matrix:\")\n",
    "        print(f\"    [[{cov_matrix[0,0]:.3f}, {cov_matrix[0,1]:.3f}]\")\n",
    "        print(f\"     [{cov_matrix[1,0]:.3f}, {cov_matrix[1,1]:.3f}]]\")\n",
    "        \n",
    "        # Step 3: Find eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "        \n",
    "        print(f\"\\nStep 3: Compute eigenvalues and eigenvectors\")\n",
    "        print(f\"  Eigenvalues: [{eigenvalues[0]:.3f}, {eigenvalues[1]:.3f}]\")\n",
    "        print(f\"  Eigenvector 1: [{eigenvectors[:, 0][0]:.3f}, {eigenvectors[:, 0][1]:.3f}]\")\n",
    "        print(f\"  Eigenvector 2: [{eigenvectors[:, 1][0]:.3f}, {eigenvectors[:, 1][1]:.3f}]\")\n",
    "        \n",
    "        # Step 4: Sort by eigenvalues (descending)\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "        \n",
    "        print(f\"\\nStep 4: Sort by eigenvalues (descending)\")\n",
    "        print(f\"  First PC (largest eigenvalue): {eigenvalues[0]:.3f}\")\n",
    "        print(f\"  Second PC (smallest eigenvalue): {eigenvalues[1]:.3f}\")\n",
    "        \n",
    "        # Step 5: Store results\n",
    "        self.eigenvalues_ = eigenvalues\n",
    "        self.eigenvectors_ = eigenvectors\n",
    "        self.components_ = eigenvectors.T\n",
    "        \n",
    "        # Calculate explained variance ratio\n",
    "        total_variance = np.sum(eigenvalues)\n",
    "        self.explained_variance_ratio_ = eigenvalues / total_variance\n",
    "        \n",
    "        print(f\"\\nStep 5: Calculate explained variance\")\n",
    "        print(f\"  PC1 explains {self.explained_variance_ratio_[0]:.1%} of variance\")\n",
    "        print(f\"  PC2 explains {self.explained_variance_ratio_[1]:.1%} of variance\")\n",
    "        print(f\"  Total variance explained: {np.sum(self.explained_variance_ratio_):.1%}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform data to principal component space\n",
    "        \"\"\"\n",
    "        X_centered = X - self.mean_\n",
    "        return X_centered @ self.eigenvectors_\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit PCA and transform data\n",
    "        \"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    def inverse_transform(self, X_pca):\n",
    "        \"\"\"\n",
    "        Transform data back from PCA space to original space\n",
    "        \"\"\"\n",
    "        return X_pca @ self.eigenvectors_.T + self.mean_\n",
    "\n",
    "# Test PCA implementation with highly correlated data\n",
    "test_data = datasets['Highly Correlated']\n",
    "print(\"TESTING PCA IMPLEMENTATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Testing with: {test_data.shape[0]} data points\")\n",
    "\n",
    "# Apply our PCA implementation\n",
    "pca_manual = PCA2D()\n",
    "pca_manual.fit(test_data)\n",
    "\n",
    "# Transform data to PCA space\n",
    "transformed_data = pca_manual.transform(test_data)\n",
    "\n",
    "print(f\"\\nTransformed data shape: {transformed_data.shape}\")\n",
    "print(f\"First few transformed points:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Point {i+1}: [{transformed_data[i, 0]:.3f}, {transformed_data[i, 1]:.3f}]\")\n",
    "\n",
    "print(\"\\n‚úÖ PCA implementation completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f15914",
   "metadata": {},
   "source": [
    "## 4. Visualize Data and Principal Components\n",
    "\n",
    "Let's create comprehensive visualizations to see how PCA identifies the main directions of variation in 2D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdda8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_analysis(X, pca_fitted, title=\"PCA Analysis\"):\n",
    "    \"\"\"\n",
    "    Create comprehensive PCA visualization\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(8, 6))\n",
    "    \n",
    "    # 1. Original data with principal components\n",
    "    ax1.scatter(X[:, 0], X[:, 1], alpha=0.6, s=50, color='blue', label='Data Points')\n",
    "    \n",
    "    # Plot mean point\n",
    "    mean_point = pca_fitted.mean_\n",
    "    ax1.scatter(mean_point[0], mean_point[1], color='red', s=100, marker='x', \n",
    "               linewidth=3, label='Mean', zorder=5)\n",
    "    \n",
    "    # Plot principal component vectors\n",
    "    scale_factor = 3  # Scale arrows for visibility\n",
    "    \n",
    "    # First principal component (red arrow)\n",
    "    pc1 = pca_fitted.eigenvectors_[:, 0] * np.sqrt(pca_fitted.eigenvalues_[0]) * scale_factor\n",
    "    ax1.arrow(mean_point[0], mean_point[1], pc1[0], pc1[1],\n",
    "              head_width=0.15, head_length=0.2, fc='red', ec='red', linewidth=3,\n",
    "              label=f'PC1 ({pca_fitted.explained_variance_ratio_[0]:.1%})')\n",
    "    \n",
    "    # Second principal component (green arrow)\n",
    "    pc2 = pca_fitted.eigenvectors_[:, 1] * np.sqrt(pca_fitted.eigenvalues_[1]) * scale_factor\n",
    "    ax1.arrow(mean_point[0], mean_point[1], pc2[0], pc2[1],\n",
    "              head_width=0.15, head_length=0.2, fc='green', ec='green', linewidth=3,\n",
    "              label=f'PC2 ({pca_fitted.explained_variance_ratio_[1]:.1%})')\n",
    "    \n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "    ax1.set_title(f'{title} - Original Space')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    # 2. Transformed data (PCA space)\n",
    "    X_transformed = pca_fitted.transform(X)\n",
    "    ax2.scatter(X_transformed[:, 0], X_transformed[:, 1], alpha=0.6, s=50, color='purple')\n",
    "    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax2.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    ax2.set_xlabel('First Principal Component')\n",
    "    ax2.set_ylabel('Second Principal Component')\n",
    "    ax2.set_title(f'{title} - PCA Space')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    # 3. Explained variance\n",
    "    variance_ratios = pca_fitted.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(variance_ratios)\n",
    "    \n",
    "    components = ['PC1', 'PC2']\n",
    "    ax3.bar(components, variance_ratios, alpha=0.7, color=['red', 'green'])\n",
    "    ax3.set_ylabel('Explained Variance Ratio')\n",
    "    ax3.set_title('Explained Variance by Component')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(variance_ratios):\n",
    "        ax3.text(i, v + 0.01, f'{v:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Cumulative explained variance\n",
    "    ax4.bar(components, cumulative_variance, alpha=0.7, color=['orange', 'lightblue'])\n",
    "    ax4.set_ylabel('Cumulative Explained Variance')\n",
    "    ax4.set_title('Cumulative Explained Variance')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(cumulative_variance):\n",
    "        ax4.text(i, v + 0.01, f'{v:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed analysis\n",
    "    print(f\"\\nDETAILED PCA ANALYSIS - {title}\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Principal Component 1:\")\n",
    "    print(f\"  Direction: [{pca_fitted.eigenvectors_[:, 0][0]:.3f}, {pca_fitted.eigenvectors_[:, 0][1]:.3f}]\")\n",
    "    print(f\"  Eigenvalue: {pca_fitted.eigenvalues_[0]:.3f}\")\n",
    "    print(f\"  Explained Variance: {pca_fitted.explained_variance_ratio_[0]:.1%}\")\n",
    "    \n",
    "    print(f\"\\nPrincipal Component 2:\")\n",
    "    print(f\"  Direction: [{pca_fitted.eigenvectors_[:, 1][0]:.3f}, {pca_fitted.eigenvectors_[:, 1][1]:.3f}]\")\n",
    "    print(f\"  Eigenvalue: {pca_fitted.eigenvalues_[1]:.3f}\")\n",
    "    print(f\"  Explained Variance: {pca_fitted.explained_variance_ratio_[1]:.1%}\")\n",
    "    \n",
    "    # Check orthogonality\n",
    "    dot_product = np.dot(pca_fitted.eigenvectors_[:, 0], pca_fitted.eigenvectors_[:, 1])\n",
    "    print(f\"\\nOrthogonality Check:\")\n",
    "    print(f\"  Dot product of PC1 and PC2: {dot_product:.6f}\")\n",
    "    print(f\"  {'‚úÖ Orthogonal!' if abs(dot_product) < 1e-10 else '‚ùå Not orthogonal!'}\")\n",
    "\n",
    "# Apply PCA to different datasets\n",
    "print(\"PCA ANALYSIS FOR DIFFERENT DATA PATTERNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze a few interesting datasets\n",
    "datasets_to_analyze = ['Highly Correlated', 'Circular Pattern', 'Uncorrelated (Different Variances)']\n",
    "\n",
    "for dataset_name in datasets_to_analyze:\n",
    "    data = datasets[dataset_name]\n",
    "    pca = PCA2D()\n",
    "    pca.fit(data)\n",
    "    plot_pca_analysis(data, pca, dataset_name)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8babbd09",
   "metadata": {},
   "source": [
    "## 5. Compare Manual PCA with Scikit-learn\n",
    "\n",
    "Let's validate our implementation by comparing results with Scikit-learn's PCA to ensure our manual implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc58713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pca_implementations(X, dataset_name):\n",
    "    \"\"\"\n",
    "    Compare our manual PCA with scikit-learn's PCA\n",
    "    \"\"\"\n",
    "    print(f\"COMPARING PCA IMPLEMENTATIONS - {dataset_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Our manual implementation\n",
    "    pca_manual = PCA2D()\n",
    "    pca_manual.fit(X)\n",
    "    \n",
    "    # Scikit-learn implementation\n",
    "    pca_sklearn = PCA(n_components=2)\n",
    "    pca_sklearn.fit(X)\n",
    "    \n",
    "    print(\"COMPARISON RESULTS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Compare explained variance ratios\n",
    "    manual_variance = pca_manual.explained_variance_ratio_\n",
    "    sklearn_variance = pca_sklearn.explained_variance_ratio_\n",
    "    \n",
    "    print(f\"Explained Variance Ratios:\")\n",
    "    print(f\"  Manual PCA:    [{manual_variance[0]:.6f}, {manual_variance[1]:.6f}]\")\n",
    "    print(f\"  Scikit-learn:  [{sklearn_variance[0]:.6f}, {sklearn_variance[1]:.6f}]\")\n",
    "    print(f\"  Difference:    [{abs(manual_variance[0] - sklearn_variance[0]):.8f}, {abs(manual_variance[1] - sklearn_variance[1]):.8f}]\")\n",
    "    \n",
    "    # Compare principal components (note: sign might be flipped)\n",
    "    manual_components = pca_manual.components_\n",
    "    sklearn_components = pca_sklearn.components_\n",
    "    \n",
    "    print(f\"\\nPrincipal Components:\")\n",
    "    print(f\"  Manual PC1:    [{manual_components[0, 0]:.6f}, {manual_components[0, 1]:.6f}]\")\n",
    "    print(f\"  Scikit PC1:    [{sklearn_components[0, 0]:.6f}, {sklearn_components[0, 1]:.6f}]\")\n",
    "    print(f\"  Manual PC2:    [{manual_components[1, 0]:.6f}, {manual_components[1, 1]:.6f}]\")\n",
    "    print(f\"  Scikit PC2:    [{sklearn_components[1, 0]:.6f}, {sklearn_components[1, 1]:.6f}]\")\n",
    "    \n",
    "    # Check if components are the same or flipped\n",
    "    pc1_same = np.allclose(manual_components[0], sklearn_components[0], atol=1e-6)\n",
    "    pc1_flipped = np.allclose(manual_components[0], -sklearn_components[0], atol=1e-6)\n",
    "    pc2_same = np.allclose(manual_components[1], sklearn_components[1], atol=1e-6)\n",
    "    pc2_flipped = np.allclose(manual_components[1], -sklearn_components[1], atol=1e-6)\n",
    "    \n",
    "    print(f\"\\nComponent Direction Analysis:\")\n",
    "    print(f\"  PC1: {'‚úÖ Same direction' if pc1_same else ('‚úÖ Opposite direction (equivalent)' if pc1_flipped else '‚ùå Different')}\")\n",
    "    print(f\"  PC2: {'‚úÖ Same direction' if pc2_same else ('‚úÖ Opposite direction (equivalent)' if pc2_flipped else '‚ùå Different')}\")\n",
    "    \n",
    "    # Transform data with both methods\n",
    "    manual_transformed = pca_manual.transform(X)\n",
    "    sklearn_transformed = pca_sklearn.transform(X)\n",
    "    \n",
    "    # Check if transformations are consistent (allowing for sign flips)\n",
    "    transform_diff = np.abs(manual_transformed - sklearn_transformed)\n",
    "    transform_diff_flipped = np.abs(manual_transformed + sklearn_transformed)\n",
    "    \n",
    "    max_diff = np.min([np.max(transform_diff), np.max(transform_diff_flipped)])\n",
    "    \n",
    "    print(f\"\\nTransformation Consistency:\")\n",
    "    print(f\"  Maximum difference in transformed coordinates: {max_diff:.8f}\")\n",
    "    print(f\"  {'‚úÖ Transformations match!' if max_diff < 1e-6 else '‚ùå Transformations differ'}\")\n",
    "    \n",
    "    # Visual comparison\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(8, 6))\n",
    "    \n",
    "    # Manual PCA visualization\n",
    "    ax1.scatter(X[:, 0], X[:, 1], alpha=0.6, s=50, color='blue')\n",
    "    mean_point = pca_manual.mean_\n",
    "    \n",
    "    # Plot principal components\n",
    "    scale_factor = 2\n",
    "    pc1 = pca_manual.eigenvectors_[:, 0] * np.sqrt(pca_manual.eigenvalues_[0]) * scale_factor\n",
    "    pc2 = pca_manual.eigenvectors_[:, 1] * np.sqrt(pca_manual.eigenvalues_[1]) * scale_factor\n",
    "    \n",
    "    ax1.arrow(mean_point[0], mean_point[1], pc1[0], pc1[1],\n",
    "              head_width=0.1, head_length=0.15, fc='red', ec='red', linewidth=2)\n",
    "    ax1.arrow(mean_point[0], mean_point[1], pc2[0], pc2[1],\n",
    "              head_width=0.1, head_length=0.15, fc='green', ec='green', linewidth=2)\n",
    "    \n",
    "    ax1.set_title('Manual PCA')\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_aspect('equal')\n",
    "    \n",
    "    # Scikit-learn PCA visualization\n",
    "    ax2.scatter(X[:, 0], X[:, 1], alpha=0.6, s=50, color='blue')\n",
    "    sklearn_mean = np.mean(X, axis=0)\n",
    "    \n",
    "    pc1_sklearn = sklearn_components[0] * np.sqrt(pca_sklearn.explained_variance_[0]) * scale_factor\n",
    "    pc2_sklearn = sklearn_components[1] * np.sqrt(pca_sklearn.explained_variance_[1]) * scale_factor\n",
    "    \n",
    "    ax2.arrow(sklearn_mean[0], sklearn_mean[1], pc1_sklearn[0], pc1_sklearn[1],\n",
    "              head_width=0.1, head_length=0.15, fc='red', ec='red', linewidth=2)\n",
    "    ax2.arrow(sklearn_mean[0], sklearn_mean[1], pc2_sklearn[0], pc2_sklearn[1],\n",
    "              head_width=0.1, head_length=0.15, fc='green', ec='green', linewidth=2)\n",
    "    \n",
    "    ax2.set_title('Scikit-learn PCA')\n",
    "    ax2.set_xlabel('X')\n",
    "    ax2.set_ylabel('Y')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_aspect('equal')\n",
    "    \n",
    "    # Transformed data comparison\n",
    "    ax3.scatter(manual_transformed[:, 0], manual_transformed[:, 1], \n",
    "               alpha=0.6, s=50, color='red', label='Manual PCA')\n",
    "    ax3.set_title('Manual PCA - Transformed Data')\n",
    "    ax3.set_xlabel('PC1')\n",
    "    ax3.set_ylabel('PC2')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_aspect('equal')\n",
    "    \n",
    "    ax4.scatter(sklearn_transformed[:, 0], sklearn_transformed[:, 1], \n",
    "               alpha=0.6, s=50, color='blue', label='Scikit-learn PCA')\n",
    "    ax4.set_title('Scikit-learn PCA - Transformed Data')\n",
    "    ax4.set_xlabel('PC1')\n",
    "    ax4.set_ylabel('PC2')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return max_diff < 1e-6\n",
    "\n",
    "# Test comparison on different datasets\n",
    "test_datasets = ['Highly Correlated', 'Diagonal Pattern', 'Uncorrelated (Different Variances)']\n",
    "\n",
    "all_match = True\n",
    "for dataset_name in test_datasets:\n",
    "    data = datasets[dataset_name]\n",
    "    matches = compare_pca_implementations(data, dataset_name)\n",
    "    all_match = all_match and matches\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"FINAL VALIDATION RESULT:\")\n",
    "print(\"=\" * 30)\n",
    "if all_match:\n",
    "    print(\"üéâ SUCCESS! Our manual PCA implementation matches scikit-learn perfectly!\")\n",
    "    print(\"‚úÖ All eigenvalues, eigenvectors, and transformations are consistent\")\n",
    "else:\n",
    "    print(\"‚ùå There are some differences between implementations\")\n",
    "    print(\"üîç Review the comparison details above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67361b01",
   "metadata": {},
   "source": [
    "## 6. Interactive PCA Explorer\n",
    "\n",
    "Now let's create an interactive tool that allows you to modify data parameters in real-time and see how PCA responds to different data patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c39855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractivePCAExplorer:\n",
    "    \"\"\"\n",
    "    Interactive tool for exploring PCA with different data parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fig = None\n",
    "        self.axes = None\n",
    "    \n",
    "    def generate_interactive_data(self, correlation, x_variance, y_variance, rotation_angle, n_samples):\n",
    "        \"\"\"\n",
    "        Generate 2D data with specified parameters\n",
    "        \"\"\"\n",
    "        # Create covariance matrix with specified variances and correlation\n",
    "        cov_matrix = np.array([[x_variance, correlation * np.sqrt(x_variance * y_variance)],\n",
    "                              [correlation * np.sqrt(x_variance * y_variance), y_variance]])\n",
    "        \n",
    "        # Generate base data\n",
    "        data = np.random.multivariate_normal([0, 0], cov_matrix, n_samples)\n",
    "        \n",
    "        # Apply rotation\n",
    "        rotation_rad = np.radians(rotation_angle)\n",
    "        rotation_matrix = np.array([[np.cos(rotation_rad), -np.sin(rotation_rad)],\n",
    "                                   [np.sin(rotation_rad), np.cos(rotation_rad)]])\n",
    "        data = data @ rotation_matrix.T\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def plot_interactive_pca(self, correlation=0.7, x_variance=1.0, y_variance=2.0, \n",
    "                           rotation_angle=0, n_samples=300):\n",
    "        \"\"\"\n",
    "        Interactive PCA visualization\n",
    "        \"\"\"\n",
    "        # Generate data\n",
    "        np.random.seed(42)  # For consistency\n",
    "        data = self.generate_interactive_data(correlation, x_variance, y_variance, \n",
    "                                            rotation_angle, n_samples)\n",
    "        \n",
    "        # Fit PCA\n",
    "        pca = PCA2D()\n",
    "        pca.fit(data)\n",
    "        transformed_data = pca.transform(data)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(7, 6))\n",
    "        \n",
    "        # 1. Original data with PCA components\n",
    "        ax1.clear()\n",
    "        ax1.scatter(data[:, 0], data[:, 1], alpha=0.6, s=30, color='blue')\n",
    "        \n",
    "        # Plot mean and principal components\n",
    "        mean_point = pca.mean_\n",
    "        ax1.scatter(mean_point[0], mean_point[1], color='red', s=100, marker='x', \n",
    "                   linewidth=3, zorder=5)\n",
    "        \n",
    "        # Scale arrows based on eigenvalues\n",
    "        scale_factor = 2\n",
    "        pc1_arrow = pca.eigenvectors_[:, 0] * np.sqrt(pca.eigenvalues_[0]) * scale_factor\n",
    "        pc2_arrow = pca.eigenvectors_[:, 1] * np.sqrt(pca.eigenvalues_[1]) * scale_factor\n",
    "        \n",
    "        ax1.arrow(mean_point[0], mean_point[1], pc1_arrow[0], pc1_arrow[1],\n",
    "                  head_width=0.2, head_length=0.3, fc='red', ec='red', linewidth=3,\n",
    "                  label=f'PC1 ({pca.explained_variance_ratio_[0]:.1%})', zorder=4)\n",
    "        ax1.arrow(mean_point[0], mean_point[1], pc2_arrow[0], pc2_arrow[1],\n",
    "                  head_width=0.2, head_length=0.3, fc='green', ec='green', linewidth=3,\n",
    "                  label=f'PC2 ({pca.explained_variance_ratio_[1]:.1%})', zorder=4)\n",
    "        \n",
    "        ax1.set_xlabel('X')\n",
    "        ax1.set_ylabel('Y')\n",
    "        ax1.set_title('Original Data with Principal Components')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_aspect('equal', adjustable='box')\n",
    "        \n",
    "        # Set consistent axis limits\n",
    "        data_range = np.max([np.std(data[:, 0]), np.std(data[:, 1])]) * 3\n",
    "        ax1.set_xlim(mean_point[0] - data_range, mean_point[0] + data_range)\n",
    "        ax1.set_ylim(mean_point[1] - data_range, mean_point[1] + data_range)\n",
    "        \n",
    "        # 2. Transformed data (PCA space)\n",
    "        ax2.clear()\n",
    "        ax2.scatter(transformed_data[:, 0], transformed_data[:, 1], alpha=0.6, s=30, color='purple')\n",
    "        ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        ax2.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "        ax2.set_xlabel('First Principal Component')\n",
    "        ax2.set_ylabel('Second Principal Component')\n",
    "        ax2.set_title('Data in PCA Space')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_aspect('equal', adjustable='box')\n",
    "        \n",
    "        # 3. Explained variance\n",
    "        ax3.clear()\n",
    "        components = ['PC1', 'PC2']\n",
    "        variance_ratios = pca.explained_variance_ratio_\n",
    "        bars = ax3.bar(components, variance_ratios, color=['red', 'green'], alpha=0.7)\n",
    "        ax3.set_ylabel('Explained Variance Ratio')\n",
    "        ax3.set_title('Explained Variance by Component')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.set_ylim(0, 1)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, ratio in zip(bars, variance_ratios):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{ratio:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # 4. Data statistics and PCA info\n",
    "        ax4.clear()\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        # Calculate statistics\n",
    "        actual_correlation = np.corrcoef(data[:, 0], data[:, 1])[0, 1]\n",
    "        actual_x_var = np.var(data[:, 0])\n",
    "        actual_y_var = np.var(data[:, 1])\n",
    "        \n",
    "        info_text = f\"\"\"\n",
    "        INPUT PARAMETERS:\n",
    "        ‚Ä¢ Correlation: {correlation:.2f}\n",
    "        ‚Ä¢ X Variance: {x_variance:.2f}\n",
    "        ‚Ä¢ Y Variance: {y_variance:.2f}\n",
    "        ‚Ä¢ Rotation: {rotation_angle}¬∞\n",
    "        ‚Ä¢ Samples: {n_samples}\n",
    "        \n",
    "        ACTUAL DATA STATISTICS:\n",
    "        ‚Ä¢ Correlation: {actual_correlation:.3f}\n",
    "        ‚Ä¢ X Variance: {actual_x_var:.3f}\n",
    "        ‚Ä¢ Y Variance: {actual_y_var:.3f}\n",
    "        \n",
    "        PCA RESULTS:\n",
    "        ‚Ä¢ PC1 explains {pca.explained_variance_ratio_[0]:.1%}\n",
    "        ‚Ä¢ PC2 explains {pca.explained_variance_ratio_[1]:.1%}\n",
    "        ‚Ä¢ Total: {np.sum(pca.explained_variance_ratio_):.1%}\n",
    "        \n",
    "        PC1 Direction: [{pca.eigenvectors_[0, 0]:.3f}, {pca.eigenvectors_[1, 0]:.3f}]\n",
    "        PC2 Direction: [{pca.eigenvectors_[0, 1]:.3f}, {pca.eigenvectors_[1, 1]:.3f}]\n",
    "        \"\"\"\n",
    "        \n",
    "        ax4.text(0.1, 0.9, info_text, transform=ax4.transAxes, fontsize=11,\n",
    "                verticalalignment='top', fontfamily='monospace',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print interpretation\n",
    "        if pca.explained_variance_ratio_[0] > 0.9:\n",
    "            print(\"üîç INTERPRETATION: Very strong dimensionality - almost all variance in PC1!\")\n",
    "        elif pca.explained_variance_ratio_[0] > 0.7:\n",
    "            print(\"üîç INTERPRETATION: Strong primary direction - PC1 captures most variance\")\n",
    "        elif pca.explained_variance_ratio_[0] < 0.6:\n",
    "            print(\"üîç INTERPRETATION: Balanced variance - both components important\")\n",
    "        \n",
    "        return pca\n",
    "\n",
    "# Create interactive explorer\n",
    "explorer = InteractivePCAExplorer()\n",
    "\n",
    "print(\"üéÆ INTERACTIVE PCA EXPLORER\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Adjust the parameters below to see how PCA responds to different data patterns!\")\n",
    "print()\n",
    "\n",
    "# Create interactive widgets\n",
    "interact(explorer.plot_interactive_pca,\n",
    "         correlation=widgets.FloatSlider(value=0.7, min=-0.95, max=0.95, step=0.05,\n",
    "                                       description='Correlation:', style={'description_width': 'initial'}),\n",
    "         x_variance=widgets.FloatSlider(value=1.0, min=0.1, max=5.0, step=0.1,\n",
    "                                      description='X Variance:', style={'description_width': 'initial'}),\n",
    "         y_variance=widgets.FloatSlider(value=2.0, min=0.1, max=5.0, step=0.1,\n",
    "                                      description='Y Variance:', style={'description_width': 'initial'}),\n",
    "         rotation_angle=widgets.IntSlider(value=0, min=-90, max=90, step=5,\n",
    "                                        description='Rotation (¬∞):', style={'description_width': 'initial'}),\n",
    "         n_samples=widgets.IntSlider(value=300, min=50, max=1000, step=50,\n",
    "                                   description='# Samples:', style={'description_width': 'initial'}));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72473b40",
   "metadata": {},
   "source": [
    "## 7. Apply PCA to Real 2D Datasets\n",
    "\n",
    "Let's see how PCA works on real-world data by analyzing actual datasets and understanding what the principal components represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd9c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real datasets for PCA analysis\n",
    "def analyze_real_datasets():\n",
    "    \"\"\"\n",
    "    Apply PCA to real-world datasets\n",
    "    \"\"\"\n",
    "    print(\"APPLYING PCA TO REAL DATASETS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Iris dataset - petal measurements\n",
    "    iris = load_iris()\n",
    "    iris_data_petal = iris.data[:, 2:4]  # Petal length and width\n",
    "    iris_data_sepal = iris.data[:, 0:2]  # Sepal length and width\n",
    "    \n",
    "    # 2. Create some other interesting real-world-like data\n",
    "    # Simulated industrial sensor data\n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    \n",
    "    # Temperature and pressure correlation (industrial process)\n",
    "    base_temp = np.random.normal(350, 50, n_samples)  # Temperature in Celsius\n",
    "    pressure = 2.5 * base_temp + np.random.normal(0, 20, n_samples)  # Pressure correlated with temp\n",
    "    industrial_data = np.column_stack([base_temp, pressure])\n",
    "    \n",
    "    # Economic data simulation - GDP vs unemployment (inverse correlation)\n",
    "    gdp_growth = np.random.normal(3, 2, n_samples)\n",
    "    unemployment = -0.8 * gdp_growth + 5 + np.random.normal(0, 1, n_samples)\n",
    "    unemployment = np.clip(unemployment, 1, 15)  # Keep realistic range\n",
    "    economic_data = np.column_stack([gdp_growth, unemployment])\n",
    "    \n",
    "    real_datasets = {\n",
    "        'Iris - Petal Measurements': {\n",
    "            'data': iris_data_petal,\n",
    "            'feature_names': ['Petal Length (cm)', 'Petal Width (cm)'],\n",
    "            'colors': iris.target,\n",
    "            'target_names': iris.target_names\n",
    "        },\n",
    "        'Iris - Sepal Measurements': {\n",
    "            'data': iris_data_sepal,\n",
    "            'feature_names': ['Sepal Length (cm)', 'Sepal Width (cm)'],\n",
    "            'colors': iris.target,\n",
    "            'target_names': iris.target_names\n",
    "        },\n",
    "        'Industrial Process Data': {\n",
    "            'data': industrial_data,\n",
    "            'feature_names': ['Temperature (¬∞C)', 'Pressure (Bar)'],\n",
    "            'colors': None,\n",
    "            'target_names': None\n",
    "        },\n",
    "        'Economic Indicators': {\n",
    "            'data': economic_data,\n",
    "            'feature_names': ['GDP Growth (%)', 'Unemployment (%)'],\n",
    "            'colors': None,\n",
    "            'target_names': None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return real_datasets\n",
    "\n",
    "def plot_real_dataset_pca(dataset_name, dataset_info):\n",
    "    \"\"\"\n",
    "    Plot PCA analysis for real datasets\n",
    "    \"\"\"\n",
    "    data = dataset_info['data']\n",
    "    feature_names = dataset_info['feature_names']\n",
    "    colors = dataset_info['colors']\n",
    "    target_names = dataset_info['target_names']\n",
    "    \n",
    "    print(f\"\\nANALYZING: {dataset_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA2D()\n",
    "    pca.fit(data)\n",
    "    transformed_data = pca.transform(data)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(8, 6))\n",
    "    \n",
    "    # 1. Original data\n",
    "    if colors is not None:\n",
    "        scatter = ax1.scatter(data[:, 0], data[:, 1], c=colors, alpha=0.7, s=50, cmap='viridis')\n",
    "        if target_names is not None:\n",
    "            # Create legend\n",
    "            for i, name in enumerate(target_names):\n",
    "                ax1.scatter([], [], c=plt.cm.viridis(i / len(target_names)), label=name)\n",
    "            ax1.legend()\n",
    "    else:\n",
    "        ax1.scatter(data[:, 0], data[:, 1], alpha=0.7, s=50, color='blue')\n",
    "    \n",
    "    # Plot principal components\n",
    "    mean_point = pca.mean_\n",
    "    ax1.scatter(mean_point[0], mean_point[1], color='red', s=100, marker='x', \n",
    "               linewidth=3, zorder=5)\n",
    "    \n",
    "    # Scale arrows appropriately for the data range\n",
    "    data_std = np.std(data, axis=0)\n",
    "    scale_factor = np.mean(data_std) * 1.5\n",
    "    \n",
    "    pc1_arrow = pca.eigenvectors_[:, 0] * np.sqrt(pca.eigenvalues_[0]) * scale_factor\n",
    "    pc2_arrow = pca.eigenvectors_[:, 1] * np.sqrt(pca.eigenvalues_[1]) * scale_factor\n",
    "    \n",
    "    ax1.arrow(mean_point[0], mean_point[1], pc1_arrow[0], pc1_arrow[1],\n",
    "              head_width=data_std[0]*0.1, head_length=data_std[1]*0.1, \n",
    "              fc='red', ec='red', linewidth=3, zorder=4)\n",
    "    ax1.arrow(mean_point[0], mean_point[1], pc2_arrow[0], pc2_arrow[1],\n",
    "              head_width=data_std[0]*0.1, head_length=data_std[1]*0.1, \n",
    "              fc='green', ec='green', linewidth=3, zorder=4)\n",
    "    \n",
    "    ax1.set_xlabel(feature_names[0])\n",
    "    ax1.set_ylabel(feature_names[1])\n",
    "    ax1.set_title(f'{dataset_name} - Original Data')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Transformed data\n",
    "    if colors is not None:\n",
    "        ax2.scatter(transformed_data[:, 0], transformed_data[:, 1], c=colors, \n",
    "                   alpha=0.7, s=50, cmap='viridis')\n",
    "    else:\n",
    "        ax2.scatter(transformed_data[:, 0], transformed_data[:, 1], \n",
    "                   alpha=0.7, s=50, color='purple')\n",
    "    \n",
    "    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax2.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax2.set_xlabel('First Principal Component')\n",
    "    ax2.set_ylabel('Second Principal Component')\n",
    "    ax2.set_title(f'{dataset_name} - PCA Space')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Explained variance\n",
    "    variance_ratios = pca.explained_variance_ratio_\n",
    "    components = ['PC1', 'PC2']\n",
    "    bars = ax3.bar(components, variance_ratios, color=['red', 'green'], alpha=0.7)\n",
    "    ax3.set_ylabel('Explained Variance Ratio')\n",
    "    ax3.set_title('Explained Variance')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_ylim(0, 1)\n",
    "    \n",
    "    for bar, ratio in zip(bars, variance_ratios):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{ratio:.1%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Interpretation and statistics\n",
    "    ax4.axis('off')\n",
    "    \n",
    "    # Calculate correlation\n",
    "    correlation = np.corrcoef(data[:, 0], data[:, 1])[0, 1]\n",
    "    \n",
    "    interpretation_text = f\"\"\"\n",
    "    DATASET: {dataset_name}\n",
    "    Features: {feature_names[0]} vs {feature_names[1]}\n",
    "    \n",
    "    STATISTICS:\n",
    "    ‚Ä¢ Sample size: {data.shape[0]}\n",
    "    ‚Ä¢ Correlation: {correlation:.3f}\n",
    "    ‚Ä¢ {feature_names[0]} std: {np.std(data[:, 0]):.3f}\n",
    "    ‚Ä¢ {feature_names[1]} std: {np.std(data[:, 1]):.3f}\n",
    "    \n",
    "    PCA RESULTS:\n",
    "    ‚Ä¢ PC1 explains {variance_ratios[0]:.1%} of variance\n",
    "    ‚Ä¢ PC2 explains {variance_ratios[1]:.1%} of variance\n",
    "    \n",
    "    INTERPRETATION:\n",
    "    PC1 direction: [{pca.eigenvectors_[0, 0]:.3f}, {pca.eigenvectors_[1, 0]:.3f}]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add domain-specific interpretation\n",
    "    if 'Petal' in dataset_name:\n",
    "        interpretation_text += \"\\nüå∫ PC1 represents overall petal size\"\n",
    "        interpretation_text += \"\\n   PC2 represents petal shape variation\"\n",
    "    elif 'Sepal' in dataset_name:\n",
    "        interpretation_text += \"\\nüåø PC1 represents overall sepal size\"\n",
    "        interpretation_text += \"\\n   PC2 represents sepal shape variation\"\n",
    "    elif 'Industrial' in dataset_name:\n",
    "        interpretation_text += \"\\nüè≠ PC1 represents overall process intensity\"\n",
    "        interpretation_text += \"\\n   (high temp + high pressure)\"\n",
    "    elif 'Economic' in dataset_name:\n",
    "        interpretation_text += \"\\nüìä PC1 represents economic health\"\n",
    "        interpretation_text += \"\\n   (GDP growth vs unemployment trade-off)\"\n",
    "    \n",
    "    ax4.text(0.05, 0.95, interpretation_text, transform=ax4.transAxes,\n",
    "            fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed analysis\n",
    "    print(f\"Correlation between {feature_names[0]} and {feature_names[1]}: {correlation:.3f}\")\n",
    "    print(f\"PC1 explains {variance_ratios[0]:.1%} of the total variance\")\n",
    "    print(f\"PC2 explains {variance_ratios[1]:.1%} of the total variance\")\n",
    "    \n",
    "    if variance_ratios[0] > 0.8:\n",
    "        print(\"üëâ Strong dimensionality reduction possible - PC1 captures most information\")\n",
    "    elif variance_ratios[0] > 0.6:\n",
    "        print(\"üëâ Moderate dimensionality reduction - PC1 is dominant but PC2 is important\")\n",
    "    else:\n",
    "        print(\"üëâ Both components are important - limited dimensionality reduction\")\n",
    "\n",
    "# Load and analyze real datasets\n",
    "real_datasets = analyze_real_datasets()\n",
    "\n",
    "print(f\"Loaded {len(real_datasets)} real-world datasets for PCA analysis:\")\n",
    "for name in real_datasets.keys():\n",
    "    print(f\"  ‚Ä¢ {name}\")\n",
    "\n",
    "# Analyze each dataset\n",
    "for dataset_name, dataset_info in real_datasets.items():\n",
    "    plot_real_dataset_pca(dataset_name, dataset_info)\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e36b316",
   "metadata": {},
   "source": [
    "## 8. Demonstrate Dimensionality Reduction Effects\n",
    "\n",
    "Let's explore how projecting 2D data onto just the first principal component reduces dimensionality while preserving maximum variance, and visualize the reconstruction error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aabfd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_dimensionality_reduction(data, dataset_name):\n",
    "    \"\"\"\n",
    "    Show the effects of reducing 2D data to 1D using PCA\n",
    "    \"\"\"\n",
    "    print(f\"DIMENSIONALITY REDUCTION DEMONSTRATION - {dataset_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Fit PCA\n",
    "    pca = PCA2D()\n",
    "    pca.fit(data)\n",
    "    \n",
    "    # Full transformation (2D -> 2D)\n",
    "    data_2d_transformed = pca.transform(data)\n",
    "    data_2d_reconstructed = pca.inverse_transform(data_2d_transformed)\n",
    "    \n",
    "    # Dimension reduction (2D -> 1D -> 2D)\n",
    "    # Project only onto first principal component\n",
    "    data_1d_transformed = data_2d_transformed.copy()\n",
    "    data_1d_transformed[:, 1] = 0  # Set second component to zero\n",
    "    data_1d_reconstructed = pca.inverse_transform(data_1d_transformed)\n",
    "    \n",
    "    # Calculate reconstruction errors\n",
    "    error_2d = np.mean(np.sum((data - data_2d_reconstructed)**2, axis=1))\n",
    "    error_1d = np.mean(np.sum((data - data_1d_reconstructed)**2, axis=1))\n",
    "    \n",
    "    # Calculate information retention\n",
    "    variance_retained = pca.explained_variance_ratio_[0]\n",
    "    information_lost = 1 - variance_retained\n",
    "    \n",
    "    print(f\"Original data shape: {data.shape}\")\n",
    "    print(f\"Reduced data would be: ({data.shape[0]}, 1) - {50}% size reduction\")\n",
    "    print(f\"Information retained: {variance_retained:.1%}\")\n",
    "    print(f\"Information lost: {information_lost:.1%}\")\n",
    "    print(f\"Reconstruction error (2D): {error_2d:.6f}\")\n",
    "    print(f\"Reconstruction error (1D): {error_1d:.6f}\")\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig = plt.figure(figsize=(9, 6))\n",
    "    \n",
    "    # Create a grid layout\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Original data\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.scatter(data[:, 0], data[:, 1], alpha=0.7, s=50, color='blue')\n",
    "    ax1.set_title('Original 2D Data')\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_aspect('equal')\n",
    "    \n",
    "    # 2. Data with PCA components\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.scatter(data[:, 0], data[:, 1], alpha=0.7, s=50, color='blue')\n",
    "    \n",
    "    # Plot PCA components\n",
    "    mean_point = pca.mean_\n",
    "    ax2.scatter(mean_point[0], mean_point[1], color='red', s=100, marker='x', \n",
    "               linewidth=3, zorder=5)\n",
    "    \n",
    "    scale_factor = 2\n",
    "    pc1_arrow = pca.eigenvectors_[:, 0] * np.sqrt(pca.eigenvalues_[0]) * scale_factor\n",
    "    pc2_arrow = pca.eigenvectors_[:, 1] * np.sqrt(pca.eigenvalues_[1]) * scale_factor\n",
    "    \n",
    "    ax2.arrow(mean_point[0], mean_point[1], pc1_arrow[0], pc1_arrow[1],\n",
    "              head_width=0.1, head_length=0.15, fc='red', ec='red', linewidth=3)\n",
    "    ax2.arrow(mean_point[0], mean_point[1], pc2_arrow[0], pc2_arrow[1],\n",
    "              head_width=0.1, head_length=0.15, fc='green', ec='green', linewidth=3)\n",
    "    \n",
    "    ax2.set_title('Data with Principal Components')\n",
    "    ax2.set_xlabel('X')\n",
    "    ax2.set_ylabel('Y')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_aspect('equal')\n",
    "    \n",
    "    # 3. Full PCA transformation (2D -> 2D)\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    ax3.scatter(data_2d_transformed[:, 0], data_2d_transformed[:, 1], \n",
    "               alpha=0.7, s=50, color='purple')\n",
    "    ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax3.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax3.set_title('Full PCA Space (2D)')\n",
    "    ax3.set_xlabel('PC1')\n",
    "    ax3.set_ylabel('PC2')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.set_aspect('equal')\n",
    "    \n",
    "    # 4. Dimensionality reduction (2D -> 1D)\n",
    "    ax4 = fig.add_subplot(gs[1, 0])\n",
    "    ax4.scatter(data_1d_transformed[:, 0], np.zeros_like(data_1d_transformed[:, 0]), \n",
    "               alpha=0.7, s=50, color='orange')\n",
    "    ax4.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax4.set_title('Reduced to 1D (PC1 only)')\n",
    "    ax4.set_xlabel('PC1')\n",
    "    ax4.set_ylabel('PC2 (set to 0)')\n",
    "    ax4.set_ylim(-0.5, 0.5)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Reconstructed from 1D\n",
    "    ax5 = fig.add_subplot(gs[1, 1])\n",
    "    ax5.scatter(data[:, 0], data[:, 1], alpha=0.5, s=30, color='blue', label='Original')\n",
    "    ax5.scatter(data_1d_reconstructed[:, 0], data_1d_reconstructed[:, 1], \n",
    "               alpha=0.7, s=30, color='red', label='Reconstructed')\n",
    "    \n",
    "    # Draw lines showing reconstruction error\n",
    "    for i in range(0, len(data), 10):  # Show every 10th point for clarity\n",
    "        ax5.plot([data[i, 0], data_1d_reconstructed[i, 0]], \n",
    "                [data[i, 1], data_1d_reconstructed[i, 1]], \n",
    "                'k-', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    ax5.set_title('Original vs Reconstructed')\n",
    "    ax5.set_xlabel('X')\n",
    "    ax5.set_ylabel('Y')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    ax5.set_aspect('equal')\n",
    "    \n",
    "    # 6. Error visualization\n",
    "    ax6 = fig.add_subplot(gs[1, 2])\n",
    "    errors = np.sqrt(np.sum((data - data_1d_reconstructed)**2, axis=1))\n",
    "    scatter = ax6.scatter(data[:, 0], data[:, 1], c=errors, s=50, \n",
    "                         cmap='Reds', alpha=0.7)\n",
    "    plt.colorbar(scatter, ax=ax6, label='Reconstruction Error')\n",
    "    ax6.set_title('Reconstruction Error Map')\n",
    "    ax6.set_xlabel('X')\n",
    "    ax6.set_ylabel('Y')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    ax6.set_aspect('equal')\n",
    "    \n",
    "    # 7. Error statistics\n",
    "    ax7 = fig.add_subplot(gs[2, :])\n",
    "    ax7.axis('off')\n",
    "    \n",
    "    stats_text = f\"\"\"\n",
    "    DIMENSIONALITY REDUCTION ANALYSIS - {dataset_name}\n",
    "    \n",
    "    ORIGINAL DATA:           {data.shape[0]} samples √ó 2 dimensions = {data.shape[0] * 2} values\n",
    "    REDUCED DATA:            {data.shape[0]} samples √ó 1 dimension  = {data.shape[0]} values\n",
    "    COMPRESSION RATIO:       50% size reduction\n",
    "    \n",
    "    INFORMATION ANALYSIS:\n",
    "    ‚Ä¢ PC1 captures {pca.explained_variance_ratio_[0]:.1%} of total variance\n",
    "    ‚Ä¢ PC2 captures {pca.explained_variance_ratio_[1]:.1%} of total variance\n",
    "    ‚Ä¢ Information retained after reduction: {variance_retained:.1%}\n",
    "    ‚Ä¢ Information lost: {information_lost:.1%}\n",
    "    \n",
    "    RECONSTRUCTION ERROR:\n",
    "    ‚Ä¢ Mean squared error: {error_1d:.6f}\n",
    "    ‚Ä¢ Root mean squared error: {np.sqrt(error_1d):.6f}\n",
    "    ‚Ä¢ Maximum error: {np.max(errors):.6f}\n",
    "    ‚Ä¢ Points with error > 1.0: {np.sum(errors > 1.0)}/{len(errors)} ({100*np.sum(errors > 1.0)/len(errors):.1f}%)\n",
    "    \n",
    "    PRACTICAL IMPLICATIONS:\n",
    "    ‚Ä¢ Storage savings: 50% less memory required\n",
    "    ‚Ä¢ Computation savings: ~50% fewer operations for many algorithms\n",
    "    ‚Ä¢ Visualization: Can plot 1D data as histogram or line plot\n",
    "    ‚Ä¢ Noise reduction: Removing PC2 can eliminate noise if it's in that direction\n",
    "    \"\"\"\n",
    "    \n",
    "    ax7.text(0.05, 0.95, stats_text, transform=ax7.transAxes, fontsize=11,\n",
    "            verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    plt.suptitle(f'Dimensionality Reduction: 2D ‚Üí 1D using PCA\\n{dataset_name}', \n",
    "                fontsize=16, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'original_data': data,\n",
    "        'reduced_data': data_1d_transformed[:, 0],  # Just the 1D values\n",
    "        'reconstructed_data': data_1d_reconstructed,\n",
    "        'reconstruction_error': error_1d,\n",
    "        'variance_retained': variance_retained,\n",
    "        'compression_ratio': 0.5\n",
    "    }\n",
    "\n",
    "# Demonstrate dimensionality reduction on different datasets\n",
    "print(\"COMPREHENSIVE DIMENSIONALITY REDUCTION DEMONSTRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test on synthetic datasets\n",
    "demo_datasets = {\n",
    "    'Highly Correlated': datasets['Highly Correlated'],\n",
    "    'Circular Pattern': datasets['Circular Pattern'],\n",
    "    'Uncorrelated (Different Variances)': datasets['Uncorrelated (Different Variances)']\n",
    "}\n",
    "\n",
    "reduction_results = {}\n",
    "\n",
    "for name, data in demo_datasets.items():\n",
    "    result = demonstrate_dimensionality_reduction(data, name)\n",
    "    reduction_results[name] = result\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"DIMENSIONALITY REDUCTION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Dataset': list(reduction_results.keys()),\n",
    "    'Variance Retained': [r['variance_retained'] for r in reduction_results.values()],\n",
    "    'Reconstruction Error': [r['reconstruction_error'] for r in reduction_results.values()],\n",
    "    'Compression Ratio': [r['compression_ratio'] for r in reduction_results.values()]\n",
    "})\n",
    "\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\nKEY INSIGHTS:\")\n",
    "print(\"üîç Highly correlated data ‚Üí Low reconstruction error, high variance retained\")\n",
    "print(\"üîç Uncorrelated data ‚Üí Higher reconstruction error, but still decent compression\")\n",
    "print(\"üîç Circular/complex patterns ‚Üí May not be suitable for linear PCA reduction\")\n",
    "print(\"üîç Best candidates for reduction: data with strong linear correlations\")\n",
    "\n",
    "print(\"\\n‚úÖ PCA 2D demonstration complete!\")\n",
    "print(\"üéØ You now understand how PCA finds principal components and reduces dimensionality!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9456bc8f",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways and Understanding PCA\n",
    "\n",
    "### **What We've Learned About PCA:**\n",
    "\n",
    "#### **üîç Mathematical Understanding:**\n",
    "- **Principal Components**: Are the directions of maximum variance in the data\n",
    "- **Eigenvectors**: Define the directions of principal components\n",
    "- **Eigenvalues**: Represent the amount of variance along each principal component\n",
    "- **Orthogonality**: Principal components are always perpendicular to each other\n",
    "\n",
    "#### **üéØ PCA Process (Step-by-Step):**\n",
    "1. **Center the data**: Subtract the mean from each data point\n",
    "2. **Compute covariance matrix**: Shows how variables vary together\n",
    "3. **Find eigenvalues/eigenvectors**: Mathematical recipe for principal directions\n",
    "4. **Sort by importance**: Largest eigenvalue = most important direction\n",
    "5. **Transform data**: Project original data onto new coordinate system\n",
    "\n",
    "#### **üìä When PCA Works Best:**\n",
    "- **Linear correlations**: Strong linear relationships between variables\n",
    "- **High-dimensional data**: More effective with more dimensions\n",
    "- **Continuous variables**: Works best with numerical data\n",
    "- **Normalized data**: Variables should be on similar scales\n",
    "\n",
    "#### **‚ö†Ô∏è When PCA May Not Work Well:**\n",
    "- **Non-linear relationships**: PCA only captures linear patterns\n",
    "- **Circular/curved patterns**: Linear projections may not be meaningful\n",
    "- **Categorical data**: PCA is designed for continuous variables\n",
    "- **Equal variance in all directions**: No dimensionality reduction possible\n",
    "\n",
    "### **Practical Applications:**\n",
    "\n",
    "#### **üè≠ Industrial Control Systems:**\n",
    "- **Sensor fusion**: Combine multiple sensor readings into principal components\n",
    "- **Process monitoring**: Detect abnormal patterns in multivariate process data\n",
    "- **Quality control**: Reduce measurement dimensions while preserving quality information\n",
    "- **Fault detection**: Use PCA to identify when systems deviate from normal operation\n",
    "\n",
    "#### **üìà Data Analysis:**\n",
    "- **Dimensionality reduction**: Reduce storage and computation requirements\n",
    "- **Noise reduction**: Lower-importance components often contain noise\n",
    "- **Visualization**: Project high-dimensional data to 2D/3D for plotting\n",
    "- **Feature engineering**: Create new features that capture most variance\n",
    "\n",
    "### **Key Insights from Our Experiments:**\n",
    "\n",
    "#### **üî¨ Correlation Impact:**\n",
    "- High correlation (>0.8) ‚Üí PC1 dominates, excellent dimensionality reduction\n",
    "- Medium correlation (0.4-0.7) ‚Üí Both components important, moderate reduction\n",
    "- Low correlation (<0.3) ‚Üí Limited reduction potential, both components needed\n",
    "\n",
    "#### **üìê Geometric Interpretation:**\n",
    "- PC1 arrow points in direction of maximum data spread\n",
    "- PC2 arrow is perpendicular and points in second-largest spread direction\n",
    "- Arrow length represents importance (square root of eigenvalue)\n",
    "\n",
    "#### **üíæ Information Trade-offs:**\n",
    "- Keeping PC1 only: 50% storage reduction, but some information loss\n",
    "- Information retained = PC1's explained variance ratio\n",
    "- Reconstruction error quantifies information loss\n",
    "\n",
    "### **Extension to Higher Dimensions:**\n",
    "\n",
    "The concepts we learned with 2D data extend directly to higher dimensions:\n",
    "- **3D data**: 3 principal components (PC1, PC2, PC3)\n",
    "- **High-D data**: As many PCs as dimensions, but typically few dominate\n",
    "- **Dimensionality reduction**: Keep only the most important components\n",
    "- **Visualization**: Project high-D data to 2D using first 2 PCs\n",
    "\n",
    "### **Best Practices:**\n",
    "\n",
    "1. **Always center your data** before applying PCA\n",
    "2. **Scale variables** if they have different units/ranges\n",
    "3. **Check explained variance** to decide how many components to keep\n",
    "4. **Visualize results** to ensure PCA makes sense for your data\n",
    "5. **Validate reconstructions** to understand information loss\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "- **Try PCA on your own datasets** using the tools from this notebook\n",
    "- **Experiment with different data patterns** using the interactive explorer\n",
    "- **Learn about advanced techniques**: Kernel PCA, Sparse PCA, Incremental PCA\n",
    "- **Apply to real problems**: Image compression, face recognition, genomics\n",
    "\n",
    "---\n",
    "\n",
    "**This tutorial provided a complete foundation for understanding PCA through 2D visualizations. The mathematical concepts, geometric intuition, and practical applications you learned here form the basis for using PCA effectively in real-world data analysis and industrial applications.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
